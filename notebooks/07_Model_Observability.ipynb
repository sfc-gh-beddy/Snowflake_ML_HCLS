{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# \ud83d\udcca Snowflake ML Demo: Model Observability & Monitoring\n",
    "\n",
    "This notebook sets up comprehensive monitoring for your deployed ML model to track performance, detect data drift, and ensure production reliability.\n",
    "\n",
    "## \ud83c\udfaf What We're Building\n",
    "- **Performance Monitoring**: Track accuracy, precision, recall over time\n",
    "- **Data Drift Detection**: Monitor feature distribution changes\n",
    "- **Prediction Quality**: Analyze prediction patterns and confidence\n",
    "- **Automated Alerting**: Set up notifications for model degradation\n",
    "- **Business Dashboards**: Create executive views of model health\n",
    "\n",
    "## \ud83d\udea8 Why Observability Matters\n",
    "- **Early Warning**: Detect issues before they impact business\n",
    "- **Model Reliability**: Ensure consistent performance in production\n",
    "- **Regulatory Compliance**: Maintain audit trails for healthcare AI\n",
    "- **Continuous Improvement**: Data-driven model enhancement\n",
    "\n",
    "## \ud83d\udccb Key Components\n",
    "- **Drift Monitoring**: Statistical tests for feature changes\n",
    "- **Performance Tracking**: Model metrics over time\n",
    "- **Alert System**: Automated notifications for issues\n",
    "- **Executive Dashboards**: Business-friendly monitoring views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for model observability\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, lit, count, avg, stddev, min, max, abs\n",
    "from snowflake.ml.model_monitoring import ModelMonitor\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "print(\"\u2705 Observability libraries imported successfully!\")\n",
    "print(\"\ud83d\udcca Ready for comprehensive model monitoring setup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current session and set context\n",
    "session = Session.builder.getOrCreate()\n",
    "\n",
    "# Set context for monitoring setup\n",
    "session.use_database(\"ADVERSE_EVENT_MONITORING\")\n",
    "session.use_schema(\"ML_MODELS\")\n",
    "session.use_warehouse(\"ADVERSE_EVENT_WH\")\n",
    "\n",
    "print(\"\u2705 Session configured for model observability\")\n",
    "print(f\"\ud83d\udccd Database: {session.get_current_database()}\")\n",
    "print(f\"\ud83d\udccd Schema: {session.get_current_schema()}\")\n",
    "print(f\"\ud83d\udccd Warehouse: {session.get_current_warehouse()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \ud83c\udfd7\ufe0f Setting Up Monitoring Infrastructure\n",
    "\n",
    "First, let's create the additional monitoring tables we need for comprehensive observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create additional monitoring tables\n",
    "USE SCHEMA ML_MODELS;\n",
    "\n",
    "-- Model performance tracking over time\n",
    "CREATE TABLE IF NOT EXISTS MODEL_PERFORMANCE_TRACKING (\n",
    "    tracking_id VARCHAR(50),\n",
    "    model_id VARCHAR(50),\n",
    "    model_name VARCHAR(100),\n",
    "    evaluation_date TIMESTAMP,\n",
    "    dataset_name VARCHAR(100),\n",
    "    sample_size INTEGER,\n",
    "    accuracy_score FLOAT,\n",
    "    precision_score FLOAT,\n",
    "    recall_score FLOAT,\n",
    "    f1_score FLOAT,\n",
    "    auc_score FLOAT,\n",
    "    performance_status VARCHAR(20),\n",
    "    notes VARCHAR(1000)\n",
    ");\n",
    "\n",
    "-- Data drift monitoring\n",
    "CREATE TABLE IF NOT EXISTS DATA_DRIFT_MONITORING (\n",
    "    drift_id VARCHAR(50),\n",
    "    model_id VARCHAR(50),\n",
    "    feature_name VARCHAR(100),\n",
    "    monitoring_date TIMESTAMP,\n",
    "    reference_mean FLOAT,\n",
    "    current_mean FLOAT,\n",
    "    reference_std FLOAT,\n",
    "    current_std FLOAT,\n",
    "    drift_score FLOAT,\n",
    "    drift_threshold FLOAT,\n",
    "    drift_detected BOOLEAN,\n",
    "    drift_severity VARCHAR(20)\n",
    ");\n",
    "\n",
    "-- Prediction quality monitoring\n",
    "CREATE TABLE IF NOT EXISTS PREDICTION_QUALITY_MONITORING (\n",
    "    quality_id VARCHAR(50),\n",
    "    model_id VARCHAR(50),\n",
    "    monitoring_date TIMESTAMP,\n",
    "    total_predictions INTEGER,\n",
    "    high_confidence_predictions INTEGER,\n",
    "    low_confidence_predictions INTEGER,\n",
    "    avg_confidence_score FLOAT,\n",
    "    prediction_distribution OBJECT,\n",
    "    quality_status VARCHAR(20)\n",
    ");\n",
    "\n",
    "SELECT '\u2705 Monitoring infrastructure created' as STATUS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "## \ud83d\udd0d Data Drift Detection\n",
    "\n",
    "Let's implement data drift detection by comparing current inference data with our reference training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd0d Implementing data drift detection...\")\n",
    "\n",
    "# Load reference training data and current inference data\n",
    "session.use_schema(\"DEMO_ANALYTICS\")\n",
    "\n",
    "try:\n",
    "    # Reference data (training set)\n",
    "    training_data = session.table(\"PREPARED_HEALTHCARE_DATA\")\n",
    "    \n",
    "    # Current data (inference set)\n",
    "    current_data = session.table(\"NEW_PATIENT_INFERENCE_DATA\")\n",
    "    \n",
    "    print(f\"\u2705 Data loaded:\")\n",
    "    print(f\"   \u2022 Training data: {training_data.count()} records\")\n",
    "    print(f\"   \u2022 Current data: {current_data.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define features to monitor for drift\n",
    "numerical_features = [\"AGE\", \"TOTAL_CLAIM_AMOUNT_SUM\", \"NUM_CLAIMS\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\"]\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Monitoring {len(numerical_features)} numerical features for drift:\")\n",
    "for feature in numerical_features:\n",
    "    print(f\"   \u2022 {feature}\")\n",
    "\n",
    "# Calculate drift for each feature\n",
    "model_id = \"demo-model-id\"  # In practice, get from MODEL_REGISTRY\n",
    "drift_results = []\n",
    "drift_threshold = 2.0  # 2 standard deviations\n",
    "\n",
    "print(f\"\\n\ud83d\udd2c Calculating drift scores (threshold: {drift_threshold})...\")\n",
    "\n",
    "for feature in numerical_features:\n",
    "    try:\n",
    "        # Calculate reference statistics\n",
    "        ref_stats = training_data.select(\n",
    "            avg(col(feature)).alias(\"ref_mean\"),\n",
    "            stddev(col(feature)).alias(\"ref_std\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Calculate current statistics\n",
    "        curr_stats = current_data.select(\n",
    "            avg(col(feature)).alias(\"curr_mean\"), \n",
    "            stddev(col(feature)).alias(\"curr_std\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Handle None values\n",
    "        ref_mean = ref_stats[\"REF_MEAN\"] or 0\n",
    "        curr_mean = curr_stats[\"CURR_MEAN\"] or 0\n",
    "        ref_std = ref_stats[\"REF_STD\"] or 1\n",
    "        curr_std = curr_stats[\"CURR_STD\"] or 1\n",
    "        \n",
    "        # Calculate drift score (normalized difference)\n",
    "        drift_score = abs(curr_mean - ref_mean) / (ref_std + 1e-8)\n",
    "        drift_detected = drift_score > drift_threshold\n",
    "        \n",
    "        # Determine severity\n",
    "        if drift_score > 3.0:\n",
    "            severity = \"HIGH\"\n",
    "        elif drift_score > 2.0:\n",
    "            severity = \"MEDIUM\"\n",
    "        else:\n",
    "            severity = \"LOW\"\n",
    "            \n",
    "        drift_results.append([\n",
    "            str(uuid.uuid4()),  # drift_id\n",
    "            model_id,\n",
    "            feature,\n",
    "            datetime.datetime.now(),\n",
    "            ref_mean,\n",
    "            curr_mean,\n",
    "            ref_std,\n",
    "            curr_std,\n",
    "            drift_score,\n",
    "            drift_threshold,\n",
    "            drift_detected,\n",
    "            severity\n",
    "        ])\n",
    "        \n",
    "        # Print results\n",
    "        status = \"\ud83d\udea8 DRIFT DETECTED\" if drift_detected else \"\u2705 No drift\"\n",
    "        print(f\"   {feature}: {status} (score: {drift_score:.3f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f Error processing {feature}: {e}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd Saving drift monitoring results...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save drift monitoring results\n",
    "if drift_results:\n",
    "    session.use_schema(\"ML_MODELS\")\n",
    "    \n",
    "    drift_df = session.create_dataframe(\n",
    "        drift_results,\n",
    "        schema=[\n",
    "            \"drift_id\", \"model_id\", \"feature_name\", \"monitoring_date\",\n",
    "            \"reference_mean\", \"current_mean\", \"reference_std\", \"current_std\",\n",
    "            \"drift_score\", \"drift_threshold\", \"drift_detected\", \"drift_severity\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    drift_df.write.mode(\"append\").save_as_table(\"DATA_DRIFT_MONITORING\")\n",
    "    \n",
    "    print(f\"\u2705 Drift monitoring results saved for {len(numerical_features)} features\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_drift = sum(1 for result in drift_results if result[10])  # drift_detected column\n",
    "    print(f\"\ud83d\udcca Drift Summary:\")\n",
    "    print(f\"   \u2022 Features monitored: {len(drift_results)}\")\n",
    "    print(f\"   \u2022 Drift detected: {total_drift}\")\n",
    "    print(f\"   \u2022 Drift rate: {total_drift/len(drift_results)*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No drift results to save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \ud83d\udcc8 Prediction Quality Monitoring\n",
    "\n",
    "Now let's analyze the quality and patterns of our model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcc8 Analyzing prediction quality...\")\n",
    "\n",
    "session.use_schema(\"DEMO_ANALYTICS\")\n",
    "\n",
    "try:\n",
    "    # Analyze predictions from AE_PREDICTIONS table\n",
    "    predictions_stats = session.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_predictions,\n",
    "            COUNT(CASE WHEN PROBABILITY >= 0.7 THEN 1 END) as high_confidence_count,\n",
    "            COUNT(CASE WHEN PROBABILITY < 0.7 THEN 1 END) as low_confidence_count,\n",
    "            AVG(PROBABILITY) as avg_confidence,\n",
    "            MAX(PROBABILITY) as max_confidence,\n",
    "            MIN(PROBABILITY) as min_confidence,\n",
    "            COUNT(CASE WHEN PREDICTED_AE LIKE '%High%' THEN 1 END) as high_risk_predictions,\n",
    "            COUNT(CASE WHEN PREDICTED_AE LIKE '%Low%' THEN 1 END) as low_risk_predictions\n",
    "        FROM AE_PREDICTIONS\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if predictions_stats and predictions_stats[0]['TOTAL_PREDICTIONS'] > 0:\n",
    "        stats = predictions_stats[0]\n",
    "        \n",
    "        total_preds = stats[\"TOTAL_PREDICTIONS\"]\n",
    "        high_confidence = stats[\"HIGH_CONFIDENCE_COUNT\"] \n",
    "        low_confidence = stats[\"LOW_CONFIDENCE_COUNT\"]\n",
    "        avg_conf = stats[\"AVG_CONFIDENCE\"]\n",
    "        high_risk = stats[\"HIGH_RISK_PREDICTIONS\"]\n",
    "        low_risk = stats[\"LOW_RISK_PREDICTIONS\"]\n",
    "        \n",
    "        print(f\"\u2705 Prediction Quality Analysis:\")\n",
    "        print(f\"   \u2022 Total predictions: {total_preds}\")\n",
    "        print(f\"   \u2022 High confidence (\u22650.7): {high_confidence} ({high_confidence/total_preds*100:.1f}%)\")\n",
    "        print(f\"   \u2022 Low confidence (<0.7): {low_confidence} ({low_confidence/total_preds*100:.1f}%)\")\n",
    "        print(f\"   \u2022 Average confidence: {avg_conf:.3f}\")\n",
    "        print(f\"   \u2022 High risk predictions: {high_risk} ({high_risk/total_preds*100:.1f}%)\")\n",
    "        print(f\"   \u2022 Low risk predictions: {low_risk} ({low_risk/total_preds*100:.1f}%)\")\n",
    "        \n",
    "        # Determine quality status\n",
    "        positive_rate = high_risk / total_preds if total_preds > 0 else 0\n",
    "        \n",
    "        if positive_rate > 0.8 or positive_rate < 0.1:\n",
    "            quality_status = \"WARNING\"  # Extreme prediction bias\n",
    "            quality_note = \"Extreme prediction bias detected\"\n",
    "        elif avg_conf < 0.5:\n",
    "            quality_status = \"WARNING\"  # Low confidence\n",
    "            quality_note = \"Low average confidence scores\"\n",
    "        elif total_preds < 10:\n",
    "            quality_status = \"INSUFFICIENT_DATA\"\n",
    "            quality_note = \"Insufficient prediction volume\"\n",
    "        else:\n",
    "            quality_status = \"GOOD\"\n",
    "            quality_note = \"Predictions within normal parameters\"\n",
    "            \n",
    "        print(f\"\\n\ud83c\udfaf Quality Assessment: {quality_status}\")\n",
    "        print(f\"   \ud83d\udcdd {quality_note}\")\n",
    "        \n",
    "        # Save quality monitoring results\n",
    "        session.use_schema(\"ML_MODELS\")\n",
    "        session.sql(f\"\"\"\n",
    "            INSERT INTO PREDICTION_QUALITY_MONITORING (\n",
    "                quality_id, model_id, monitoring_date, total_predictions,\n",
    "                high_confidence_predictions, low_confidence_predictions,\n",
    "                avg_confidence_score, prediction_distribution, quality_status\n",
    "            ) VALUES (\n",
    "                '{str(uuid.uuid4())}', '{model_id}', CURRENT_TIMESTAMP(), {total_preds},\n",
    "                {high_confidence}, {low_confidence}, {avg_conf}, \n",
    "                OBJECT_CONSTRUCT(\n",
    "                    'high_risk', {high_risk}, \n",
    "                    'low_risk', {low_risk}, \n",
    "                    'positive_rate', {positive_rate},\n",
    "                    'avg_confidence', {avg_conf}\n",
    "                ),\n",
    "                '{quality_status}'\n",
    "            )\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        print(f\"\u2705 Prediction quality results saved\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No predictions found for quality analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error analyzing prediction quality: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \ud83d\udcca Creating Monitoring Dashboards\n",
    "\n",
    "Let's create SQL views that provide executive-level monitoring dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create comprehensive monitoring dashboard views\n",
    "USE SCHEMA ML_MODELS;\n",
    "\n",
    "-- Executive Model Health Dashboard\n",
    "CREATE OR REPLACE VIEW MODEL_HEALTH_DASHBOARD AS\n",
    "SELECT \n",
    "    mr.model_name,\n",
    "    mr.model_version,\n",
    "    mr.accuracy_score,\n",
    "    mr.f1_score,\n",
    "    mr.model_status,\n",
    "    mr.training_date,\n",
    "    \n",
    "    -- Drift indicators\n",
    "    (SELECT COUNT(*) FROM DATA_DRIFT_MONITORING ddm \n",
    "     WHERE ddm.model_id = mr.model_id \n",
    "     AND ddm.drift_detected = TRUE \n",
    "     AND ddm.monitoring_date > DATEADD(day, -7, CURRENT_TIMESTAMP())) as features_with_drift_7d,\n",
    "     \n",
    "    -- Quality indicators  \n",
    "    (SELECT quality_status FROM PREDICTION_QUALITY_MONITORING pqm \n",
    "     WHERE pqm.model_id = mr.model_id \n",
    "     ORDER BY pqm.monitoring_date DESC LIMIT 1) as latest_quality_status,\n",
    "     \n",
    "    -- Prediction volume\n",
    "    (SELECT total_predictions FROM PREDICTION_QUALITY_MONITORING pqm \n",
    "     WHERE pqm.model_id = mr.model_id \n",
    "     ORDER BY pqm.monitoring_date DESC LIMIT 1) as daily_prediction_volume,\n",
    "     \n",
    "    -- Overall health score\n",
    "    CASE \n",
    "        WHEN mr.f1_score >= 0.8 AND \n",
    "             (SELECT COUNT(*) FROM DATA_DRIFT_MONITORING ddm \n",
    "              WHERE ddm.model_id = mr.model_id AND ddm.drift_detected = TRUE \n",
    "              AND ddm.monitoring_date > DATEADD(day, -7, CURRENT_TIMESTAMP())) = 0\n",
    "        THEN 'HEALTHY \ud83d\udfe2'\n",
    "        WHEN mr.f1_score >= 0.6 \n",
    "        THEN 'WARNING \ud83d\udfe1'\n",
    "        ELSE 'CRITICAL \ud83d\udd34'\n",
    "    END as overall_health\n",
    "    \n",
    "FROM MODEL_REGISTRY mr\n",
    "ORDER BY mr.training_date DESC;\n",
    "\n",
    "-- View the dashboard\n",
    "SELECT * FROM MODEL_HEALTH_DASHBOARD;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "## \ud83d\udea8 Setting Up Automated Alerting\n",
    "\n",
    "Now let's create an automated alerting system for model performance issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create alerting system infrastructure\n",
    "USE SCHEMA ML_MODELS;\n",
    "\n",
    "-- Alert configuration table\n",
    "CREATE TABLE IF NOT EXISTS ALERT_CONFIGURATION (\n",
    "    alert_id VARCHAR(50),\n",
    "    alert_type VARCHAR(50),\n",
    "    metric_name VARCHAR(100),\n",
    "    threshold_value FLOAT,\n",
    "    comparison_operator VARCHAR(10),\n",
    "    notification_email VARCHAR(200),\n",
    "    is_active BOOLEAN,\n",
    "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- Insert default alert configurations\n",
    "INSERT INTO ALERT_CONFIGURATION (alert_id, alert_type, metric_name, threshold_value, comparison_operator, notification_email, is_active) VALUES\n",
    "('ALERT_001', 'PERFORMANCE', 'F1_SCORE', 0.6, '<', 'ml-team@company.com', TRUE),\n",
    "('ALERT_002', 'DRIFT', 'DRIFT_SCORE', 2.0, '>', 'ml-team@company.com', TRUE),\n",
    "('ALERT_003', 'QUALITY', 'POSITIVE_RATE', 0.9, '>', 'ml-team@company.com', TRUE),\n",
    "('ALERT_004', 'QUALITY', 'POSITIVE_RATE', 0.05, '<', 'ml-team@company.com', TRUE),\n",
    "('ALERT_005', 'VOLUME', 'DAILY_PREDICTIONS', 10, '<', 'ml-team@company.com', TRUE);\n",
    "\n",
    "-- Alert history table\n",
    "CREATE TABLE IF NOT EXISTS ALERT_HISTORY (\n",
    "    alert_history_id VARCHAR(50),\n",
    "    alert_id VARCHAR(50),\n",
    "    model_id VARCHAR(50),\n",
    "    alert_triggered_date TIMESTAMP,\n",
    "    alert_message VARCHAR(1000),\n",
    "    current_value FLOAT,\n",
    "    threshold_value FLOAT,\n",
    "    alert_status VARCHAR(20)\n",
    ");\n",
    "\n",
    "SELECT '\u2705 Alerting infrastructure created' as STATUS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create comprehensive alert checking stored procedure\n",
    "CREATE OR REPLACE PROCEDURE CHECK_MODEL_ALERTS()\n",
    "RETURNS VARCHAR\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "DECLARE\n",
    "    alert_count INTEGER := 0;\n",
    "    result_msg VARCHAR := '';\n",
    "    drift_count INTEGER := 0;\n",
    "    quality_issues INTEGER := 0;\n",
    "BEGIN\n",
    "    -- Check for performance alerts\n",
    "    SELECT COUNT(*) INTO alert_count\n",
    "    FROM MODEL_REGISTRY mr\n",
    "    JOIN ALERT_CONFIGURATION ac ON ac.metric_name = 'F1_SCORE'\n",
    "    WHERE mr.f1_score < ac.threshold_value \n",
    "    AND ac.is_active = TRUE;\n",
    "    \n",
    "    IF (alert_count > 0) THEN\n",
    "        result_msg := 'PERFORMANCE ALERT: Model F1 score below threshold. ';\n",
    "    END IF;\n",
    "    \n",
    "    -- Check for drift alerts\n",
    "    SELECT COUNT(*) INTO drift_count\n",
    "    FROM DATA_DRIFT_MONITORING ddm\n",
    "    JOIN ALERT_CONFIGURATION ac ON ac.metric_name = 'DRIFT_SCORE'\n",
    "    WHERE ddm.drift_score > ac.threshold_value\n",
    "    AND ddm.monitoring_date > DATEADD(day, -1, CURRENT_TIMESTAMP())\n",
    "    AND ac.is_active = TRUE;\n",
    "    \n",
    "    IF (drift_count > 0) THEN\n",
    "        result_msg := result_msg || 'DRIFT ALERT: ' || drift_count || ' features showing drift. ';\n",
    "    END IF;\n",
    "    \n",
    "    -- Check for prediction quality issues\n",
    "    SELECT COUNT(*) INTO quality_issues\n",
    "    FROM PREDICTION_QUALITY_MONITORING pqm\n",
    "    WHERE pqm.quality_status = 'WARNING'\n",
    "    AND pqm.monitoring_date > DATEADD(day, -1, CURRENT_TIMESTAMP());\n",
    "    \n",
    "    IF (quality_issues > 0) THEN\n",
    "        result_msg := result_msg || 'QUALITY ALERT: Prediction quality issues detected. ';\n",
    "    END IF;\n",
    "    \n",
    "    -- Final result\n",
    "    IF (result_msg = '') THEN\n",
    "        result_msg := '\u2705 All monitoring checks passed - model is healthy';\n",
    "    ELSE\n",
    "        result_msg := '\ud83d\udea8 ALERTS DETECTED: ' || result_msg;\n",
    "    END IF;\n",
    "    \n",
    "    RETURN result_msg;\n",
    "END;\n",
    "$$;\n",
    "\n",
    "-- Test the alert system\n",
    "CALL CHECK_MODEL_ALERTS();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "## \u2705 Model Observability Complete!\n",
    "\n",
    "Your comprehensive ML monitoring system is now operational:\n",
    "\n",
    "### \ud83c\udfaf **Monitoring Capabilities Deployed**\n",
    "- \u2705 **Data Drift Detection**: Statistical monitoring of feature distributions\n",
    "- \u2705 **Prediction Quality Analysis**: Pattern detection in model outputs  \n",
    "- \u2705 **Performance Tracking**: Historical model metrics and trends\n",
    "- \u2705 **Executive Dashboards**: Business-friendly health monitoring\n",
    "- \u2705 **Automated Alerting**: Proactive issue detection and notification\n",
    "\n",
    "### \ud83d\udcca **Key Monitoring Views**\n",
    "- **MODEL_HEALTH_DASHBOARD**: Executive summary of model status\n",
    "- **DATA_DRIFT_MONITORING**: Feature-level drift detection results\n",
    "- **PREDICTION_QUALITY_MONITORING**: Output quality and distribution analysis\n",
    "- **ALERT_CONFIGURATION**: Customizable alerting thresholds\n",
    "\n",
    "### \ud83d\udea8 **Alerting System**\n",
    "- **Performance Alerts**: F1 score degradation detection\n",
    "- **Drift Alerts**: Feature distribution change notifications\n",
    "- **Quality Alerts**: Prediction bias and confidence warnings\n",
    "- **Volume Alerts**: Prediction frequency monitoring\n",
    "- **Custom Thresholds**: Configurable alerting parameters\n",
    "\n",
    "### \ud83c\udfe5 **Healthcare-Specific Benefits**\n",
    "- **Patient Safety**: Early detection of model reliability issues\n",
    "- **Regulatory Compliance**: Audit trails for AI decision making\n",
    "- **Clinical Confidence**: Transparent model performance metrics\n",
    "- **Risk Management**: Proactive intervention triggers\n",
    "\n",
    "### \ud83d\udcc8 **Business Value**\n",
    "- **Reduced Downtime**: Early warning prevents model failures\n",
    "- **Trust & Compliance**: Transparent AI for healthcare regulations\n",
    "- **Continuous Improvement**: Data-driven model enhancement\n",
    "- **Operational Excellence**: Automated monitoring reduces manual effort\n",
    "\n",
    "### \ud83c\udfaf **Sample Monitoring Queries**\n",
    "```sql\n",
    "-- Check current model health\n",
    "SELECT * FROM MODEL_HEALTH_DASHBOARD;\n",
    "\n",
    "-- Review recent drift detections\n",
    "SELECT * FROM DATA_DRIFT_MONITORING \n",
    "WHERE drift_detected = TRUE \n",
    "ORDER BY monitoring_date DESC;\n",
    "\n",
    "-- Run alert checks\n",
    "CALL CHECK_MODEL_ALERTS();\n",
    "```\n",
    "\n",
    "## \ud83d\udccb Next Steps\n",
    "1. **Schedule Monitoring**: Set up automated runs of drift detection and alerting\n",
    "2. **Customize Thresholds**: Adjust alert thresholds based on business requirements\n",
    "3. **Integration**: Connect alerts to existing incident management systems\n",
    "4. **Complete Demo**: Use `08_Demo_Walkthrough` for end-to-end demonstration\n",
    "\n",
    "---\n",
    "*Comprehensive ML observability ensures reliable, trustworthy AI in production healthcare environments.*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}