{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Snowflake Container Runtime for Distributed ML\n",
    "\n",
    "This notebook sets up **true distributed training** using Snowflake's **Container Runtime** with compute pools - **no Docker builds required!**\n",
    "\n",
    "## Simplified Infrastructure (Container Runtime):\n",
    "1. **Compute Pools** - Multi-node clusters for distributed training\n",
    "2. **Pre-built Images** - Use Anaconda/public ML images via Container Runtime\n",
    "3. **Distributed Training** - Multi-node XGBoost, Ray ML, and more\n",
    "4. **Resource Management** - Auto-scaling and load balancing\n",
    "5. **Integration** - Seamless connection to Snowflake ML Registry\n",
    "\n",
    "## Prerequisites:\n",
    "- SPCS enabled in your Snowflake account\n",
    "- ACCOUNTADMIN privileges for compute pool creation\n",
    "- **No Docker needed** - uses Container Runtime!\n",
    "\n",
    "## Container Runtime Benefits:\n",
    "- **No manual Docker builds**\n",
    "- **Pre-built ML images** from Anaconda\n",
    "- **Snowflake UI configuration**\n",
    "- **Automatic dependency management**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup for SPCS\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Fix path for snowflake_connection module\n",
    "current_dir = os.getcwd()\n",
    "if \"notebooks\" in current_dir:\n",
    "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
    "else:\n",
    "    src_path = os.path.join(current_dir, \"src\")\n",
    "\n",
    "sys.path.append(src_path)\n",
    "print(f\"Added to Python path: {src_path}\")\n",
    "\n",
    "from snowflake_connection import get_session\n",
    "\n",
    "# Get Snowflake session with admin privileges\n",
    "session = get_session()\n",
    "print(\"SUCCESS: Snowflake connection established for SPCS setup\")\n",
    "print(\"Ready to configure distributed training infrastructure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Compute Pools for Distributed Training\n",
    "print(\"Setting up compute pools for distributed ML training...\")\n",
    "\n",
    "# Create GPU-enabled compute pool for intensive ML workloads\n",
    "gpu_pool_sql = \"\"\"\n",
    "CREATE COMPUTE POOL IF NOT EXISTS ML_DISTRIBUTED_GPU_POOL\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 8\n",
    "INSTANCE_FAMILY = GPU_NV_S\n",
    "AUTO_RESUME = TRUE\n",
    "AUTO_SUSPEND_SECS = 300\n",
    "COMMENT = 'GPU compute pool for distributed ML training with Ray/XGBoost'\n",
    "\"\"\"\n",
    "\n",
    "# Create CPU compute pool for general distributed training\n",
    "cpu_pool_sql = \"\"\"\n",
    "CREATE COMPUTE POOL IF NOT EXISTS ML_DISTRIBUTED_CPU_POOL\n",
    "MIN_NODES = 2\n",
    "MAX_NODES = 16\n",
    "INSTANCE_FAMILY = CPU_X64_S\n",
    "AUTO_RESUME = TRUE\n",
    "AUTO_SUSPEND_SECS = 300\n",
    "COMMENT = 'CPU compute pool for distributed ML training and data processing'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Execute compute pool creation\n",
    "    session.sql(gpu_pool_sql).collect()\n",
    "    print(\"SUCCESS: GPU compute pool created: ML_DISTRIBUTED_GPU_POOL\")\n",
    "    \n",
    "    session.sql(cpu_pool_sql).collect()\n",
    "    print(\"SUCCESS: CPU compute pool created: ML_DISTRIBUTED_CPU_POOL\")\n",
    "    \n",
    "    # List compute pools\n",
    "    pools = session.sql(\"SHOW COMPUTE POOLS\").collect()\n",
    "    print(f\"\\nAvailable compute pools ({len(pools)} total):\")\n",
    "    for pool in pools:\n",
    "        if 'ML_DISTRIBUTED' in pool['name']:\n",
    "            print(f\"   - {pool['name']} - {pool['instance_family']} ({pool['state']})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Compute pool setup error: {e}\")\n",
    "    print(\"Note: SPCS requires ACCOUNTADMIN privileges and may need to be enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Verify Compute Pool Setup for Distributed Training\n",
    "print(\"Compute pools are ready for distributed ML training!\")\n",
    "\n",
    "# Check compute pool status\n",
    "try:\n",
    "    pools = session.sql(\"SHOW COMPUTE POOLS\").collect()\n",
    "    print(\"\\nAvailable compute pools for distributed training:\")\n",
    "    \n",
    "    ml_pools = []\n",
    "    for pool in pools:\n",
    "        if 'ML_DISTRIBUTED' in pool['name']:\n",
    "            ml_pools.append(pool)\n",
    "            print(f\"   - {pool['name']}\")\n",
    "            print(f\"      Instance Family: {pool['instance_family']}\")\n",
    "            print(f\"      State: {pool['state']}\")\n",
    "            print(f\"      Min/Max Nodes: {pool['min_nodes']}/{pool['max_nodes']}\")\n",
    "            print()\n",
    "    \n",
    "    if len(ml_pools) >= 1:\n",
    "        print(\"SUCCESS: Compute pools ready for distributed training!\")\n",
    "        print(\"Snowflake ML APIs will automatically use these pools\")\n",
    "        print(\"No additional container setup needed\")\n",
    "    else:\n",
    "        print(\"WARNING: No ML compute pools found - check previous cell\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Error checking compute pools: {e}\")\n",
    "\n",
    "print(\"\\nReady for Distributed Training!\")\n",
    "print(\"What's set up:\")\n",
    "print(\"   - Multi-node compute pools (CPU + GPU)\")\n",
    "print(\"   - Auto-scaling from 1-16 nodes\")  \n",
    "print(\"   - Direct integration with Snowflake ML APIs\")\n",
    "print(\"   - Native distributed XGBoost support\")\n",
    "print(\"\\nNext: Run 05b_True_Distributed_Training.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Distributed ML Setup Complete!\n",
    "\n",
    "### What We Set Up (Simple & Clean):\n",
    "\n",
    "1. **Compute Pools Only**\n",
    "   - `ML_DISTRIBUTED_GPU_POOL` - GPU instances for intensive ML\n",
    "   - `ML_DISTRIBUTED_CPU_POOL` - CPU cluster for distributed training\n",
    "   - **Auto-scaling** from 1-16 nodes as needed\n",
    "\n",
    "2. **Native Snowflake ML Integration**\n",
    "   - **No containers needed** - Snowflake ML APIs use compute pools directly\n",
    "   - **No Docker complexity** - everything works natively\n",
    "   - **Built-in distributed training** - XGBoost, scikit-learn, etc.\n",
    "\n",
    "### Why This Approach Works Better:\n",
    "\n",
    "- **Simplest setup** - just compute pools\n",
    "- **Native ML APIs** - Snowflake handles distribution automatically  \n",
    "- **No container management** - focus on ML, not infrastructure\n",
    "- **Enterprise ready** - integrated with ML Registry and Feature Store\n",
    "\n",
    "### Next Step: Distributed Training\n",
    "\n",
    "**Ready for notebook `05b_True_Distributed_Training.ipynb`!**\n",
    "\n",
    "The Snowflake ML APIs will automatically:\n",
    "- **Use the compute pools** for distributed training\n",
    "- **Scale across multiple nodes** (2-16 nodes)\n",
    "- **Handle data distribution** automatically\n",
    "- **Integrate results** back to ML Registry\n",
    "\n",
    "### Verify Setup:\n",
    "- Check **Snowflake UI > Admin > Compute Pools**\n",
    "- Look for `ML_DISTRIBUTED_CPU_POOL` and `ML_DISTRIBUTED_GPU_POOL`\n",
    "- Status should show \"ACTIVE\" or \"IDLE\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}