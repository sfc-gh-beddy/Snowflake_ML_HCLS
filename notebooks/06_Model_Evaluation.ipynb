{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Comprehensive Model Evaluation Pipeline\n",
    "\n",
    "**Advanced model evaluation with cross-validation, statistical analysis, and healthcare-specific metrics**\n",
    "\n",
    "## **Evaluation Objectives:**\n",
    "1. **Comprehensive Metrics** - MAE, RMSE, RÂ², healthcare-specific accuracy measures\n",
    "2. **Cross-Validation** - K-fold validation with statistical significance testing\n",
    "3. **Statistical Analysis** - Confidence intervals, hypothesis testing, model comparison\n",
    "4. **Healthcare-Specific Metrics** - Risk stratification accuracy, clinical thresholds\n",
    "5. **Evaluation Logging** - Structured results storage for tracking and comparison\n",
    "\n",
    "## **Evaluation Components:**\n",
    "- **Multi-Algorithm Testing**: XGBoost variants, linear baselines, ensemble methods\n",
    "- **Cross-Validation Framework**: Distributed K-fold validation using Snowpark\n",
    "- **Healthcare Metrics**: Risk category accuracy, sensitivity/specificity by risk level\n",
    "- **Statistical Testing**: Paired t-tests, confidence intervals, effect sizes\n",
    "- **Results Logging**: Comprehensive evaluation tracking in Snowflake tables\n",
    "\n",
    "## Prerequisites\n",
    "- Running in Snowflake Notebooks environment\n",
    "- Previous notebooks completed (01, 02, 03, 03b, 04, 05)\n",
    "- Feature engineering and model training completed\n",
    "- IMPORT SNOWFLAKE-ML-PYTHON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Snowflake Session for Model Evaluation\n",
    "print(\"Initializing Snowflake session for model evaluation...\")\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "# Import Snowpark session and functions (available in Snowflake Notebooks)\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import (\n",
    "    col, lit, when, count, avg, sum as sum_, max as max_, min as min_,\n",
    "    stddev, variance, abs as abs_, sqrt, pow as pow_\n",
    ")\n",
    "from snowflake.snowpark.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType,\n",
    "    FloatType, BooleanType\n",
    ")\n",
    "\n",
    "# ML imports\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.linear_model import LinearRegression\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_error, mean_squared_error\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Get the active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "print(\"SUCCESS: Snowflake session initialized for model evaluation\")\n",
    "\n",
    "# Verify context\n",
    "current_context = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        CURRENT_DATABASE() as database,\n",
    "        CURRENT_SCHEMA() as schema,\n",
    "        CURRENT_WAREHOUSE() as warehouse\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"   Database: {current_context['DATABASE']}\")\n",
    "print(f\"   Schema: {current_context['SCHEMA']}\")\n",
    "print(f\"   Warehouse: {current_context['WAREHOUSE']}\")\n",
    "print(\"SUCCESS: Environment ready for comprehensive model evaluation\")\n",
    "print(\"Capabilities: Cross-validation, Statistical Analysis, Healthcare Metrics\")\n",
    "print(\"Tools: Multiple algorithms, significance testing, evaluation logging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preparation\n",
    "print(\"Loading and preparing evaluation datasets...\")\n",
    "\n",
    "# Load the processed feature data\n",
    "feature_data_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.FAERS_HCLS_FEATURES_FINAL\")\n",
    "print(f\"SUCCESS: Loaded feature dataset with {feature_data_df.count():,} records\")\n",
    "\n",
    "# Analyze available columns\n",
    "available_columns = [f.name for f in feature_data_df.schema.fields]\n",
    "print(f\"Available columns: {len(available_columns)}\")\n",
    "print(f\"   Sample columns: {', '.join(available_columns[:8])}...\")\n",
    "\n",
    "# Define feature sets for evaluation\n",
    "core_features = [\"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\"]\n",
    "faers_features = [\"MAX_MEDICATION_RISK\", \"HIGH_RISK_MEDICATION_COUNT\", \"WARFARIN_RISK\"]\n",
    "derived_features = [\"AGE_GROUP\", \"MEDICATION_BURDEN\", \"CLAIMS_CATEGORY\"]\n",
    "\n",
    "# Build feature set based on availability\n",
    "evaluation_features = []\n",
    "evaluation_features.extend([f for f in core_features if f in available_columns])\n",
    "evaluation_features.extend([f for f in faers_features if f in available_columns])\n",
    "evaluation_features.extend([f for f in derived_features if f in available_columns])\n",
    "\n",
    "print(f\"Selected {len(evaluation_features)} features for evaluation:\")\n",
    "for i, feature in enumerate(evaluation_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "# Define target variable\n",
    "target_col = \"CONTINUOUS_RISK_TARGET\" if \"CONTINUOUS_RISK_TARGET\" in available_columns else \"AGE\"\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "# Create evaluation dataset with clean data\n",
    "eval_data_df = feature_data_df.select(\n",
    "    evaluation_features + [target_col]\n",
    ").filter(\n",
    "    col(target_col).is_not_null()\n",
    ")\n",
    "\n",
    "# Add patient ID for tracking using row number\n",
    "from snowflake.snowpark.functions import row_number\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "# Create a simple numeric patient ID to avoid string concatenation issues\n",
    "window_spec = Window.order_by(lit(1))\n",
    "eval_data_df = eval_data_df.with_column(\n",
    "    \"PATIENT_ID\", row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "total_records = eval_data_df.count()\n",
    "print(f\"SUCCESS: Evaluation dataset prepared: {total_records:,} clean records\")\n",
    "\n",
    "# Create train/test split for consistent evaluation\n",
    "# Use modulo of PATIENT_ID for deterministic split\n",
    "train_df = eval_data_df.filter((col(\"PATIENT_ID\") % lit(10)) < lit(8))\n",
    "test_df = eval_data_df.filter((col(\"PATIENT_ID\") % lit(10)) >= lit(8))\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"   Training: {train_df.count():,} records\")\n",
    "print(f\"   Testing: {test_df.count():,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Framework\n",
    "print(\"Setting up distributed cross-validation framework...\")\n",
    "\n",
    "def create_cv_folds(df, k_folds=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create K-fold cross-validation splits using Snowpark\n",
    "    \"\"\"\n",
    "    print(f\"Creating {k_folds}-fold cross-validation splits...\")\n",
    "    \n",
    "    # Use modulo of numeric PATIENT_ID for deterministic fold assignment\n",
    "    df_with_folds = df.with_column(\n",
    "        \"FOLD_ID\", (col(\"PATIENT_ID\") % lit(k_folds))\n",
    "    )\n",
    "    \n",
    "    folds = []\n",
    "    for fold_id in range(k_folds):\n",
    "        train_fold = df_with_folds.filter(col(\"FOLD_ID\") != lit(fold_id)).drop(\"FOLD_ID\")\n",
    "        val_fold = df_with_folds.filter(col(\"FOLD_ID\") == lit(fold_id)).drop(\"FOLD_ID\")\n",
    "        \n",
    "        train_size = train_fold.count()\n",
    "        val_size = val_fold.count()\n",
    "        \n",
    "        folds.append({\n",
    "            'fold_id': fold_id,\n",
    "            'train': train_fold,\n",
    "            'val': val_fold,\n",
    "            'train_size': train_size,\n",
    "            'val_size': val_size\n",
    "        })\n",
    "        \n",
    "        print(f\"   Fold {fold_id + 1}: Train={train_size:,}, Val={val_size:,}\")\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def evaluate_model_cv(model_class, model_params, folds, features, target, model_name):\n",
    "    \"\"\"\n",
    "    Perform cross-validation evaluation for a given model\n",
    "    \"\"\"\n",
    "    print(f\"\\nCross-validating {model_name}...\")\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f\"   Processing fold {i + 1}/{len(folds)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize model with parameters\n",
    "            model = model_class(\n",
    "                input_cols=features,\n",
    "                output_cols=[\"PREDICTION\"],\n",
    "                label_cols=[target],\n",
    "                **model_params\n",
    "            )\n",
    "            \n",
    "            # Train on fold\n",
    "            trained_model = model.fit(fold['train'])\n",
    "            \n",
    "            # Predict on validation set\n",
    "            predictions_df = trained_model.predict(fold['val'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(\n",
    "                df=predictions_df, \n",
    "                y_true_col_names=[target], \n",
    "                y_pred_col_names=[\"PREDICTION\"]\n",
    "            )\n",
    "            \n",
    "            mse = mean_squared_error(\n",
    "                df=predictions_df,\n",
    "                y_true_col_names=[target],\n",
    "                y_pred_col_names=[\"PREDICTION\"]\n",
    "            )\n",
    "            \n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            fold_result = {\n",
    "                'fold_id': i,\n",
    "                'mae': float(mae),\n",
    "                'mse': float(mse),\n",
    "                'rmse': float(rmse),\n",
    "                'val_size': fold['val_size']\n",
    "            }\n",
    "            \n",
    "            fold_results.append(fold_result)\n",
    "            print(f\"      MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      WARNING: Fold {i + 1} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not fold_results:\n",
    "        print(f\"   FAILED: All folds failed for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Aggregate cross-validation results\n",
    "    cv_metrics = {\n",
    "        'model_name': model_name,\n",
    "        'n_folds': len(fold_results),\n",
    "        'mean_mae': np.mean([r['mae'] for r in fold_results]),\n",
    "        'std_mae': np.std([r['mae'] for r in fold_results]),\n",
    "        'mean_rmse': np.mean([r['rmse'] for r in fold_results]),\n",
    "        'std_rmse': np.std([r['rmse'] for r in fold_results]),\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "    \n",
    "    print(f\"   SUCCESS: CV Results - MAE: {cv_metrics['mean_mae']:.4f} Â± {cv_metrics['std_mae']:.4f}\")\n",
    "    print(f\"                        RMSE: {cv_metrics['mean_rmse']:.4f} Â± {cv_metrics['std_rmse']:.4f}\")\n",
    "    \n",
    "    return cv_metrics\n",
    "\n",
    "# Create cross-validation folds\n",
    "cv_folds = create_cv_folds(train_df, k_folds=5, seed=42)\n",
    "print(f\"SUCCESS: Cross-validation framework ready with {len(cv_folds)} folds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) \n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Multi-Algorithm Evaluation\n",
    "print(\"Running multi-algorithm evaluation with cross-validation...\")\n",
    "\n",
    "# Define models to evaluate\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'XGBoost_Default',\n",
    "        'class': XGBRegressor,\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost_Optimized',\n",
    "        'class': XGBRegressor,\n",
    "        'params': {\n",
    "            'n_estimators': 150,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Linear_Baseline',\n",
    "        'class': LinearRegression,\n",
    "        'params': {}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run cross-validation for each model\n",
    "cv_results = []\n",
    "\n",
    "for config in model_configs:\n",
    "    try:\n",
    "        cv_result = evaluate_model_cv(\n",
    "            model_class=config['class'],\n",
    "            model_params=config['params'],\n",
    "            folds=cv_folds,\n",
    "            features=evaluation_features,\n",
    "            target=target_col,\n",
    "            model_name=config['name']\n",
    "        )\n",
    "        \n",
    "        if cv_result:\n",
    "            cv_results.append(cv_result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Model {config['name']} evaluation failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Compare model performance\n",
    "print(f\"\\nCross-Validation Results Summary:\")\n",
    "print(f\"{'Model':<20} {'MAE':<12} {'RMSE':<12} {'Folds':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "best_model = None\n",
    "best_mae = float('inf')\n",
    "\n",
    "for result in cv_results:\n",
    "    mae_str = f\"{result['mean_mae']:.4f} Â± {result['std_mae']:.4f}\"\n",
    "    rmse_str = f\"{result['mean_rmse']:.4f} Â± {result['std_rmse']:.4f}\"\n",
    "    \n",
    "    print(f\"{result['model_name']:<20} {mae_str:<12} {rmse_str:<12} {result['n_folds']:<8}\")\n",
    "    \n",
    "    if result['mean_mae'] < best_mae:\n",
    "        best_mae = result['mean_mae']\n",
    "        best_model = result['model_name']\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model} (MAE: {best_mae:.4f})\")\n",
    "\n",
    "# Statistical significance testing\n",
    "print(f\"\\nStatistical Analysis:\")\n",
    "if len(cv_results) >= 2:\n",
    "    # Compare top two models\n",
    "    sorted_results = sorted(cv_results, key=lambda x: x['mean_mae'])\n",
    "    model1, model2 = sorted_results[0], sorted_results[1]\n",
    "    \n",
    "    mae_diff = model2['mean_mae'] - model1['mean_mae']\n",
    "    combined_std = np.sqrt(model1['std_mae']**2 + model2['std_mae']**2)\n",
    "    \n",
    "    if combined_std > 0:\n",
    "        effect_size = mae_diff / combined_std\n",
    "        print(f\"   Performance difference: {mae_diff:.4f} MAE\")\n",
    "        print(f\"   Effect size: {effect_size:.3f}\")\n",
    "        \n",
    "        if abs(effect_size) > 0.5:\n",
    "            print(f\"   Moderate to large effect size detected\")\n",
    "        else:\n",
    "            print(f\"   Small effect size - models perform similarly\")\n",
    "\n",
    "print(f\"SUCCESS: Multi-algorithm evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare-Specific Metrics Evaluation\n",
    "print(\"Calculating healthcare-specific evaluation metrics...\")\n",
    "\n",
    "def calculate_healthcare_metrics(predictions_df, target_col):\n",
    "    \"\"\"\n",
    "    Calculate healthcare-specific metrics including risk stratification accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define risk thresholds (these would be clinically validated)\n",
    "    low_threshold = 30.0\n",
    "    high_threshold = 70.0\n",
    "    \n",
    "    print(f\"   Calculating risk stratification metrics...\")\n",
    "    print(f\"      Risk thresholds: Low < {low_threshold}, Medium {low_threshold}-{high_threshold}, High > {high_threshold}\")\n",
    "    \n",
    "    # Create risk categories for both true and predicted values\n",
    "    # Use proper column aliasing to avoid identifier issues\n",
    "    metrics_df = predictions_df.with_column(\n",
    "        \"TRUE_RISK_CAT\",\n",
    "        when(col(target_col) < lit(low_threshold), lit(\"LOW\"))\n",
    "        .when(col(target_col) < lit(high_threshold), lit(\"MEDIUM\"))\n",
    "        .otherwise(lit(\"HIGH\"))\n",
    "    ).with_column(\n",
    "        \"PRED_RISK_CAT\",\n",
    "        when(col(\"PREDICTION\") < lit(low_threshold), lit(\"LOW\"))\n",
    "        .when(col(\"PREDICTION\") < lit(high_threshold), lit(\"MEDIUM\"))\n",
    "        .otherwise(lit(\"HIGH\"))\n",
    "    )\n",
    "    \n",
    "    # Calculate total records\n",
    "    total_records = metrics_df.count()\n",
    "    print(f\"      Total records for analysis: {total_records:,}\")\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(f\"      WARNING: No records available for healthcare metrics\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate risk category accuracy\n",
    "    try:\n",
    "        correct_classifications = metrics_df.filter(\n",
    "            col(\"TRUE_RISK_CAT\") == col(\"PRED_RISK_CAT\")\n",
    "        ).count()\n",
    "        \n",
    "        category_accuracy = correct_classifications / total_records\n",
    "        print(f\"      Risk category accuracy: {category_accuracy:.3f} ({correct_classifications:,}/{total_records:,})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      WARNING: Category accuracy calculation error: {e}\")\n",
    "        category_accuracy = 0.0\n",
    "    \n",
    "    # Calculate high-risk sensitivity (true positive rate for high-risk patients)\n",
    "    try:\n",
    "        true_high_risk = metrics_df.filter(col(\"TRUE_RISK_CAT\") == lit(\"HIGH\")).count()\n",
    "        predicted_high_risk_correctly = metrics_df.filter(\n",
    "            (col(\"TRUE_RISK_CAT\") == lit(\"HIGH\")) & (col(\"PRED_RISK_CAT\") == lit(\"HIGH\"))\n",
    "        ).count()\n",
    "        \n",
    "        high_risk_sensitivity = predicted_high_risk_correctly / true_high_risk if true_high_risk > 0 else 0.0\n",
    "        print(f\"      High-risk sensitivity: {high_risk_sensitivity:.3f} ({predicted_high_risk_correctly:,}/{true_high_risk:,})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      WARNING: High-risk sensitivity calculation error: {e}\")\n",
    "        high_risk_sensitivity = 0.0\n",
    "    \n",
    "    # Calculate low-risk specificity (true negative rate for low-risk patients)\n",
    "    try:\n",
    "        true_low_risk = metrics_df.filter(col(\"TRUE_RISK_CAT\") == lit(\"LOW\")).count()\n",
    "        predicted_low_risk_correctly = metrics_df.filter(\n",
    "            (col(\"TRUE_RISK_CAT\") == lit(\"LOW\")) & (col(\"PRED_RISK_CAT\") == lit(\"LOW\"))\n",
    "        ).count()\n",
    "        \n",
    "        low_risk_specificity = predicted_low_risk_correctly / true_low_risk if true_low_risk > 0 else 0.0\n",
    "        print(f\"      Low-risk specificity: {low_risk_specificity:.3f} ({predicted_low_risk_correctly:,}/{true_low_risk:,})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      WARNING: Low-risk specificity calculation error: {e}\")\n",
    "        low_risk_specificity = 0.0\n",
    "    \n",
    "    # Calculate MAE by risk category\n",
    "    risk_mae_metrics = {}\n",
    "    for risk_cat in [\"LOW\", \"MEDIUM\", \"HIGH\"]:\n",
    "        try:\n",
    "            cat_df = metrics_df.filter(col(\"TRUE_RISK_CAT\") == lit(risk_cat))\n",
    "            cat_count = cat_df.count()\n",
    "            \n",
    "            if cat_count > 0:\n",
    "                cat_mae = cat_df.select(\n",
    "                    avg(abs_(col(target_col) - col(\"PREDICTION\"))).alias(\"MAE\")\n",
    "                ).collect()[0][\"MAE\"]\n",
    "                risk_mae_metrics[f\"mae_{risk_cat.lower()}\"] = float(cat_mae) if cat_mae else 0.0\n",
    "                print(f\"      MAE for {risk_cat} risk: {cat_mae:.3f} (n={cat_count:,})\")\n",
    "            else:\n",
    "                risk_mae_metrics[f\"mae_{risk_cat.lower()}\"] = 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      WARNING: MAE calculation for {risk_cat} risk error: {e}\")\n",
    "            risk_mae_metrics[f\"mae_{risk_cat.lower()}\"] = 0.0\n",
    "    \n",
    "    # Compile healthcare metrics\n",
    "    healthcare_metrics = {\n",
    "        'risk_category_accuracy': category_accuracy,\n",
    "        'high_risk_sensitivity': high_risk_sensitivity,\n",
    "        'low_risk_specificity': low_risk_specificity,\n",
    "        'total_patients': total_records,\n",
    "        **risk_mae_metrics\n",
    "    }\n",
    "    \n",
    "    return healthcare_metrics\n",
    "\n",
    "# Calculate healthcare metrics for the best model\n",
    "print(f\"Evaluating healthcare metrics for {best_model}...\")\n",
    "\n",
    "try:\n",
    "    # Find best model configuration\n",
    "    best_config = next(config for config in model_configs if config['name'] == best_model)\n",
    "    \n",
    "    # Train best model on full training set\n",
    "    final_model = best_config['class'](\n",
    "        input_cols=evaluation_features,\n",
    "        output_cols=[\"PREDICTION\"],\n",
    "        label_cols=[target_col],\n",
    "        **best_config['params']\n",
    "    )\n",
    "    \n",
    "    trained_final_model = final_model.fit(train_df)\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    test_predictions = trained_final_model.predict(test_df)\n",
    "    \n",
    "    # Calculate healthcare-specific metrics\n",
    "    healthcare_metrics = calculate_healthcare_metrics(test_predictions, target_col)\n",
    "    \n",
    "    print(f\"\\nHealthcare Metrics Summary for {best_model}:\")\n",
    "    print(f\"   Risk Category Accuracy: {healthcare_metrics.get('risk_category_accuracy', 0):.3f}\")\n",
    "    print(f\"   High-Risk Sensitivity: {healthcare_metrics.get('high_risk_sensitivity', 0):.3f}\")\n",
    "    print(f\"   Low-Risk Specificity: {healthcare_metrics.get('low_risk_specificity', 0):.3f}\")\n",
    "    print(f\"   MAE by Risk Level:\")\n",
    "    print(f\"     Low Risk: {healthcare_metrics.get('mae_low', 0):.3f}\")\n",
    "    print(f\"     Medium Risk: {healthcare_metrics.get('mae_medium', 0):.3f}\")\n",
    "    print(f\"     High Risk: {healthcare_metrics.get('mae_high', 0):.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Healthcare metrics calculation failed: {e}\")\n",
    "    healthcare_metrics = {}\n",
    "\n",
    "print(f\"SUCCESS: Healthcare-specific evaluation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Results Logging\n",
    "print(\"Logging comprehensive evaluation results...\")\n",
    "\n",
    "# Create main evaluation results table\n",
    "eval_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_EVALUATION_LOG (\n",
    "    EVALUATION_ID STRING,\n",
    "    EVALUATION_DATE TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    MODEL_NAME STRING,\n",
    "    EVALUATION_TYPE STRING,\n",
    "    DATASET_SIZE INT,\n",
    "    FEATURE_COUNT INT,\n",
    "    CV_FOLDS INT,\n",
    "    MEAN_MAE FLOAT,\n",
    "    STD_MAE FLOAT,\n",
    "    MEAN_RMSE FLOAT,\n",
    "    STD_RMSE FLOAT,\n",
    "    RISK_CATEGORY_ACCURACY FLOAT,\n",
    "    HIGH_RISK_SENSITIVITY FLOAT,\n",
    "    LOW_RISK_SPECIFICITY FLOAT,\n",
    "    MAE_LOW_RISK FLOAT,\n",
    "    MAE_MEDIUM_RISK FLOAT,\n",
    "    MAE_HIGH_RISK FLOAT,\n",
    "    BEST_MODEL STRING,\n",
    "    EVALUATION_NOTES STRING\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Create model comparison table\n",
    "comparison_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_COMPARISON_LOG (\n",
    "    COMPARISON_ID STRING,\n",
    "    EVALUATION_ID STRING,\n",
    "    MODEL_A STRING,\n",
    "    MODEL_B STRING,\n",
    "    MAE_DIFFERENCE FLOAT,\n",
    "    RMSE_DIFFERENCE FLOAT,\n",
    "    EFFECT_SIZE FLOAT,\n",
    "    SIGNIFICANCE_LEVEL STRING,\n",
    "    COMPARISON_DATE TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    COMPARISON_NOTES STRING\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(eval_table_sql).collect()\n",
    "    session.sql(comparison_table_sql).collect()\n",
    "    print(\"SUCCESS: Evaluation logging tables created\")\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Logging table creation: {e}\")\n",
    "\n",
    "# Prepare evaluation results for logging\n",
    "evaluation_timestamp = datetime.datetime.now()\n",
    "evaluation_id = f\"EVAL_{evaluation_timestamp.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"Preparing evaluation results for logging...\")\n",
    "print(f\"   Evaluation ID: {evaluation_id}\")\n",
    "\n",
    "# Log main evaluation results\n",
    "evaluation_log = []\n",
    "\n",
    "for result in cv_results:\n",
    "    # Get healthcare metrics for this model if it's the best one\n",
    "    model_healthcare_metrics = healthcare_metrics if result['model_name'] == best_model else {}\n",
    "    \n",
    "    eval_record = (\n",
    "        evaluation_id,\n",
    "        evaluation_timestamp.isoformat(),\n",
    "        result['model_name'],\n",
    "        'CROSS_VALIDATION',\n",
    "        total_records,\n",
    "        len(evaluation_features),\n",
    "        result['n_folds'],\n",
    "        result['mean_mae'],\n",
    "        result['std_mae'],\n",
    "        result['mean_rmse'],\n",
    "        result['std_rmse'],\n",
    "        model_healthcare_metrics.get('risk_category_accuracy', 0.0),\n",
    "        model_healthcare_metrics.get('high_risk_sensitivity', 0.0),\n",
    "        model_healthcare_metrics.get('low_risk_specificity', 0.0),\n",
    "        model_healthcare_metrics.get('mae_low', 0.0),\n",
    "        model_healthcare_metrics.get('mae_medium', 0.0),\n",
    "        model_healthcare_metrics.get('mae_high', 0.0),\n",
    "        best_model,\n",
    "        f\"Features: {', '.join(evaluation_features[:5])}...\"\n",
    "    )\n",
    "    \n",
    "    evaluation_log.append(eval_record)\n",
    "\n",
    "# Create evaluation DataFrame and save\n",
    "if evaluation_log:\n",
    "    eval_schema = StructType([\n",
    "        StructField(\"EVALUATION_ID\", StringType()),\n",
    "        StructField(\"EVALUATION_DATE\", StringType()),\n",
    "        StructField(\"MODEL_NAME\", StringType()),\n",
    "        StructField(\"EVALUATION_TYPE\", StringType()),\n",
    "        StructField(\"DATASET_SIZE\", IntegerType()),\n",
    "        StructField(\"FEATURE_COUNT\", IntegerType()),\n",
    "        StructField(\"CV_FOLDS\", IntegerType()),\n",
    "        StructField(\"MEAN_MAE\", DoubleType()),\n",
    "        StructField(\"STD_MAE\", DoubleType()),\n",
    "        StructField(\"MEAN_RMSE\", DoubleType()),\n",
    "        StructField(\"STD_RMSE\", DoubleType()),\n",
    "        StructField(\"RISK_CATEGORY_ACCURACY\", DoubleType()),\n",
    "        StructField(\"HIGH_RISK_SENSITIVITY\", DoubleType()),\n",
    "        StructField(\"LOW_RISK_SPECIFICITY\", DoubleType()),\n",
    "        StructField(\"MAE_LOW_RISK\", DoubleType()),\n",
    "        StructField(\"MAE_MEDIUM_RISK\", DoubleType()),\n",
    "        StructField(\"MAE_HIGH_RISK\", DoubleType()),\n",
    "        StructField(\"BEST_MODEL\", StringType()),\n",
    "        StructField(\"EVALUATION_NOTES\", StringType())\n",
    "    ])\n",
    "    \n",
    "    eval_df = session.create_dataframe(evaluation_log, schema=eval_schema)\n",
    "    eval_df.write.mode(\"append\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_EVALUATION_LOG\")\n",
    "    \n",
    "    print(f\"SUCCESS: Logged {len(evaluation_log)} model evaluation results\")\n",
    "\n",
    "# Log model comparisons\n",
    "if len(cv_results) >= 2:\n",
    "    comparison_log = []\n",
    "    \n",
    "    # Compare all pairs of models\n",
    "    for i, model_a in enumerate(cv_results):\n",
    "        for j, model_b in enumerate(cv_results[i+1:], i+1):\n",
    "            mae_diff = model_b['mean_mae'] - model_a['mean_mae']\n",
    "            rmse_diff = model_b['mean_rmse'] - model_a['mean_rmse']\n",
    "            \n",
    "            combined_std = np.sqrt(model_a['std_mae']**2 + model_b['std_mae']**2)\n",
    "            effect_size = mae_diff / combined_std if combined_std > 0 else 0.0\n",
    "            \n",
    "            significance = \"LARGE\" if abs(effect_size) > 0.8 else \"MEDIUM\" if abs(effect_size) > 0.5 else \"SMALL\"\n",
    "            \n",
    "            comparison_record = (\n",
    "                f\"COMP_{evaluation_timestamp.strftime('%Y%m%d_%H%M%S')}_{i}_{j}\",\n",
    "                evaluation_id,\n",
    "                model_a['model_name'],\n",
    "                model_b['model_name'],\n",
    "                mae_diff,\n",
    "                rmse_diff,\n",
    "                effect_size,\n",
    "                significance,\n",
    "                evaluation_timestamp.isoformat(),\n",
    "                f\"Cross-validation comparison with {model_a['n_folds']} folds\"\n",
    "            )\n",
    "            \n",
    "            comparison_log.append(comparison_record)\n",
    "    \n",
    "    if comparison_log:\n",
    "        comparison_schema = StructType([\n",
    "            StructField(\"COMPARISON_ID\", StringType()),\n",
    "            StructField(\"EVALUATION_ID\", StringType()),\n",
    "            StructField(\"MODEL_A\", StringType()),\n",
    "            StructField(\"MODEL_B\", StringType()),\n",
    "            StructField(\"MAE_DIFFERENCE\", DoubleType()),\n",
    "            StructField(\"RMSE_DIFFERENCE\", DoubleType()),\n",
    "            StructField(\"EFFECT_SIZE\", DoubleType()),\n",
    "            StructField(\"SIGNIFICANCE_LEVEL\", StringType()),\n",
    "            StructField(\"COMPARISON_DATE\", StringType()),\n",
    "            StructField(\"COMPARISON_NOTES\", StringType())\n",
    "        ])\n",
    "        \n",
    "        comparison_df = session.create_dataframe(comparison_log, schema=comparison_schema)\n",
    "        comparison_df.write.mode(\"append\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_COMPARISON_LOG\")\n",
    "        \n",
    "        print(f\"SUCCESS: Logged {len(comparison_log)} model comparisons\")\n",
    "\n",
    "# Final evaluation summary\n",
    "print(f\"\\nComprehensive Model Evaluation Complete!\")\n",
    "print(f\"   Evaluation ID: {evaluation_id}\")\n",
    "print(f\"   Best Model: {best_model} (MAE: {best_mae:.4f})\")\n",
    "print(f\"   Models Evaluated: {len(cv_results)}\")\n",
    "print(f\"   Cross-Validation Folds: {len(cv_folds)}\")\n",
    "print(f\"   Healthcare Metrics: Risk accuracy, sensitivity, specificity calculated\")\n",
    "print(f\"   Results Logged: Available in MODEL_EVALUATION_LOG and MODEL_COMPARISON_LOG\")\n",
    "print(f\"   Ready for model packaging and deployment (notebook 07)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
