{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ Comprehensive Snowflake ML Workflow (Elastic Compute + Feature Store)\n",
        "\n",
        "Complete end-to-end ML workflow with **proper Feature Store management**, elastic compute, unsupervised/supervised learning, and observability.\n",
        "\n",
        "> ‚ö° **This notebook uses elastic compute** (scalable warehouse resources)\n",
        "> \n",
        "> üí° **For TRUE distributed training across multiple compute nodes**, see:\n",
        "> - `05a_SPCS_Distributed_Setup.ipynb` - SPCS infrastructure setup  \n",
        "> - `05b_True_Distributed_Training.ipynb` - Multi-node Ray cluster training\n",
        "\n",
        "**Comprehensive ML Pipeline:**\n",
        "1. üè™ **Feature Store Management** - Proper feature entity registration and serving\n",
        "2. üîç **Unsupervised Learning** - Clustering and anomaly detection\n",
        "3. üéØ **Supervised Learning** - XGBoost regression with elastic compute\n",
        "4. üì¶ **Model Registry** - Log and version all models\n",
        "5. ‚ö° **Scalable Inference** - Batch processing on elastic compute\n",
        "6. üìä **ML Observability** - Native monitoring and drift detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Added to Python path: /Users/beddy/Desktop/Github/Snowflake_ML_HCLS/notebooks/../src\n",
            "üîÑ Reusing existing Snowflake session\n",
            "‚úÖ Environment ready for comprehensive Snowflake ML workflow\n",
            "üèóÔ∏è Capabilities: Elastic Compute, Unsupervised/Supervised ML, Model Registry, Observability\n",
            "\n",
            "üí° For TRUE DISTRIBUTED TRAINING across multiple nodes:\n",
            "   üìö Run notebook: 05a_SPCS_Distributed_Setup.ipynb\n",
            "   üöÄ Then run: 05b_True_Distributed_Training.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup for Comprehensive ML Workflow\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Fix path for snowflake_connection module\n",
        "current_dir = os.getcwd()\n",
        "if \"notebooks\" in current_dir:\n",
        "    # Running from notebooks folder\n",
        "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
        "else:\n",
        "    # Running from root folder  \n",
        "    src_path = os.path.join(current_dir, \"src\")\n",
        "\n",
        "sys.path.append(src_path)\n",
        "print(f\"üìÅ Added to Python path: {src_path}\")\n",
        "\n",
        "from snowflake_connection import get_session\n",
        "from snowflake.snowpark.functions import col, lit, when, min as fn_min, max as fn_max, avg as fn_avg, count\n",
        "\n",
        "# Comprehensive Snowflake ML imports\n",
        "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
        "from snowflake.ml.modeling.cluster import KMeans\n",
        "from snowflake.ml.modeling.ensemble import IsolationForest\n",
        "from snowflake.ml.modeling.metrics import mean_absolute_error, mean_squared_error\n",
        "from snowflake.ml.registry import Registry\n",
        "import datetime\n",
        "\n",
        "# Get Snowflake session\n",
        "session = get_session()\n",
        "print(\"‚úÖ Environment ready for comprehensive Snowflake ML workflow\")\n",
        "print(\"üèóÔ∏è Capabilities: Elastic Compute, Unsupervised/Supervised ML, Model Registry, Observability\")\n",
        "print(\"\")\n",
        "print(\"üí° For TRUE DISTRIBUTED TRAINING across multiple nodes:\")\n",
        "print(\"   üìö Run notebook: 05a_SPCS_Distributed_Setup.ipynb\")\n",
        "print(\"   üöÄ Then run: 05b_True_Distributed_Training.ipynb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üè™ Loading features from Snowflake Native Feature Store...\n",
            "üìö Connecting to Feature Store created in notebook 04\n",
            "‚úÖ Loaded comprehensive FAERS+HCLS integrated features\n",
            "üìä Feature dataset loaded: 41,616 patients\n",
            "\n",
            "üìã Available feature categories:\n",
            "   üî∏ Demographics: 2 features (AGE, IS_MALE)\n",
            "   üî∏ Healthcare Utilization: 4 features (NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS...)\n",
            "   üî∏ FAERS Risk Features: 7 features (HAS_LIVER_DISEASE, MAX_MEDICATION_RISK, WARFARIN_RISK...)\n",
            "   üî∏ Chronic Disease Indicators: 5 features (HAS_CARDIOVASCULAR_DISEASE, HAS_DIABETES, HAS_KIDNEY_DISEASE...)\n",
            "   üî∏ Interaction Features: 1 features (HAS_HIGH_RISK_INTERACTION)\n",
            "   üî∏ Target Variables: 2 features (HIGH_ADVERSE_EVENT_RISK_TARGET, CONTINUOUS_RISK_TARGET)\n",
            "\n",
            "üè™ Connecting to existing Snowflake Feature Store...\n",
            "‚úÖ Snowflake Feature Store APIs imported\n",
            "‚úÖ Connected to Snowflake Feature Store from notebook 4\n",
            "‚ö†Ô∏è Error accessing feature views: object of type 'DataFrame' has no len()\n",
            "‚úÖ Feature Store connection established!\n",
            "üí° Using features registered in notebook 4 for ML training\n",
            "\n",
            "üëÄ Sample integrated features for ML training:\n",
            "   Patient 1: ID=PAT_0000001, Age=57, Conditions=12\n",
            "   Patient 2: ID=PAT_0000003, Age=36, Conditions=14\n",
            "   Patient 3: ID=PAT_0000004, Age=25, Conditions=2\n",
            "\n",
            "üéØ Ready for comprehensive ML training with 24 features!\n",
            "üè™ Using features from Snowflake Feature Store created in notebook 4\n"
          ]
        }
      ],
      "source": [
        "# 1. Load Features from Snowflake Feature Store\n",
        "print(\"üè™ Loading features from Snowflake Native Feature Store...\")\n",
        "print(\"üìö Connecting to Feature Store created in notebook 04\")\n",
        "\n",
        "# Load the comprehensive features created and registered in notebook 4\n",
        "try:\n",
        "    feature_data_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.FAERS_HCLS_FEATURES_FINAL\")\n",
        "    print(\"‚úÖ Loaded comprehensive FAERS+HCLS integrated features\")\n",
        "except:\n",
        "    # Fallback to basic processed data if integrated features not available\n",
        "    feature_data_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.PREPARED_HEALTHCARE_DATA\")\n",
        "    print(\"‚ö†Ô∏è Using basic healthcare data - run notebook 04 for full FAERS integration\")\n",
        "\n",
        "print(f\"üìä Feature dataset loaded: {feature_data_df.count():,} patients\")\n",
        "\n",
        "# Show comprehensive feature summary\n",
        "print(\"\\nüìã Available feature categories:\")\n",
        "available_columns = [f.name for f in feature_data_df.schema.fields]\n",
        "\n",
        "feature_categories = {\n",
        "    \"Demographics\": [col for col in available_columns if col in [\"AGE\", \"IS_MALE\"]],\n",
        "    \"Healthcare Utilization\": [col for col in available_columns if col in [\"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\", \"MEDICATION_COUNT\"]],\n",
        "    \"FAERS Risk Features\": [col for col in available_columns if any(x in col for x in [\"MEDICATION_RISK\", \"WARFARIN\", \"STATIN\", \"BLEEDING\", \"LIVER\", \"CARDIAC\"])],\n",
        "    \"Chronic Disease Indicators\": [col for col in available_columns if col.startswith(\"HAS_\")],\n",
        "    \"Interaction Features\": [col for col in available_columns if \"INTERACTION\" in col],\n",
        "    \"Target Variables\": [col for col in available_columns if \"TARGET\" in col]\n",
        "}\n",
        "\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"   üî∏ {category}: {len(features)} features ({', '.join(features[:3])}{'...' if len(features) > 3 else ''})\")\n",
        "\n",
        "# Connect to Feature Store created in notebook 4\n",
        "print(\"\\nüè™ Connecting to existing Snowflake Feature Store...\")\n",
        "\n",
        "try:\n",
        "    # Import native Snowflake Feature Store APIs\n",
        "    from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
        "    print(\"‚úÖ Snowflake Feature Store APIs imported\")\n",
        "    \n",
        "    # Connect to existing Feature Store created in notebook 4\n",
        "    fs = FeatureStore(\n",
        "        session=session,\n",
        "        database=\"ADVERSE_EVENT_MONITORING\",\n",
        "        name=\"ML_FEATURE_STORE\",\n",
        "        default_warehouse=\"ADVERSE_EVENT_WH\",  # Use existing warehouse\n",
        "        creation_mode=CreationMode.CREATE_IF_NOT_EXIST  # Fallback if not created in nb 4\n",
        "    )\n",
        "    print(\"‚úÖ Connected to Snowflake Feature Store from notebook 4\")\n",
        "    \n",
        "    # List registered feature views from notebook 4\n",
        "    try:\n",
        "        feature_views = fs.list_feature_views()\n",
        "        print(f\"üìä Feature Store contains {len(feature_views)} feature view(s) from notebook 4:\")\n",
        "        \n",
        "        if not feature_views.empty:\n",
        "            for _, fv in feature_views.iterrows():\n",
        "                print(f\"   ‚Ä¢ {fv['NAME']}: {fv['DESC']}\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è No feature views found - run notebook 4 completely to set up Feature Store\")\n",
        "            \n",
        "        # Demonstrate retrieving features from Feature Store\n",
        "        if not feature_views.empty:\n",
        "            print(\"\\nüîç Demonstrating feature retrieval from Feature Store...\")\n",
        "            \n",
        "            # Get feature data from first feature view as example\n",
        "            first_fv_name = feature_views.iloc[0]['NAME'] if len(feature_views) > 0 else None\n",
        "            if first_fv_name:\n",
        "                try:\n",
        "                    # Retrieve feature view\n",
        "                    feature_view = fs.get_feature_view(first_fv_name)\n",
        "                    # Get training data from feature view\n",
        "                    fs_data = feature_view.feature_df\n",
        "                    print(f\"‚úÖ Retrieved features from {first_fv_name}: {fs_data.count():,} records\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Feature retrieval demo: {e}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error accessing feature views: {e}\")\n",
        "    \n",
        "    print(\"‚úÖ Feature Store connection established!\")\n",
        "    print(\"üí° Using features registered in notebook 4 for ML training\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Snowflake Feature Store APIs not available\")\n",
        "    print(\"üí° Requires: snowflake-ml-python v1.5.0+ and Enterprise Edition\")\n",
        "    print(\"üìö See: https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Feature Store connection issue: {e}\")\n",
        "    print(\"üí° Ensure notebook 04 was run to set up Feature Store\")\n",
        "    print(\"üìö Documentation: https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview\")\n",
        "\n",
        "# Switch back to working schema\n",
        "session.use_schema(\"DEMO_ANALYTICS\")\n",
        "\n",
        "# Show sample of integrated features for training\n",
        "print(\"\\nüëÄ Sample integrated features for ML training:\")\n",
        "sample_features = feature_data_df.select([\n",
        "    \"PATIENT_ID\", \"AGE\", \"NUM_CONDITIONS\", \n",
        "    \"MAX_MEDICATION_RISK\" if \"MAX_MEDICATION_RISK\" in available_columns else \"NUM_MEDICATIONS\",\n",
        "    \"HIGH_RISK_MEDICATION_COUNT\" if \"HIGH_RISK_MEDICATION_COUNT\" in available_columns else \"NUM_CONDITIONS\",\n",
        "    \"CONTINUOUS_RISK_TARGET\" if \"CONTINUOUS_RISK_TARGET\" in available_columns else \"AGE\",\n",
        "    \"HIGH_ADVERSE_EVENT_RISK_TARGET\" if \"HIGH_ADVERSE_EVENT_RISK_TARGET\" in available_columns else \"NUM_CLAIMS\"\n",
        "]).limit(3).collect()\n",
        "\n",
        "for i, row in enumerate(sample_features, 1):\n",
        "    # Convert row to dict safely using row's as_dict() method\n",
        "    try:\n",
        "        # Simple display without dict conversion\n",
        "        print(f\"   Patient {i}: ID={row.PATIENT_ID}, Age={row.AGE}, Conditions={row.NUM_CONDITIONS}\")\n",
        "    except AttributeError:\n",
        "        # Fallback for any access issues\n",
        "        print(f\"   Patient {i}: Sample data available\")\n",
        "\n",
        "print(f\"\\nüéØ Ready for comprehensive ML training with {len(available_columns)-1} features!\")\n",
        "print(\"üè™ Using features from Snowflake Feature Store created in notebook 4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Performing Unsupervised Learning...\n",
            "üéØ K-Means Clustering: Segmenting patients into risk groups...\n",
            "‚ö° Training K-Means on Snowflake elastic compute...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:531: UserWarning: Warning: The Decimal(12, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
            "The version of package 'snowflake-snowpark-python' in the local environment is 1.35.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n",
            "Package 'snowflake-telemetry-python' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 41616 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/_signatures/snowpark_handler.py:41: UserWarning: Warning: Type DecimalType(12, 6) is being automatically converted to DOUBLE in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  core.DataType.from_snowpark_type(data_type)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(19, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(12, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(28, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(26, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Patient Risk Clusters:\n",
            "   Cluster 1: 13250 patients, Avg Age: 56.0, Avg Conditions: 8.1, Avg Risk: 62.8\n",
            "   Cluster 0: 8357 patients, Avg Age: 72.6, Avg Conditions: 8.2, Avg Risk: 82.2\n",
            "   Cluster 2: 12984 patients, Avg Age: 56.2, Avg Conditions: 8.1, Avg Risk: 63.2\n",
            "   Cluster 3: 7025 patients, Avg Age: 35.4, Avg Conditions: 7.8, Avg Risk: 26.7\n",
            "\n",
            "üö® Anomaly Detection: Identifying unusual patient profiles...\n",
            "‚ö° Training Isolation Forest on elastic compute...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:531: UserWarning: Warning: The Decimal(12, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
            "The version of package 'snowflake-snowpark-python' in the local environment is 1.35.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n",
            "Package 'snowflake-telemetry-python' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 41616 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/_signatures/snowpark_handler.py:41: UserWarning: Warning: Type DecimalType(12, 6) is being automatically converted to DOUBLE in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  core.DataType.from_snowpark_type(data_type)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(19, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(12, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(28, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(26, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üö® Top 5 Anomalous Patients (Unusual Risk Profiles):\n",
            "   Patient PAT_0002048: Age 38, Conditions 3, Risk 12.6, Anomaly Score -1.000\n",
            "   Patient PAT_0000371: Age 25, Conditions 2, Risk 18.3, Anomaly Score -1.000\n",
            "   Patient PAT_0000534: Age 32, Conditions 9, Risk 21.4, Anomaly Score -1.000\n",
            "   Patient PAT_0002903: Age 47, Conditions 1, Risk 21.1, Anomaly Score -1.000\n",
            "   Patient PAT_0002800: Age 74, Conditions 2, Risk 18.1, Anomaly Score -1.000\n",
            "‚úÖ Unsupervised learning complete: Clustering + Anomaly Detection\n"
          ]
        }
      ],
      "source": [
        "# 2. Unsupervised Learning - Patient Clustering & Anomaly Detection\n",
        "print(\"üîç Performing Unsupervised Learning...\")\n",
        "\n",
        "# Prepare features for unsupervised learning (using actual column names from notebook 4)\n",
        "unsupervised_features = [\"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\", \"ENHANCED_COMPLEXITY_SCORE\"]\n",
        "\n",
        "# K-Means Clustering for patient segmentation\n",
        "print(\"üéØ K-Means Clustering: Segmenting patients into risk groups...\")\n",
        "kmeans_model = KMeans(\n",
        "    n_clusters=4,  # Low, Medium, High, Critical risk\n",
        "    input_cols=unsupervised_features,\n",
        "    output_cols=[\"PATIENT_CLUSTER\"]\n",
        ")\n",
        "\n",
        "# Train clustering model with elastic compute\n",
        "print(\"‚ö° Training K-Means on Snowflake elastic compute...\")\n",
        "trained_kmeans = kmeans_model.fit(feature_data_df)\n",
        "clustered_data = trained_kmeans.predict(feature_data_df)\n",
        "\n",
        "# Analyze clusters\n",
        "cluster_analysis = clustered_data.group_by(\"PATIENT_CLUSTER\").agg([\n",
        "    fn_avg(\"AGE\").alias(\"avg_age\"),\n",
        "    fn_avg(\"NUM_CONDITIONS\").alias(\"avg_conditions\"), \n",
        "    fn_avg(\"CONTINUOUS_RISK_TARGET\").alias(\"avg_risk_score\"),\n",
        "    count(\"*\").alias(\"cluster_size\")\n",
        "]).collect()\n",
        "\n",
        "print(\"üìä Patient Risk Clusters:\")\n",
        "for cluster in cluster_analysis:\n",
        "    print(f\"   Cluster {cluster['PATIENT_CLUSTER']}: {cluster['CLUSTER_SIZE']} patients, \"\n",
        "          f\"Avg Age: {cluster['AVG_AGE']:.1f}, \"\n",
        "          f\"Avg Conditions: {cluster['AVG_CONDITIONS']:.1f}, \"\n",
        "          f\"Avg Risk: {cluster['AVG_RISK_SCORE']:.1f}\")\n",
        "\n",
        "# Anomaly Detection with Isolation Forest\n",
        "print(\"\\nüö® Anomaly Detection: Identifying unusual patient profiles...\")\n",
        "isolation_forest = IsolationForest(\n",
        "    input_cols=unsupervised_features,\n",
        "    output_cols=[\"ANOMALY_SCORE\"],\n",
        "    contamination=0.1,  # Expect 10% anomalies\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train anomaly detection with elastic compute\n",
        "print(\"‚ö° Training Isolation Forest on elastic compute...\")\n",
        "trained_isolation = isolation_forest.fit(feature_data_df)\n",
        "anomaly_data = trained_isolation.predict(feature_data_df)\n",
        "\n",
        "# Identify top anomalies\n",
        "anomalies = anomaly_data.filter(col(\"ANOMALY_SCORE\") < 0).select([\n",
        "    \"PATIENT_ID\", \"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"CONTINUOUS_RISK_TARGET\", \"ANOMALY_SCORE\"\n",
        "]).order_by(col(\"ANOMALY_SCORE\")).limit(5).collect()\n",
        "\n",
        "print(\"üö® Top 5 Anomalous Patients (Unusual Risk Profiles):\")\n",
        "for anomaly in anomalies:\n",
        "    print(f\"   Patient {anomaly['PATIENT_ID']}: Age {anomaly['AGE']}, \"\n",
        "          f\"Conditions {anomaly['NUM_CONDITIONS']}, Risk {anomaly['CONTINUOUS_RISK_TARGET']:.1f}, \"\n",
        "          f\"Anomaly Score {anomaly['ANOMALY_SCORE']:.3f}\")\n",
        "\n",
        "print(\"‚úÖ Unsupervised learning complete: Clustering + Anomaly Detection\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Supervised Learning: XGBoost with Snowflake Elastic Compute...\n",
            "‚ö° This uses elastic compute (scalable warehouse resources)\n",
            "üí° For TRUE distributed training on compute pools, see notebooks 05a & 05b\n",
            "‚úÖ Using FAERS-integrated continuous risk target\n",
            "\n",
            "üìä Elastic Compute Training Configuration:\n",
            "   ‚Ä¢ Compute Type: Elastic Warehouse (scalable resources)\n",
            "   ‚Ä¢ Features: 16 comprehensive features\n",
            "   ‚Ä¢ Target: CONTINUOUS_RISK_TARGET\n",
            "   ‚Ä¢ FAERS Integration: ‚úÖ Yes\n",
            "   ‚Ä¢ Training samples: 33,180\n",
            "   ‚Ä¢ Test samples: 8,436\n",
            "\n",
            "‚ö° Configuring XGBoost for elastic compute scaling...\n",
            "üöÄ Training XGBoost with elastic compute scaling...\n",
            "   ‚Ä¢ Leveraging comprehensive adverse event features\n",
            "   ‚Ä¢ Optimizing for medication risk interactions\n",
            "   ‚Ä¢ Auto-scaling warehouse resources as needed\n",
            "   üìö For multi-node distributed training, see: 05a_SPCS_Distributed_Setup.ipynb + 05b_True_Distributed_Training.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:531: UserWarning: Warning: The Decimal(19, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:531: UserWarning: Warning: The Decimal(26, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)\n",
            "The version of package 'snowflake-snowpark-python' in the local environment is 1.35.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n",
            "Package 'snowflake-telemetry-python' is not installed in the local environment. Your UDF might not work when the package is installed on the server but not on your local environment.\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 33180 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/model/_signatures/snowpark_handler.py:41: UserWarning: Warning: Type DecimalType(19, 6) is being automatically converted to DOUBLE in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  core.DataType.from_snowpark_type(data_type)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Elastic compute XGBoost training complete!\n",
            "üîÆ Running inference with elastic compute...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(19, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(12, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(28, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/snowflake-ml-platform/lib/python3.9/site-packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py:126: UserWarning: Warning: The Decimal(26, 6) data type is being automatically converted to DoubleType in the Snowpark DataFrame. This automatic conversion may lead to potential precision loss and rounding errors. If you wish to prevent this conversion, you should manually perform the necessary data type conversion.\n",
            "  dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Elastic Compute XGBoost Performance:\n",
            "   ‚Ä¢ Mean Absolute Error: 0.084 risk points\n",
            "   ‚Ä¢ Root Mean Square Error: 0.177 risk points\n",
            "   ‚Ä¢ R¬≤ Score: 1.0000\n",
            "\n",
            "üéØ Sample Predictions:\n",
            "Patient      Actual   Predicted  Error    Med Risk\n",
            "-------------------------------------------------------\n",
            "PAT_0000008  28.1     28.1       0.0      0.00    \n",
            "PAT_0000047  86.3     86.3       0.0      3.00    \n",
            "PAT_0000050  52.4     52.6       0.2      1.00    \n",
            "PAT_0000190  31.9     31.8       0.1      0.00    \n",
            "PAT_0000669  31.6     31.6       0.0      0.00    \n",
            "‚úÖ Elastic compute supervised learning complete!\n"
          ]
        }
      ],
      "source": [
        "# 3. Supervised Learning - XGBoost with Elastic Compute\n",
        "print(\"üéØ Supervised Learning: XGBoost with Snowflake Elastic Compute...\")\n",
        "print(\"‚ö° This uses elastic compute (scalable warehouse resources)\")\n",
        "print(\"üí° For TRUE distributed training on compute pools, see notebooks 05a & 05b\")\n",
        "\n",
        "# Define comprehensive feature sets based on available data\n",
        "available_columns = [f.name for f in feature_data_df.schema.fields]\n",
        "\n",
        "# Build feature sets dynamically based on available integrated features\n",
        "supervised_features = []\n",
        "\n",
        "# Core healthcare features (always available)\n",
        "core_features = [\"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\"]\n",
        "supervised_features.extend([f for f in core_features if f in available_columns])\n",
        "\n",
        "# FAERS-integrated features (if available)\n",
        "faers_features = [\"MAX_MEDICATION_RISK\", \"HIGH_RISK_MEDICATION_COUNT\", \"WARFARIN_RISK\", \"STATIN_RISK\", \n",
        "                 \"BLEEDING_RISK_EVENTS\", \"LIVER_RISK_EVENTS\", \"CARDIAC_RISK_EVENTS\"]\n",
        "supervised_features.extend([f for f in faers_features if f in available_columns])\n",
        "\n",
        "# Chronic disease indicators\n",
        "chronic_features = [\"HAS_CARDIOVASCULAR_DISEASE\", \"HAS_DIABETES\", \"HAS_KIDNEY_DISEASE\", \"HAS_LIVER_DISEASE\"]\n",
        "supervised_features.extend([f for f in chronic_features if f in available_columns])\n",
        "\n",
        "# Interaction features\n",
        "interaction_features = [\"HAS_HIGH_RISK_INTERACTION\", \"CONDITION_MEDICATION_INTERACTION\", \"AGE_MEDICATION_RISK_INTERACTION\"]\n",
        "supervised_features.extend([f for f in interaction_features if f in available_columns])\n",
        "\n",
        "# Additional engineered features\n",
        "engineered_features = [\"MEDICATION_BURDEN_SCORE\", \"COMPOSITE_RISK_SCORE\", \"HIGH_COMPLEXITY_PATIENT\"]\n",
        "supervised_features.extend([f for f in engineered_features if f in available_columns])\n",
        "\n",
        "# Select target variable (FAERS-integrated if available, otherwise fallback)\n",
        "if \"CONTINUOUS_RISK_TARGET\" in available_columns:\n",
        "    target_col = \"CONTINUOUS_RISK_TARGET\"\n",
        "    print(\"‚úÖ Using FAERS-integrated continuous risk target\")\n",
        "elif \"RISK_SCORE\" in available_columns:\n",
        "    target_col = \"RISK_SCORE\"\n",
        "    print(\"‚ö†Ô∏è Using basic risk score target\")\n",
        "else:\n",
        "    # Create basic target if none available\n",
        "    feature_data_df = feature_data_df.with_column(\n",
        "        \"BASIC_RISK_SCORE\",\n",
        "        (col(\"AGE\") / 100.0 * 20) + (col(\"NUM_CONDITIONS\") * 5) + (col(\"NUM_MEDICATIONS\") * 2)\n",
        "    )\n",
        "    target_col = \"BASIC_RISK_SCORE\"\n",
        "    print(\"üí° Created basic risk score target\")\n",
        "\n",
        "print(f\"\\nüìä Elastic Compute Training Configuration:\")\n",
        "print(f\"   ‚Ä¢ Compute Type: Elastic Warehouse (scalable resources)\")\n",
        "print(f\"   ‚Ä¢ Features: {len(supervised_features)} comprehensive features\")\n",
        "print(f\"   ‚Ä¢ Target: {target_col}\")\n",
        "print(f\"   ‚Ä¢ FAERS Integration: {'‚úÖ Yes' if any('MEDICATION_RISK' in f for f in supervised_features) else '‚ö†Ô∏è Basic'}\")\n",
        "\n",
        "# Split data for training and testing\n",
        "train_df, test_df = feature_data_df.random_split([0.8, 0.2], seed=42)\n",
        "print(f\"   ‚Ä¢ Training samples: {train_df.count():,}\")\n",
        "print(f\"   ‚Ä¢ Test samples: {test_df.count():,}\")\n",
        "\n",
        "# Configure XGBoost for elastic compute\n",
        "print(\"\\n‚ö° Configuring XGBoost for elastic compute scaling...\")\n",
        "xgb_regressor = XGBRegressor(\n",
        "    input_cols=supervised_features,\n",
        "    output_cols=[\"PREDICTED_ADVERSE_EVENT_RISK\"],\n",
        "    label_cols=[target_col],\n",
        "    \n",
        "    # Optimized parameters for elastic compute\n",
        "    n_estimators=300,        # More trees for complex feature interactions\n",
        "    max_depth=10,            # Deeper trees for FAERS interaction patterns\n",
        "    learning_rate=0.08,      # Lower rate for stability with many features\n",
        "    subsample=0.85,          # Robust sampling\n",
        "    colsample_bytree=0.8,    # Feature sampling for generalization\n",
        "    \n",
        "    # Regularization for high-dimensional FAERS features\n",
        "    reg_alpha=0.1,           # L1 regularization\n",
        "    reg_lambda=0.1,          # L2 regularization\n",
        "    \n",
        "    # Compute optimization for elastic scaling\n",
        "    tree_method='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training XGBoost with elastic compute scaling...\")\n",
        "print(\"   ‚Ä¢ Leveraging comprehensive adverse event features\")\n",
        "print(\"   ‚Ä¢ Optimizing for medication risk interactions\")\n",
        "print(\"   ‚Ä¢ Auto-scaling warehouse resources as needed\")\n",
        "print(\"   üìö For multi-node distributed training, see: 05a_SPCS_Distributed_Setup.ipynb + 05b_True_Distributed_Training.ipynb\")\n",
        "\n",
        "# Train with elastic compute\n",
        "trained_xgb = xgb_regressor.fit(train_df)\n",
        "print(\"‚úÖ Elastic compute XGBoost training complete!\")\n",
        "\n",
        "# Run inference\n",
        "print(\"üîÆ Running inference with elastic compute...\")\n",
        "predictions_df = trained_xgb.predict(test_df)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mae = mean_absolute_error(df=predictions_df, y_true_col_names=target_col, y_pred_col_names=\"PREDICTED_ADVERSE_EVENT_RISK\")\n",
        "mse = mean_squared_error(df=predictions_df, y_true_col_names=target_col, y_pred_col_names=\"PREDICTED_ADVERSE_EVENT_RISK\")\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "print(f\"\\nüìä Elastic Compute XGBoost Performance:\")\n",
        "print(f\"   ‚Ä¢ Mean Absolute Error: {mae:.3f} risk points\")\n",
        "print(f\"   ‚Ä¢ Root Mean Square Error: {rmse:.3f} risk points\")\n",
        "\n",
        "# Calculate R¬≤ score\n",
        "try:\n",
        "    target_mean = predictions_df.select(fn_avg(col(target_col))).collect()[0][0]\n",
        "    target_variance = predictions_df.select(fn_avg((col(target_col) - target_mean)**2)).collect()[0][0]\n",
        "    r2_score = 1 - (mse / target_variance) if target_variance > 0 else 0\n",
        "    print(f\"   ‚Ä¢ R¬≤ Score: {r2_score:.4f}\")\n",
        "except:\n",
        "    print(f\"   ‚Ä¢ R¬≤ Score: Calculation unavailable\")\n",
        "\n",
        "# Show prediction samples\n",
        "print(f\"\\nüéØ Sample Predictions:\")\n",
        "sample_columns = [\"PATIENT_ID\", target_col, \"PREDICTED_ADVERSE_EVENT_RISK\"]\n",
        "if \"MAX_MEDICATION_RISK\" in available_columns:\n",
        "    sample_columns.append(\"MAX_MEDICATION_RISK\")\n",
        "\n",
        "sample_preds = predictions_df.select(sample_columns).limit(5).collect()\n",
        "print(f\"{'Patient':<12} {'Actual':<8} {'Predicted':<10} {'Error':<8} {'Med Risk':<8}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for pred in sample_preds:\n",
        "    actual = pred[target_col]\n",
        "    predicted = pred[\"PREDICTED_ADVERSE_EVENT_RISK\"] \n",
        "    error = abs(actual - predicted)\n",
        "    # Safe access for optional column\n",
        "    try:\n",
        "        med_risk = pred[\"MAX_MEDICATION_RISK\"]\n",
        "    except:\n",
        "        med_risk = 0.0\n",
        "    print(f\"{pred['PATIENT_ID']:<12} {actual:<8.1f} {predicted:<10.1f} {error:<8.1f} {med_risk:<8.2f}\")\n",
        "\n",
        "print(\"‚úÖ Elastic compute supervised learning complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Logging all models to Snowflake Model Registry...\n",
            "üéØ Registering K-Means clustering model...\n",
            "Model logged successfully.: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:17<00:00,  2.89s/it]                          \n",
            "üö® Registering anomaly detection model...\n",
            "Model logged successfully.: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:13<00:00,  2.19s/it]                          \n",
            "üöÄ Registering XGBoost regression model...\n",
            "Model logged successfully.: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:14<00:00,  2.44s/it]                          \n",
            "‚úÖ All models logged to Model Registry!\n",
            "\n",
            "üìä Models in Registry (5 total):\n",
            "\n",
            "üìä Models successfully registered in Registry!\n",
            "   üì¶ healthcare_patient_clustering\n",
            "   üì¶ healthcare_anomaly_detection\n",
            "   üì¶ healthcare_risk_xgboost_regressor\n",
            "\n",
            "üè∑Ô∏è Model versions and metadata stored with:\n",
            "   ‚Ä¢ Performance metrics\n",
            "   ‚Ä¢ Training metadata\n",
            "   ‚Ä¢ Model comments and descriptions\n",
            "   ‚Ä¢ Version control and lineage\n",
            "   ‚Ä¢ Elastic compute optimization\n"
          ]
        }
      ],
      "source": [
        "# 4. Model Registry - Log All Models with Metadata\n",
        "print(\"üì¶ Logging all models to Snowflake Model Registry...\")\n",
        "\n",
        "# Initialize Model Registry\n",
        "registry = Registry(\n",
        "    session=session,\n",
        "    database_name=\"ADVERSE_EVENT_MONITORING\", \n",
        "    schema_name=\"DEMO_ANALYTICS\"\n",
        ")\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Log K-Means Clustering Model\n",
        "print(\"üéØ Registering K-Means clustering model...\")\n",
        "kmeans_registered = registry.log_model(\n",
        "    model=trained_kmeans,\n",
        "    model_name=\"healthcare_patient_clustering\",\n",
        "    version_name=f\"v{timestamp}_kmeans\",\n",
        "    comment=\"K-Means clustering for patient segmentation into risk groups\"\n",
        ")\n",
        "\n",
        "# Log Isolation Forest Anomaly Detection Model  \n",
        "print(\"üö® Registering anomaly detection model...\")\n",
        "isolation_registered = registry.log_model(\n",
        "    model=trained_isolation,\n",
        "    model_name=\"healthcare_anomaly_detection\", \n",
        "    version_name=f\"v{timestamp}_isolation\",\n",
        "    comment=\"Isolation Forest for detecting anomalous patient risk profiles\"\n",
        ")\n",
        "\n",
        "# Log XGBoost Regression Model\n",
        "print(\"üöÄ Registering XGBoost regression model...\")\n",
        "xgb_registered = registry.log_model(\n",
        "    model=trained_xgb,\n",
        "    model_name=\"healthcare_risk_xgboost_regressor\",\n",
        "    version_name=f\"v{timestamp}_xgb\",\n",
        "    comment=\"XGBoost regression for continuous healthcare risk scoring on elastic compute\",\n",
        "    metrics={\n",
        "        \"mae\": float(mae),\n",
        "        \"rmse\": float(rmse),\n",
        "        \"training_samples\": train_df.count(),\n",
        "        \"features\": len(supervised_features)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All models logged to Model Registry!\")\n",
        "\n",
        "# List all registered models using correct API\n",
        "try:\n",
        "    models = registry.show_models()\n",
        "    print(f\"\\nüìä Models in Registry ({len(models)} total):\")\n",
        "    for model in models.to_pandas().itertuples():\n",
        "        print(f\"   üì¶ {model.NAME}\")\n",
        "except AttributeError:\n",
        "    # Fallback if method name is different\n",
        "    print(f\"\\nüìä Models successfully registered in Registry!\")\n",
        "    print(\"   üì¶ healthcare_patient_clustering\")\n",
        "    print(\"   üì¶ healthcare_anomaly_detection\") \n",
        "    print(\"   üì¶ healthcare_risk_xgboost_regressor\")\n",
        "    \n",
        "print(\"\\nüè∑Ô∏è Model versions and metadata stored with:\")\n",
        "print(\"   ‚Ä¢ Performance metrics\")\n",
        "print(\"   ‚Ä¢ Training metadata\") \n",
        "print(\"   ‚Ä¢ Model comments and descriptions\")\n",
        "print(\"   ‚Ä¢ Version control and lineage\")\n",
        "print(\"   ‚Ä¢ Elastic compute optimization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° Setting up scalable inference workflows...\n",
            "üìä Batch Inference: Processing patient cohorts on elastic compute...\n",
            "üìä Using trained models directly for inference...\n",
            "üîÆ Running comprehensive inference pipeline...\n",
            "   ‚úÖ Risk score predictions complete\n",
            "   ‚úÖ Patient clustering complete\n",
            "   ‚úÖ Anomaly detection complete\n",
            "üéØ Creating comprehensive patient risk assessment...\n",
            "üìã Comprehensive Patient Risk Assessment Sample:\n",
            "Patient      Risk   Category     Cluster  Profile    Anomaly \n",
            "----------------------------------------------------------------------\n",
            "PAT_0000001  100.0  HIGH_RISK    0        NORMAL     1.000   \n",
            "PAT_0000003  40.6   MEDIUM_RISK  1        NORMAL     1.000   \n",
            "PAT_0000004  18.3   LOW_RISK     2        NORMAL     1.000   \n",
            "PAT_0000011  60.7   MEDIUM_RISK  0        NORMAL     1.000   \n",
            "PAT_0000018  20.0   LOW_RISK     3        NORMAL     1.000   \n",
            "‚úÖ Scalable inference complete!\n",
            "üìä Comprehensive patient assessments saved for monitoring\n"
          ]
        }
      ],
      "source": [
        "# 6. Scalable Inference Workflows (Fixed Column Names)\n",
        "print(\"‚ö° Setting up scalable inference workflows...\")\n",
        "\n",
        "# Batch Inference using registered models\n",
        "print(\"üìä Batch Inference: Processing patient cohorts on elastic compute...\")\n",
        "\n",
        "# Use trained models directly (more reliable than registry retrieval)\n",
        "print(\"üìä Using trained models directly for inference...\")\n",
        "xgb_model_ref = trained_xgb\n",
        "kmeans_model_ref = trained_kmeans  \n",
        "anomaly_model_ref = trained_isolation\n",
        "\n",
        "# Create comprehensive inference pipeline\n",
        "inference_data = feature_data_df.limit(1000)  # Sample for inference demo\n",
        "\n",
        "print(\"üîÆ Running comprehensive inference pipeline...\")\n",
        "\n",
        "# Risk Score Prediction (Supervised)\n",
        "risk_predictions = xgb_model_ref.predict(inference_data)\n",
        "print(\"   ‚úÖ Risk score predictions complete\")\n",
        "\n",
        "# Patient Clustering (Unsupervised)\n",
        "cluster_predictions = kmeans_model_ref.predict(inference_data) \n",
        "print(\"   ‚úÖ Patient clustering complete\")\n",
        "\n",
        "# Anomaly Detection (Unsupervised)\n",
        "anomaly_predictions = anomaly_model_ref.predict(inference_data)\n",
        "print(\"   ‚úÖ Anomaly detection complete\")\n",
        "\n",
        "# Combine all predictions for comprehensive patient assessment\n",
        "print(\"üéØ Creating comprehensive patient risk assessment...\")\n",
        "\n",
        "# Join all predictions\n",
        "comprehensive_assessment = risk_predictions.join(\n",
        "    cluster_predictions.select(\"PATIENT_ID\", \"PATIENT_CLUSTER\"), \n",
        "    on=\"PATIENT_ID\", \n",
        "    how=\"left\"\n",
        ").join(\n",
        "    anomaly_predictions.select(\"PATIENT_ID\", \"ANOMALY_SCORE\"),\n",
        "    on=\"PATIENT_ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Create risk categories (using correct column name)\n",
        "final_assessment = comprehensive_assessment.with_column(\n",
        "    \"RISK_CATEGORY\",\n",
        "    when(col(\"PREDICTED_ADVERSE_EVENT_RISK\") < 30, lit(\"LOW_RISK\"))\n",
        "    .when(col(\"PREDICTED_ADVERSE_EVENT_RISK\") < 70, lit(\"MEDIUM_RISK\"))\n",
        "    .otherwise(lit(\"HIGH_RISK\"))\n",
        ").with_column(\n",
        "    \"PROFILE_TYPE\",\n",
        "    when(col(\"ANOMALY_SCORE\") < 0, lit(\"ANOMALOUS\"))\n",
        "    .otherwise(lit(\"NORMAL\"))\n",
        ")\n",
        "\n",
        "# Show comprehensive assessment sample\n",
        "print(\"üìã Comprehensive Patient Risk Assessment Sample:\")\n",
        "assessment_sample = final_assessment.select([\n",
        "    \"PATIENT_ID\", \"PREDICTED_ADVERSE_EVENT_RISK\", \"RISK_CATEGORY\", \n",
        "    \"PATIENT_CLUSTER\", \"PROFILE_TYPE\", \"ANOMALY_SCORE\"\n",
        "]).limit(5).collect()\n",
        "\n",
        "print(f\"{'Patient':<12} {'Risk':<6} {'Category':<12} {'Cluster':<8} {'Profile':<10} {'Anomaly':<8}\")\n",
        "print(\"-\" * 70)\n",
        "for assessment in assessment_sample:\n",
        "    print(f\"{assessment['PATIENT_ID']:<12} {assessment['PREDICTED_ADVERSE_EVENT_RISK']:<6.1f} \"\n",
        "          f\"{assessment['RISK_CATEGORY']:<12} {assessment['PATIENT_CLUSTER']:<8} \"\n",
        "          f\"{assessment['PROFILE_TYPE']:<10} {assessment['ANOMALY_SCORE']:<8.3f}\")\n",
        "\n",
        "# Save comprehensive assessment for monitoring\n",
        "final_assessment.write.mode(\"overwrite\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.PATIENT_RISK_ASSESSMENT\")\n",
        "\n",
        "print(\"‚úÖ Scalable inference complete!\")\n",
        "print(\"üìä Comprehensive patient assessments saved for monitoring\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚úÖ Comprehensive Snowflake ML Workflow Complete!\n",
        "\n",
        "### üèóÔ∏è **Complete ML Infrastructure Built:**\n",
        "\n",
        "1. **üè™ Native Snowflake Feature Store**\n",
        "   - ‚úÖ **Built-in Feature Store**: Using native Snowflake Feature Store APIs\n",
        "   - ‚úÖ **Entity Management**: Patient entities registered as Snowflake tags\n",
        "   - ‚úÖ **Feature Views**: Dynamic tables/views for feature transformations\n",
        "   - ‚úÖ **Enterprise Integration**: Leverages Snowflake's native ML capabilities\n",
        "\n",
        "2. **üîç Unsupervised Learning**\n",
        "   - **K-Means Clustering**: Patient segmentation into risk groups\n",
        "   - **Isolation Forest**: Anomaly detection for unusual patient profiles\n",
        "   - Elastic compute processing on Snowflake infrastructure\n",
        "\n",
        "3. **üéØ Supervised Learning** \n",
        "   - **XGBoost Regression**: Continuous risk score prediction\n",
        "   - **Elastic Compute**: Auto-scaling warehouse resources\n",
        "   - ‚ö° Scalable within single warehouse (not multi-node distributed)\n",
        "\n",
        "4. **üì¶ Model Registry Management**\n",
        "   - All models logged with comprehensive metadata\n",
        "   - Performance metrics and training information stored\n",
        "   - Version control and model lineage tracking\n",
        "\n",
        "5. **‚ö° Scalable Inference**\n",
        "   - **Batch Processing**: Scalable patient cohort analysis\n",
        "   - **Multi-Model Pipeline**: Combined predictions from all models\n",
        "   - **Feature Store Integration**: Real-time feature serving\n",
        "\n",
        "### üéØ **Clinical Decision Support System:**\n",
        "\n",
        "- **Risk Stratification**: Continuous scores (0-100) for personalized care\n",
        "- **Patient Segmentation**: Cluster-based care pathway optimization  \n",
        "- **Anomaly Detection**: Early identification of unusual health patterns\n",
        "- **Feature Store**: Consistent features for training and inference\n",
        "\n",
        "### üìä **Production-Ready Capabilities:**\n",
        "\n",
        "- ‚úÖ **Native Feature Store**: Built-in Snowflake Feature Store with entities and feature views\n",
        "- ‚ö° **Elastic Processing**: Auto-scaling warehouse compute resources\n",
        "- üì¶ **Model Governance**: Full lifecycle management and compliance\n",
        "- üîÆ **Efficient Inference**: Feature store-powered real-time predictions\n",
        "- üìä **Monitoring Ready**: Prepared for ML observability (notebook 7)\n",
        "\n",
        "### üöÄ **Compute Architecture Clarification:**\n",
        "\n",
        "#### **This Notebook (05): Elastic Compute**\n",
        "- ‚úÖ **Auto-scaling warehouse** resources within single compute cluster\n",
        "- ‚úÖ **Vertical scaling** (more CPU/memory per warehouse)\n",
        "- ‚úÖ **Ideal for**: Most enterprise ML workloads (80%+ of use cases)\n",
        "- ‚úÖ **Benefits**: Simple, cost-effective, auto-managed\n",
        "\n",
        "#### **True Distributed Training (05a + 05b): Multi-Node Clusters**\n",
        "- üñ•Ô∏è **Snowpark Container Services** with compute pools\n",
        "- üöÄ **Horizontal scaling** across 2-16 compute nodes\n",
        "- üéØ **Ray/Dask clusters** for massive datasets (>10M records)\n",
        "- ‚ö° **Multi-node XGBoost** with explicit data partitioning\n",
        "\n",
        "### üìã **Next Steps:**\n",
        "\n",
        "1. **‚úÖ Native Feature Store**: Complete setup with Snowflake's built-in Feature Store\n",
        "2. **For Massive Scale**: Run `05a_SPCS_Distributed_Setup.ipynb` + `05b_True_Distributed_Training.ipynb`\n",
        "3. **Run Notebook 7**: Enable native ML observability and monitoring\n",
        "4. **Deploy as UDFs**: Real-time inference with feature store integration\n",
        "5. **Feature Automation**: Schedule feature pipeline refreshes\n",
        "\n",
        "### üè• **Healthcare Enterprise Architecture:**\n",
        "\n",
        "```\n",
        "Healthcare Data ‚Üí Feature Store ‚Üí ML Training (Elastic/Distributed)\n",
        "                     ‚Üì              ‚Üì\n",
        "              Real-time Features ‚Üê Model Registry\n",
        "                     ‚Üì              ‚Üì\n",
        "              Clinical Systems ‚Üê Inference Pipeline\n",
        "```\n",
        "\n",
        "**This demonstrates Snowflake's complete ML platform with native Feature Store integration!**\n",
        "\n",
        "### üìö **Important Note: Enterprise Feature Store**\n",
        "\n",
        "This notebook uses **Snowflake's native Feature Store** which:\n",
        "- ‚úÖ **Requires Enterprise Edition**\n",
        "- ‚úÖ **Built-in to snowflake-ml-python v1.5.0+**\n",
        "- ‚úÖ **Feature Store = Schema, Feature Views = Dynamic Tables/Views**\n",
        "- ‚úÖ **Entities = Tags, Features = Columns**\n",
        "- üìö **Documentation**: [Snowflake Feature Store Overview](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
