{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Comprehensive Snowflake ML Workflow (Elastic Compute + Feature Store)\n",
    "\n",
    "Complete end-to-end ML workflow with **proper Feature Store management**, elastic compute, unsupervised/supervised learning, and observability.\n",
    "\n",
    "> **This notebook uses elastic compute** (scalable warehouse resources)\n",
    "> \n",
    "> **For TRUE distributed training across multiple compute nodes**, see:\n",
    "> - `05a_SPCS_Distributed_Setup.ipynb` - SPCS infrastructure setup  \n",
    "> - `05b_True_Distributed_Training.ipynb` - Multi-node Ray cluster training\n",
    "\n",
    "**Comprehensive ML Pipeline:**\n",
    "1. **Feature Store Management** - Proper feature entity registration and serving\n",
    "2. **Unsupervised Learning** - Clustering and anomaly detection\n",
    "3. **Supervised Learning** - XGBoost regression with elastic compute\n",
    "4. **Model Registry** - Log and version all models\n",
    "5. **Scalable Inference** - Batch processing on elastic compute\n",
    "6. **ML Observability** - Native monitoring and drift detection\n",
    "\n",
    "## Prerequisites\n",
    "- Running in Snowflake Notebooks environment\n",
    "- Previous notebooks completed (01, 02, 03, 03b, 04)\n",
    "- Feature engineering completed with integrated FAERS+HCLS dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Snowflake Session for Comprehensive ML Workflow\n",
    "print(\"Initializing Snowflake session for comprehensive ML workflow...\")\n",
    "\n",
    "# Import Snowpark session and functions (available in Snowflake Notebooks)\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, lit, when, min as fn_min, max as fn_max, avg as fn_avg, count\n",
    "\n",
    "# Comprehensive Snowflake ML imports\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.cluster import KMeans\n",
    "from snowflake.ml.modeling.ensemble import IsolationForest\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_error, mean_squared_error\n",
    "from snowflake.ml.registry import Registry\n",
    "import datetime\n",
    "\n",
    "# Get the active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "print(\"SUCCESS: Snowflake session initialized for comprehensive ML workflow\")\n",
    "\n",
    "# Verify context\n",
    "current_context = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        CURRENT_DATABASE() as database,\n",
    "        CURRENT_SCHEMA() as schema,\n",
    "        CURRENT_WAREHOUSE() as warehouse\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"   Database: {current_context['DATABASE']}\")\n",
    "print(f\"   Schema: {current_context['SCHEMA']}\")\n",
    "print(f\"   Warehouse: {current_context['WAREHOUSE']}\")\n",
    "print(\"SUCCESS: Environment ready for comprehensive Snowflake ML workflow\")\n",
    "print(\"Capabilities: Elastic Compute, Unsupervised/Supervised ML, Model Registry, Observability\")\n",
    "print(\"\")\n",
    "print(\"For TRUE DISTRIBUTED TRAINING across multiple nodes:\")\n",
    "print(\"   Run notebook: 05a_SPCS_Distributed_Setup.ipynb\")\n",
    "print(\"   Then run: 05b_True_Distributed_Training.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Features from Snowflake Feature Store\n",
    "print(\"Loading features from Snowflake Native Feature Store...\")\n",
    "print(\"Connecting to Feature Store created in notebook 04\")\n",
    "\n",
    "# Load the comprehensive features created and registered in notebook 4\n",
    "try:\n",
    "    feature_data_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.FAERS_HCLS_FEATURES_FINAL\")\n",
    "    print(\"SUCCESS: Loaded comprehensive FAERS+HCLS integrated features\")\n",
    "except:\n",
    "    # Fallback to basic processed data if integrated features not available\n",
    "    feature_data_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.PREPARED_HEALTHCARE_DATA\")\n",
    "    print(\"WARNING: Using basic healthcare data - run notebook 04 for full FAERS integration\")\n",
    "\n",
    "print(f\"Feature dataset loaded: {feature_data_df.count():,} patients\")\n",
    "\n",
    "# Show comprehensive feature summary\n",
    "print(\"\\nAvailable feature categories:\")\n",
    "available_columns = [f.name for f in feature_data_df.schema.fields]\n",
    "\n",
    "feature_categories = {\n",
    "    \"Demographics\": [col for col in available_columns if col in [\"AGE\", \"IS_MALE\"]],\n",
    "    \"Healthcare Utilization\": [col for col in available_columns if col in [\"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\", \"MEDICATION_COUNT\"]],\n",
    "    \"FAERS Risk Features\": [col for col in available_columns if any(x in col for x in [\"MEDICATION_RISK\", \"WARFARIN\", \"STATIN\", \"BLEEDING\", \"LIVER\", \"CARDIAC\"])],\n",
    "    \"Chronic Disease Indicators\": [col for col in available_columns if col.startswith(\"HAS_\")],\n",
    "    \"Interaction Features\": [col for col in available_columns if \"INTERACTION\" in col],\n",
    "    \"Target Variables\": [col for col in available_columns if \"TARGET\" in col]\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"   - {category}: {len(features)} features ({', '.join(features[:3])}{'...' if len(features) > 3 else ''})\")\n",
    "\n",
    "# Connect to Feature Store created in notebook 4\n",
    "print(\"\\nConnecting to existing Snowflake Feature Store...\")\n",
    "\n",
    "try:\n",
    "    # Import native Snowflake Feature Store APIs\n",
    "    from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
    "    print(\"SUCCESS: Snowflake Feature Store APIs imported\")\n",
    "    \n",
    "    # Connect to existing Feature Store created in notebook 4\n",
    "    fs = FeatureStore(\n",
    "        session=session,\n",
    "        database=\"ADVERSE_EVENT_MONITORING\",\n",
    "        name=\"ML_FEATURE_STORE\",\n",
    "        default_warehouse=\"ADVERSE_EVENT_WH\",  # Use existing warehouse\n",
    "        creation_mode=CreationMode.CREATE_IF_NOT_EXIST  # Fallback if not created in nb 4\n",
    "    )\n",
    "    print(\"SUCCESS: Connected to Snowflake Feature Store from notebook 4\")\n",
    "    \n",
    "    # List registered feature views from notebook 4\n",
    "    try:\n",
    "        feature_views = fs.list_feature_views()\n",
    "        print(f\"Feature Store contains {len(feature_views)} feature view(s) from notebook 4:\")\n",
    "        \n",
    "        if not feature_views.empty:\n",
    "            for _, fv in feature_views.iterrows():\n",
    "                print(f\"   • {fv['NAME']}: {fv['DESC']}\")\n",
    "        else:\n",
    "            print(\"   WARNING: No feature views found - run notebook 4 completely to set up Feature Store\")\n",
    "            \n",
    "        # Demonstrate retrieving features from Feature Store\n",
    "        if not feature_views.empty:\n",
    "            print(\"\\nDemonstrating feature retrieval from Feature Store...\")\n",
    "            \n",
    "            # Get feature data from first feature view as example\n",
    "            first_fv_name = feature_views.iloc[0]['NAME'] if len(feature_views) > 0 else None\n",
    "            if first_fv_name:\n",
    "                try:\n",
    "                    # Retrieve feature view\n",
    "                    feature_view = fs.get_feature_view(first_fv_name)\n",
    "                    # Get training data from feature view\n",
    "                    fs_data = feature_view.feature_df\n",
    "                    print(f\"SUCCESS: Retrieved features from {first_fv_name}: {fs_data.count():,} records\")\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: Feature retrieval demo: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Error accessing feature views: {e}\")\n",
    "    \n",
    "    print(\"SUCCESS: Feature Store connection established!\")\n",
    "    print(\"Using features registered in notebook 4 for ML training\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WARNING: Snowflake Feature Store APIs not available\")\n",
    "    print(\"Requires: snowflake-ml-python v1.5.0+ and Enterprise Edition\")\n",
    "    print(\"See: https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Feature Store connection issue: {e}\")\n",
    "    print(\"Ensure notebook 04 was run to set up Feature Store\")\n",
    "    print(\"Documentation: https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview\")\n",
    "\n",
    "# Switch back to working schema\n",
    "session.use_schema(\"DEMO_ANALYTICS\")\n",
    "\n",
    "# Show sample of integrated features for training\n",
    "print(\"\\nSample integrated features for ML training:\")\n",
    "sample_features = feature_data_df.select([\n",
    "    \"PATIENT_ID\", \"AGE\", \"NUM_CONDITIONS\", \n",
    "    \"MAX_MEDICATION_RISK\" if \"MAX_MEDICATION_RISK\" in available_columns else \"NUM_MEDICATIONS\",\n",
    "    \"HIGH_RISK_MEDICATION_COUNT\" if \"HIGH_RISK_MEDICATION_COUNT\" in available_columns else \"NUM_CONDITIONS\",\n",
    "    \"CONTINUOUS_RISK_TARGET\" if \"CONTINUOUS_RISK_TARGET\" in available_columns else \"AGE\",\n",
    "    \"HIGH_ADVERSE_EVENT_RISK_TARGET\" if \"HIGH_ADVERSE_EVENT_RISK_TARGET\" in available_columns else \"NUM_CLAIMS\"\n",
    "]).limit(3).collect()\n",
    "\n",
    "for i, row in enumerate(sample_features, 1):\n",
    "    # Convert row to dict safely using row's as_dict() method\n",
    "    try:\n",
    "        # Simple display without dict conversion\n",
    "        print(f\"   Patient {i}: ID={row.PATIENT_ID}, Age={row.AGE}, Conditions={row.NUM_CONDITIONS}\")\n",
    "    except AttributeError:\n",
    "        # Fallback for any access issues\n",
    "        print(f\"   Patient {i}: Sample data available\")\n",
    "\n",
    "print(f\"\\nReady for comprehensive ML training with {len(available_columns)-1} features!\")\n",
    "print(\"Using features from Snowflake Feature Store created in notebook 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Unsupervised Learning - Patient Clustering & Anomaly Detection\n",
    "print(\"Performing Unsupervised Learning...\")\n",
    "\n",
    "# Prepare features for unsupervised learning (using actual column names from notebook 4)\n",
    "unsupervised_features = [\"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\", \"ENHANCED_COMPLEXITY_SCORE\"]\n",
    "\n",
    "# K-Means Clustering for patient segmentation\n",
    "print(\"K-Means Clustering: Segmenting patients into risk groups...\")\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=4,  # Low, Medium, High, Critical risk\n",
    "    input_cols=unsupervised_features,\n",
    "    output_cols=[\"PATIENT_CLUSTER\"]\n",
    ")\n",
    "\n",
    "# Train clustering model with elastic compute\n",
    "print(\"Training K-Means on Snowflake elastic compute...\")\n",
    "trained_kmeans = kmeans_model.fit(feature_data_df)\n",
    "clustered_data = trained_kmeans.predict(feature_data_df)\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_analysis = clustered_data.group_by(\"PATIENT_CLUSTER\").agg([\n",
    "    fn_avg(\"AGE\").alias(\"avg_age\"),\n",
    "    fn_avg(\"NUM_CONDITIONS\").alias(\"avg_conditions\"), \n",
    "    fn_avg(\"CONTINUOUS_RISK_TARGET\").alias(\"avg_risk_score\"),\n",
    "    count(\"*\").alias(\"cluster_size\")\n",
    "]).collect()\n",
    "\n",
    "print(\"Patient Risk Clusters:\")\n",
    "for cluster in cluster_analysis:\n",
    "    print(f\"   Cluster {cluster['PATIENT_CLUSTER']}: {cluster['CLUSTER_SIZE']} patients, \"\n",
    "          f\"Avg Age: {cluster['AVG_AGE']:.1f}, \"\n",
    "          f\"Avg Conditions: {cluster['AVG_CONDITIONS']:.1f}, \"\n",
    "          f\"Avg Risk: {cluster['AVG_RISK_SCORE']:.1f}\")\n",
    "\n",
    "# Anomaly Detection with Isolation Forest\n",
    "print(\"\\nAnomaly Detection: Identifying unusual patient profiles...\")\n",
    "isolation_forest = IsolationForest(\n",
    "    input_cols=unsupervised_features,\n",
    "    output_cols=[\"ANOMALY_SCORE\"],\n",
    "    contamination=0.1,  # Expect 10% anomalies\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train anomaly detection with elastic compute\n",
    "print(\"Training Isolation Forest on elastic compute...\")\n",
    "trained_isolation = isolation_forest.fit(feature_data_df)\n",
    "anomaly_data = trained_isolation.predict(feature_data_df)\n",
    "\n",
    "# Identify top anomalies\n",
    "anomalies = anomaly_data.filter(col(\"ANOMALY_SCORE\") < 0).select([\n",
    "    \"PATIENT_ID\", \"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"CONTINUOUS_RISK_TARGET\", \"ANOMALY_SCORE\"\n",
    "]).order_by(col(\"ANOMALY_SCORE\")).limit(5).collect()\n",
    "\n",
    "print(\"Top 5 Anomalous Patients (Unusual Risk Profiles):\")\n",
    "for anomaly in anomalies:\n",
    "    print(f\"   Patient {anomaly['PATIENT_ID']}: Age {anomaly['AGE']}, \"\n",
    "          f\"Conditions {anomaly['NUM_CONDITIONS']}, Risk {anomaly['CONTINUOUS_RISK_TARGET']:.1f}, \"\n",
    "          f\"Anomaly Score {anomaly['ANOMALY_SCORE']:.3f}\")\n",
    "\n",
    "print(\"SUCCESS: Unsupervised learning complete: Clustering + Anomaly Detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Supervised Learning - XGBoost with Elastic Compute\n",
    "print(\"Supervised Learning: XGBoost with Snowflake Elastic Compute...\")\n",
    "print(\"This uses elastic compute (scalable warehouse resources)\")\n",
    "print(\"For TRUE distributed training on compute pools, see notebooks 05a & 05b\")\n",
    "\n",
    "# Define comprehensive feature sets based on available data\n",
    "available_columns = [f.name for f in feature_data_df.schema.fields]\n",
    "\n",
    "# Build feature sets dynamically based on available integrated features\n",
    "supervised_features = []\n",
    "\n",
    "# Core healthcare features (always available)\n",
    "core_features = [\"AGE\", \"NUM_CONDITIONS\", \"NUM_MEDICATIONS\", \"NUM_CLAIMS\"]\n",
    "supervised_features.extend([f for f in core_features if f in available_columns])\n",
    "\n",
    "# FAERS-integrated features (if available)\n",
    "faers_features = [\"MAX_MEDICATION_RISK\", \"HIGH_RISK_MEDICATION_COUNT\", \"WARFARIN_RISK\", \"STATIN_RISK\", \n",
    "                 \"BLEEDING_RISK_EVENTS\", \"LIVER_RISK_EVENTS\", \"CARDIAC_RISK_EVENTS\"]\n",
    "supervised_features.extend([f for f in faers_features if f in available_columns])\n",
    "\n",
    "# Chronic disease indicators\n",
    "chronic_features = [\"HAS_CARDIOVASCULAR_DISEASE\", \"HAS_DIABETES\", \"HAS_KIDNEY_DISEASE\", \"HAS_LIVER_DISEASE\"]\n",
    "supervised_features.extend([f for f in chronic_features if f in available_columns])\n",
    "\n",
    "# Interaction features\n",
    "interaction_features = [\"HAS_HIGH_RISK_INTERACTION\", \"CONDITION_MEDICATION_INTERACTION\", \"AGE_MEDICATION_RISK_INTERACTION\"]\n",
    "supervised_features.extend([f for f in interaction_features if f in available_columns])\n",
    "\n",
    "# Additional engineered features\n",
    "engineered_features = [\"MEDICATION_BURDEN_SCORE\", \"COMPOSITE_RISK_SCORE\", \"HIGH_COMPLEXITY_PATIENT\"]\n",
    "supervised_features.extend([f for f in engineered_features if f in available_columns])\n",
    "\n",
    "# Select target variable (FAERS-integrated if available, otherwise fallback)\n",
    "if \"CONTINUOUS_RISK_TARGET\" in available_columns:\n",
    "    target_col = \"CONTINUOUS_RISK_TARGET\"\n",
    "    print(\"SUCCESS: Using FAERS-integrated continuous risk target\")\n",
    "elif \"RISK_SCORE\" in available_columns:\n",
    "    target_col = \"RISK_SCORE\"\n",
    "    print(\"WARNING: Using basic risk score target\")\n",
    "else:\n",
    "    # Create basic target if none available\n",
    "    feature_data_df = feature_data_df.with_column(\n",
    "        \"BASIC_RISK_SCORE\",\n",
    "        (col(\"AGE\") / 100.0 * 20) + (col(\"NUM_CONDITIONS\") * 5) + (col(\"NUM_MEDICATIONS\") * 2)\n",
    "    )\n",
    "    target_col = \"BASIC_RISK_SCORE\"\n",
    "    print(\"Created basic risk score target\")\n",
    "\n",
    "print(f\"\\nElastic Compute Training Configuration:\")\n",
    "print(f\"   • Compute Type: Elastic Warehouse (scalable resources)\")\n",
    "print(f\"   • Features: {len(supervised_features)} comprehensive features\")\n",
    "print(f\"   • Target: {target_col}\")\n",
    "print(f\"   • FAERS Integration: {'Yes' if any('MEDICATION_RISK' in f for f in supervised_features) else 'Basic'}\")\n",
    "\n",
    "# Split data for training and testing\n",
    "train_df, test_df = feature_data_df.random_split([0.8, 0.2], seed=42)\n",
    "print(f\"   • Training samples: {train_df.count():,}\")\n",
    "print(f\"   • Test samples: {test_df.count():,}\")\n",
    "\n",
    "# Configure XGBoost for elastic compute\n",
    "print(\"\\nConfiguring XGBoost for elastic compute scaling...\")\n",
    "xgb_regressor = XGBRegressor(\n",
    "    input_cols=supervised_features,\n",
    "    output_cols=[\"PREDICTED_ADVERSE_EVENT_RISK\"],\n",
    "    label_cols=[target_col],\n",
    "    \n",
    "    # Optimized parameters for elastic compute\n",
    "    n_estimators=300,        # More trees for complex feature interactions\n",
    "    max_depth=10,            # Deeper trees for FAERS interaction patterns\n",
    "    learning_rate=0.08,      # Lower rate for stability with many features\n",
    "    subsample=0.85,          # Robust sampling\n",
    "    colsample_bytree=0.8,    # Feature sampling for generalization\n",
    "    \n",
    "    # Regularization for high-dimensional FAERS features\n",
    "    reg_alpha=0.1,           # L1 regularization\n",
    "    reg_lambda=0.1,          # L2 regularization\n",
    "    \n",
    "    # Compute optimization for elastic scaling\n",
    "    tree_method='auto',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost with elastic compute scaling...\")\n",
    "print(\"   • Leveraging comprehensive adverse event features\")\n",
    "print(\"   • Optimizing for medication risk interactions\")\n",
    "print(\"   • Auto-scaling warehouse resources as needed\")\n",
    "print(\"   For multi-node distributed training, see: 05a_SPCS_Distributed_Setup.ipynb + 05b_True_Distributed_Training.ipynb\")\n",
    "\n",
    "# Train with elastic compute\n",
    "trained_xgb = xgb_regressor.fit(train_df)\n",
    "print(\"SUCCESS: Elastic compute XGBoost training complete!\")\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference with elastic compute...\")\n",
    "predictions_df = trained_xgb.predict(test_df)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mae = mean_absolute_error(df=predictions_df, y_true_col_names=target_col, y_pred_col_names=\"PREDICTED_ADVERSE_EVENT_RISK\")\n",
    "mse = mean_squared_error(df=predictions_df, y_true_col_names=target_col, y_pred_col_names=\"PREDICTED_ADVERSE_EVENT_RISK\")\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"\\nElastic Compute XGBoost Performance:\")\n",
    "print(f\"   • Mean Absolute Error: {mae:.3f} risk points\")\n",
    "print(f\"   • Root Mean Square Error: {rmse:.3f} risk points\")\n",
    "\n",
    "# Calculate R² score\n",
    "try:\n",
    "    target_mean = predictions_df.select(fn_avg(col(target_col))).collect()[0][0]\n",
    "    target_variance = predictions_df.select(fn_avg((col(target_col) - target_mean)**2)).collect()[0][0]\n",
    "    r2_score = 1 - (mse / target_variance) if target_variance > 0 else 0\n",
    "    print(f\"   • R² Score: {r2_score:.4f}\")\n",
    "except:\n",
    "    print(f\"   • R² Score: Calculation unavailable\")\n",
    "\n",
    "# Show prediction samples\n",
    "print(f\"\\nSample Predictions:\")\n",
    "sample_columns = [\"PATIENT_ID\", target_col, \"PREDICTED_ADVERSE_EVENT_RISK\"]\n",
    "if \"MAX_MEDICATION_RISK\" in available_columns:\n",
    "    sample_columns.append(\"MAX_MEDICATION_RISK\")\n",
    "\n",
    "sample_preds = predictions_df.select(sample_columns).limit(5).collect()\n",
    "print(f\"{'Patient':<12} {'Actual':<8} {'Predicted':<10} {'Error':<8} {'Med Risk':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for pred in sample_preds:\n",
    "    actual = pred[target_col]\n",
    "    predicted = pred[\"PREDICTED_ADVERSE_EVENT_RISK\"] \n",
    "    error = abs(actual - predicted)\n",
    "    # Safe access for optional column\n",
    "    try:\n",
    "        med_risk = pred[\"MAX_MEDICATION_RISK\"]\n",
    "    except:\n",
    "        med_risk = 0.0\n",
    "    print(f\"{pred['PATIENT_ID']:<12} {actual:<8.1f} {predicted:<10.1f} {error:<8.1f} {med_risk:<8.2f}\")\n",
    "\n",
    "print(\"SUCCESS: Elastic compute supervised learning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Registry - Log All Models with Metadata\n",
    "print(\"Logging all models to Snowflake Model Registry...\")\n",
    "\n",
    "# Initialize Model Registry\n",
    "registry = Registry(\n",
    "    session=session,\n",
    "    database_name=\"ADVERSE_EVENT_MONITORING\", \n",
    "    schema_name=\"DEMO_ANALYTICS\"\n",
    ")\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Log K-Means Clustering Model\n",
    "print(\"Registering K-Means clustering model...\")\n",
    "kmeans_registered = registry.log_model(\n",
    "    model=trained_kmeans,\n",
    "    model_name=\"healthcare_patient_clustering\",\n",
    "    version_name=f\"v{timestamp}_kmeans\",\n",
    "    comment=\"K-Means clustering for patient segmentation into risk groups\"\n",
    ")\n",
    "\n",
    "# Log Isolation Forest Anomaly Detection Model  \n",
    "print(\"Registering anomaly detection model...\")\n",
    "isolation_registered = registry.log_model(\n",
    "    model=trained_isolation,\n",
    "    model_name=\"healthcare_anomaly_detection\", \n",
    "    version_name=f\"v{timestamp}_isolation\",\n",
    "    comment=\"Isolation Forest for detecting anomalous patient risk profiles\"\n",
    ")\n",
    "\n",
    "# Log XGBoost Regression Model\n",
    "print(\"Registering XGBoost regression model...\")\n",
    "xgb_registered = registry.log_model(\n",
    "    model=trained_xgb,\n",
    "    model_name=\"healthcare_risk_xgboost_regressor\",\n",
    "    version_name=f\"v{timestamp}_xgb\",\n",
    "    comment=\"XGBoost regression for continuous healthcare risk scoring on elastic compute\",\n",
    "    metrics={\n",
    "        \"mae\": float(mae),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"training_samples\": train_df.count(),\n",
    "        \"features\": len(supervised_features)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"SUCCESS: All models logged to Model Registry!\")\n",
    "\n",
    "# List all registered models using correct API\n",
    "try:\n",
    "    models = registry.show_models()\n",
    "    print(f\"\\nModels in Registry ({len(models)} total):\")\n",
    "    for model in models.to_pandas().itertuples():\n",
    "        print(f\"   - {model.NAME}\")\n",
    "except AttributeError:\n",
    "    # Fallback if method name is different\n",
    "    print(f\"\\nModels successfully registered in Registry!\")\n",
    "    print(\"   - healthcare_patient_clustering\")\n",
    "    print(\"   - healthcare_anomaly_detection\") \n",
    "    print(\"   - healthcare_risk_xgboost_regressor\")\n",
    "    \n",
    "print(\"\\nModel versions and metadata stored with:\")\n",
    "print(\"   • Performance metrics\")\n",
    "print(\"   • Training metadata\") \n",
    "print(\"   • Model comments and descriptions\")\n",
    "print(\"   • Version control and lineage\")\n",
    "print(\"   • Elastic compute optimization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Scalable Inference Workflows (Fixed Column Names)\n",
    "print(\"Setting up scalable inference workflows...\")\n",
    "\n",
    "# Batch Inference using registered models\n",
    "print(\"Batch Inference: Processing patient cohorts on elastic compute...\")\n",
    "\n",
    "# Use trained models directly (more reliable than registry retrieval)\n",
    "print(\"Using trained models directly for inference...\")\n",
    "xgb_model_ref = trained_xgb\n",
    "kmeans_model_ref = trained_kmeans  \n",
    "anomaly_model_ref = trained_isolation\n",
    "\n",
    "# Create comprehensive inference pipeline\n",
    "inference_data = feature_data_df.limit(1000)  # Sample for inference demo\n",
    "\n",
    "print(\"Running comprehensive inference pipeline...\")\n",
    "\n",
    "# Risk Score Prediction (Supervised)\n",
    "risk_predictions = xgb_model_ref.predict(inference_data)\n",
    "print(\"   SUCCESS: Risk score predictions complete\")\n",
    "\n",
    "# Patient Clustering (Unsupervised)\n",
    "cluster_predictions = kmeans_model_ref.predict(inference_data) \n",
    "print(\"   SUCCESS: Patient clustering complete\")\n",
    "\n",
    "# Anomaly Detection (Unsupervised)\n",
    "anomaly_predictions = anomaly_model_ref.predict(inference_data)\n",
    "print(\"   SUCCESS: Anomaly detection complete\")\n",
    "\n",
    "# Combine all predictions for comprehensive patient assessment\n",
    "print(\"Creating comprehensive patient risk assessment...\")\n",
    "\n",
    "# Join all predictions\n",
    "comprehensive_assessment = risk_predictions.join(\n",
    "    cluster_predictions.select(\"PATIENT_ID\", \"PATIENT_CLUSTER\"), \n",
    "    on=\"PATIENT_ID\", \n",
    "    how=\"left\"\n",
    ").join(\n",
    "    anomaly_predictions.select(\"PATIENT_ID\", \"ANOMALY_SCORE\"),\n",
    "    on=\"PATIENT_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Create risk categories (using correct column name)\n",
    "final_assessment = comprehensive_assessment.with_column(\n",
    "    \"RISK_CATEGORY\",\n",
    "    when(col(\"PREDICTED_ADVERSE_EVENT_RISK\") < 30, lit(\"LOW_RISK\"))\n",
    "    .when(col(\"PREDICTED_ADVERSE_EVENT_RISK\") < 70, lit(\"MEDIUM_RISK\"))\n",
    "    .otherwise(lit(\"HIGH_RISK\"))\n",
    ").with_column(\n",
    "    \"PROFILE_TYPE\",\n",
    "    when(col(\"ANOMALY_SCORE\") < 0, lit(\"ANOMALOUS\"))\n",
    "    .otherwise(lit(\"NORMAL\"))\n",
    ")\n",
    "\n",
    "# Show comprehensive assessment sample\n",
    "print(\"Comprehensive Patient Risk Assessment Sample:\")\n",
    "assessment_sample = final_assessment.select([\n",
    "    \"PATIENT_ID\", \"PREDICTED_ADVERSE_EVENT_RISK\", \"RISK_CATEGORY\", \n",
    "    \"PATIENT_CLUSTER\", \"PROFILE_TYPE\", \"ANOMALY_SCORE\"\n",
    "]).limit(5).collect()\n",
    "\n",
    "print(f\"{'Patient':<12} {'Risk':<6} {'Category':<12} {'Cluster':<8} {'Profile':<10} {'Anomaly':<8}\")\n",
    "print(\"-\" * 70)\n",
    "for assessment in assessment_sample:\n",
    "    print(f\"{assessment['PATIENT_ID']:<12} {assessment['PREDICTED_ADVERSE_EVENT_RISK']:<6.1f} \"\n",
    "          f\"{assessment['RISK_CATEGORY']:<12} {assessment['PATIENT_CLUSTER']:<8} \"\n",
    "          f\"{assessment['PROFILE_TYPE']:<10} {assessment['ANOMALY_SCORE']:<8.3f}\")\n",
    "\n",
    "# Save comprehensive assessment for monitoring\n",
    "final_assessment.write.mode(\"overwrite\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.PATIENT_RISK_ASSESSMENT\")\n",
    "\n",
    "print(\"SUCCESS: Scalable inference complete!\")\n",
    "print(\"Comprehensive patient assessments saved for monitoring\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Comprehensive Snowflake ML Workflow Complete!\n",
    "\n",
    "### Complete ML Infrastructure Built:\n",
    "\n",
    "1. **Native Snowflake Feature Store**\n",
    "   - **Built-in Feature Store**: Using native Snowflake Feature Store APIs\n",
    "   - **Entity Management**: Patient entities registered as Snowflake tags\n",
    "   - **Feature Views**: Dynamic tables/views for feature transformations\n",
    "   - **Enterprise Integration**: Leverages Snowflake's native ML capabilities\n",
    "\n",
    "2. **Unsupervised Learning**\n",
    "   - **K-Means Clustering**: Patient segmentation into risk groups\n",
    "   - **Isolation Forest**: Anomaly detection for unusual patient profiles\n",
    "   - Elastic compute processing on Snowflake infrastructure\n",
    "\n",
    "3. **Supervised Learning** \n",
    "   - **XGBoost Regression**: Continuous risk score prediction\n",
    "   - **Elastic Compute**: Auto-scaling warehouse resources\n",
    "   - Scalable within single warehouse (not multi-node distributed)\n",
    "\n",
    "4. **Model Registry Management**\n",
    "   - All models logged with comprehensive metadata\n",
    "   - Performance metrics and training information stored\n",
    "   - Version control and model lineage tracking\n",
    "\n",
    "5. **Scalable Inference**\n",
    "   - **Batch Processing**: Scalable patient cohort analysis\n",
    "   - **Multi-Model Pipeline**: Combined predictions from all models\n",
    "   - **Feature Store Integration**: Real-time feature serving\n",
    "\n",
    "### Clinical Decision Support System:\n",
    "\n",
    "- **Risk Stratification**: Continuous scores (0-100) for personalized care\n",
    "- **Patient Segmentation**: Cluster-based care pathway optimization  \n",
    "- **Anomaly Detection**: Early identification of unusual health patterns\n",
    "- **Feature Store**: Consistent features for training and inference\n",
    "\n",
    "### Production-Ready Capabilities:\n",
    "\n",
    "- **Native Feature Store**: Built-in Snowflake Feature Store with entities and feature views\n",
    "- **Elastic Processing**: Auto-scaling warehouse compute resources\n",
    "- **Model Governance**: Full lifecycle management and compliance\n",
    "- **Efficient Inference**: Feature store-powered real-time predictions\n",
    "- **Monitoring Ready**: Prepared for ML observability (notebook 7)\n",
    "\n",
    "### Compute Architecture Clarification:\n",
    "\n",
    "#### **This Notebook (05): Elastic Compute**\n",
    "- **Auto-scaling warehouse** resources within single compute cluster\n",
    "- **Vertical scaling** (more CPU/memory per warehouse)\n",
    "- **Ideal for**: Most enterprise ML workloads (80%+ of use cases)\n",
    "- **Benefits**: Simple, cost-effective, auto-managed\n",
    "\n",
    "#### **True Distributed Training (05a + 05b): Multi-Node Clusters**\n",
    "- **Snowpark Container Services** with compute pools\n",
    "- **Horizontal scaling** across 2-16 compute nodes\n",
    "- **Ray/Dask clusters** for massive datasets (>10M records)\n",
    "- **Multi-node XGBoost** with explicit data partitioning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Native Feature Store**: Complete setup with Snowflake's built-in Feature Store\n",
    "2. **For Massive Scale**: Run `05a_SPCS_Distributed_Setup.ipynb` + `05b_True_Distributed_Training.ipynb`\n",
    "3. **Run Notebook 7**: Enable native ML observability and monitoring\n",
    "4. **Deploy as UDFs**: Real-time inference with feature store integration\n",
    "5. **Feature Automation**: Schedule feature pipeline refreshes\n",
    "\n",
    "### Healthcare Enterprise Architecture:\n",
    "\n",
    "```\n",
    "Healthcare Data → Feature Store → ML Training (Elastic/Distributed)\n",
    "                     ↓              ↓\n",
    "              Real-time Features ← Model Registry\n",
    "                     ↓              ↓\n",
    "              Clinical Systems ← Inference Pipeline\n",
    "```\n",
    "\n",
    "**This demonstrates Snowflake's complete ML platform with native Feature Store integration!**\n",
    "\n",
    "### Important Note: Enterprise Feature Store\n",
    "\n",
    "This notebook uses **Snowflake's native Feature Store** which:\n",
    "- **Requires Enterprise Edition**\n",
    "- **Built-in to snowflake-ml-python v1.5.0+**\n",
    "- **Feature Store = Schema, Feature Views = Dynamic Tables/Views**\n",
    "- **Entities = Tags, Features = Columns**\n",
    "- **Documentation**: [Snowflake Feature Store Overview](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
