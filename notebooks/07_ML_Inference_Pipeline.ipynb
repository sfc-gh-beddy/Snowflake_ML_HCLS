{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîÆ Production ML Inference Pipeline\n",
        "\n",
        "**Production-ready inference with UDFs, Streamlit applications, and real-time scoring capabilities**\n",
        "\n",
        "## üéØ **Inference Objectives:**\n",
        "1. **‚ö° Real-time Inference** - Individual patient risk scoring via UDFs\n",
        "2. **üìä Batch Inference** - Large-scale patient cohort processing\n",
        "3. **üñ•Ô∏è Streamlit Application** - Interactive healthcare dashboard\n",
        "4. **üîÑ Streaming Inference** - Real-time data pipeline integration\n",
        "5. **üìã API Integration** - REST endpoints for external system access\n",
        "\n",
        "## üõ†Ô∏è **Inference Components:**\n",
        "- **Production UDFs**: Scalable inference functions\n",
        "- **Streamlit Dashboard**: Interactive clinical decision support\n",
        "- **Batch Processing**: Automated large-scale scoring\n",
        "- **Real-time APIs**: External system integration\n",
        "- **Performance Monitoring**: Latency and throughput tracking\n",
        "\n",
        "**Prerequisites:** Run notebooks 05 (Training) and 06 (Evaluation) first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Added to Python path: /Users/beddy/Desktop/Github/Snowflake_ML_HCLS/notebooks/../src\n",
            "üîÑ Reusing existing Snowflake session\n",
            "‚úÖ Environment ready for production inference\n",
            "üîÆ Capabilities: Real-time UDFs, Batch Processing, Streamlit Integration\n",
            "‚ö° Tools: Production inference, monitoring, external API integration\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup for Production Inference\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "\n",
        "# Fix path for snowflake_connection module\n",
        "current_dir = os.getcwd()\n",
        "if \"notebooks\" in current_dir:\n",
        "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
        "else:\n",
        "    src_path = os.path.join(current_dir, \"src\")\n",
        "\n",
        "sys.path.append(src_path)\n",
        "print(f\"üìÅ Added to Python path: {src_path}\")\n",
        "\n",
        "from snowflake_connection import get_session\n",
        "from snowflake.snowpark.functions import (\n",
        "    col, lit, when, count, avg, sum as sum_, max as max_, min as min_,\n",
        "    current_timestamp, call_udf, sql_expr, udf\n",
        ")\n",
        "from snowflake.snowpark.types import (\n",
        "    StructType, StructField, StringType, DoubleType, IntegerType,\n",
        "    FloatType, BooleanType, TimestampType\n",
        ")\n",
        "\n",
        "# Get Snowflake session\n",
        "session = get_session()\n",
        "print(\"‚úÖ Environment ready for production inference\")\n",
        "print(\"üîÆ Capabilities: Real-time UDFs, Batch Processing, Streamlit Integration\")\n",
        "print(\"‚ö° Tools: Production inference, monitoring, external API integration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up ML infrastructure...\n",
            "‚ö†Ô∏è UDF creation is REQUIRED - will not proceed without it\n",
            "üìÅ Creating ML models stage...\n",
            "‚úÖ ML models stage created with full schema qualification\n",
            "üîß Creating healthcare risk scoring UDF...\n",
            "‚úÖ UDF created WITH stage location: @ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\n",
            "üß™ Testing UDF creation...\n",
            "‚úÖ UDF test SUCCESSFUL - Test risk score: 80.25\n",
            "üéØ UDF is working and ready for production inference!\n",
            "‚úÖ ALL CHECKS PASSED - UDF is fully operational\n"
          ]
        }
      ],
      "source": [
        "# Stage and UDF Setup for Inference - MANDATORY UDF CREATION\n",
        "print(\"üîß Setting up ML infrastructure...\")\n",
        "print(\"‚ö†Ô∏è UDF creation is REQUIRED - will not proceed without it\")\n",
        "\n",
        "# Step 1: Create stage using multiple approaches until one works\n",
        "print(\"üìÅ Creating ML models stage...\")\n",
        "stage_created = False\n",
        "stage_name = None\n",
        "\n",
        "# Approach 1: Try with full schema qualification\n",
        "try:\n",
        "    session.sql(\"\"\"\n",
        "        CREATE STAGE IF NOT EXISTS ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\n",
        "        COMMENT = 'Stage for storing ML model artifacts and UDF dependencies'\n",
        "    \"\"\").collect()\n",
        "    stage_name = \"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\"\n",
        "    stage_created = True\n",
        "    print(\"‚úÖ ML models stage created with full schema qualification\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Full schema stage creation failed: {e}\")\n",
        "\n",
        "# Approach 2: Try with current schema only\n",
        "if not stage_created:\n",
        "    try:\n",
        "        session.sql(\"CREATE STAGE IF NOT EXISTS ML_MODELS_STAGE\").collect()\n",
        "        stage_name = \"ML_MODELS_STAGE\"\n",
        "        stage_created = True\n",
        "        print(\"‚úÖ ML models stage created in current schema\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Current schema stage creation failed: {e}\")\n",
        "\n",
        "# Approach 3: Try with default temp stage\n",
        "if not stage_created:\n",
        "    try:\n",
        "        session.sql(\"CREATE STAGE IF NOT EXISTS TEMP_ML_STAGE\").collect()\n",
        "        stage_name = \"TEMP_ML_STAGE\"\n",
        "        stage_created = True\n",
        "        print(\"‚úÖ Temporary ML stage created\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Temp stage creation failed: {e}\")\n",
        "\n",
        "if not stage_created:\n",
        "    print(\"‚ùå CRITICAL: Could not create any stage\")\n",
        "    print(\"üí° Trying UDF creation without stage location...\")\n",
        "    stage_name = None\n",
        "\n",
        "# Step 2: Create UDF with stage if available, without stage if necessary\n",
        "print(\"üîß Creating healthcare risk scoring UDF...\")\n",
        "\n",
        "# Try with stage first if available\n",
        "udf_created = False\n",
        "if stage_name:\n",
        "    try:\n",
        "        @udf(name=\"healthcare_risk_score_udf\", \n",
        "             input_types=[FloatType(), IntegerType(), IntegerType(), IntegerType()],\n",
        "             return_type=FloatType(),\n",
        "             replace=True,\n",
        "             stage_location=f\"@{stage_name}\")\n",
        "        def healthcare_risk_score_with_stage(age: float, conditions: int, medications: int, claims: int) -> float:\n",
        "            \"\"\"Healthcare risk scoring UDF with stage location\"\"\"\n",
        "            base_risk = (age / 100.0) * 25\n",
        "            condition_risk = conditions * 6\n",
        "            medication_risk = medications * 3\n",
        "            utilization_risk = (claims / 10.0) * 4\n",
        "            total_risk = base_risk + condition_risk + medication_risk + utilization_risk\n",
        "            return min(100.0, max(0.0, total_risk))\n",
        "        \n",
        "        udf_created = True\n",
        "        print(f\"‚úÖ UDF created WITH stage location: @{stage_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è UDF creation with stage failed: {e}\")\n",
        "        print(\"üîÑ Trying without stage location...\")\n",
        "\n",
        "# Try without stage if stage approach failed\n",
        "if not udf_created:\n",
        "    try:\n",
        "        @udf(name=\"healthcare_risk_score_udf\", \n",
        "             input_types=[FloatType(), IntegerType(), IntegerType(), IntegerType()],\n",
        "             return_type=FloatType(),\n",
        "             replace=True)\n",
        "        def healthcare_risk_score_no_stage(age: float, conditions: int, medications: int, claims: int) -> float:\n",
        "            \"\"\"Healthcare risk scoring UDF without stage location\"\"\"\n",
        "            base_risk = (age / 100.0) * 25\n",
        "            condition_risk = conditions * 6\n",
        "            medication_risk = medications * 3\n",
        "            utilization_risk = (claims / 10.0) * 4\n",
        "            total_risk = base_risk + condition_risk + medication_risk + utilization_risk\n",
        "            return min(100.0, max(0.0, total_risk))\n",
        "        \n",
        "        udf_created = True\n",
        "        print(\"‚úÖ UDF created WITHOUT stage location\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL ERROR: UDF creation failed completely: {e}\")\n",
        "        print(\"üö® Cannot proceed without UDF - check permissions and database setup\")\n",
        "        raise Exception(f\"UDF creation is mandatory but failed: {e}\")\n",
        "\n",
        "# Step 3: Test the UDF - MANDATORY\n",
        "print(\"üß™ Testing UDF creation...\")\n",
        "try:\n",
        "    test_result = session.sql(\"\"\"\n",
        "        SELECT healthcare_risk_score_udf(65.0, 5, 8, 25) as RISK_SCORE\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    risk_score = test_result[0]['RISK_SCORE']\n",
        "    print(f\"‚úÖ UDF test SUCCESSFUL - Test risk score: {risk_score:.2f}\")\n",
        "    print(\"üéØ UDF is working and ready for production inference!\")\n",
        "    udf_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL ERROR: UDF test failed: {e}\")\n",
        "    print(\"üö® UDF exists but is not functional\")\n",
        "    raise Exception(f\"UDF test is mandatory but failed: {e}\")\n",
        "\n",
        "print(\"‚úÖ ALL CHECKS PASSED - UDF is fully operational\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° Setting up real-time inference pipeline...\n",
            "‚úÖ UDF is confirmed operational - proceeding with UDF-only inference\n",
            "üß™ Testing UDF-based inference pipeline...\n",
            "   Patient TEST_001: 80.2 (HIGH) - 1385.1ms [UDF]\n",
            "   Patient TEST_002: 25.8 (LOW) - 1152.04ms [UDF]\n",
            "   Patient TEST_003: 100.0 (HIGH) - 1081.1ms [UDF]\n",
            "‚úÖ UDF-based inference pipeline is operational\n"
          ]
        }
      ],
      "source": [
        "# Real-time Inference Pipeline Setup - UDF REQUIRED\n",
        "print(\"‚ö° Setting up real-time inference pipeline...\")\n",
        "print(\"‚úÖ UDF is confirmed operational - proceeding with UDF-only inference\")\n",
        "\n",
        "# Create real-time inference wrapper function\n",
        "def predict_patient_risk(patient_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Real-time patient risk prediction using UDF (REQUIRED)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Extract patient features\n",
        "        age = float(patient_data.get('age', 0))\n",
        "        conditions = int(patient_data.get('num_conditions', 0))\n",
        "        medications = int(patient_data.get('num_medications', 0))\n",
        "        claims = int(patient_data.get('num_claims', 0))\n",
        "        \n",
        "        # Make prediction using UDF (MANDATORY - no fallback)\n",
        "        prediction_sql = f\"\"\"\n",
        "            SELECT \n",
        "                healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) as RISK_SCORE,\n",
        "                CASE \n",
        "                    WHEN healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) < 30 THEN 'LOW'\n",
        "                    WHEN healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) < 70 THEN 'MEDIUM'\n",
        "                    ELSE 'HIGH'\n",
        "                END as RISK_CATEGORY\n",
        "        \"\"\"\n",
        "        \n",
        "        result = session.sql(prediction_sql).collect()[0]\n",
        "        risk_score = float(result['RISK_SCORE'])\n",
        "        risk_category = result['RISK_CATEGORY']\n",
        "        \n",
        "        # Calculate prediction metadata\n",
        "        prediction_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "        \n",
        "        # Prepare comprehensive response\n",
        "        response = {\n",
        "            'patient_id': patient_data.get('patient_id', 'UNKNOWN'),\n",
        "            'risk_score': risk_score,\n",
        "            'risk_category': risk_category,\n",
        "            'prediction_timestamp': datetime.datetime.now().isoformat(),\n",
        "            'prediction_time_ms': round(prediction_time, 2),\n",
        "            'model_version': 'v1.0.0',\n",
        "            'confidence': 0.85,  # Simulated confidence score\n",
        "            'input_features': {\n",
        "                'age': age,\n",
        "                'num_conditions': conditions,\n",
        "                'num_medications': medications,\n",
        "                'num_claims': claims\n",
        "            },\n",
        "            'clinical_recommendations': generate_clinical_recommendations(risk_score, risk_category),\n",
        "            'success': True,\n",
        "            'inference_method': 'UDF'  # Always UDF\n",
        "        }\n",
        "        \n",
        "        # Log inference request\n",
        "        log_inference_request(response)\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        # If UDF fails, this is a critical error since UDF is mandatory\n",
        "        print(f\"‚ùå CRITICAL: UDF inference failed: {e}\")\n",
        "        error_response = {\n",
        "            'patient_id': patient_data.get('patient_id', 'UNKNOWN'),\n",
        "            'error': f\"UDF_FAILURE: {str(e)}\",\n",
        "            'prediction_timestamp': datetime.datetime.now().isoformat(),\n",
        "            'prediction_time_ms': (time.time() - start_time) * 1000,\n",
        "            'success': False\n",
        "        }\n",
        "        \n",
        "        return error_response\n",
        "\n",
        "def generate_clinical_recommendations(risk_score: float, risk_category: str) -> List[str]:\n",
        "    \"\"\"Generate clinical recommendations based on risk score\"\"\"\n",
        "    \n",
        "    recommendations = []\n",
        "    \n",
        "    if risk_category == 'HIGH':\n",
        "        recommendations.extend([\n",
        "            \"üö® High risk patient - Consider immediate clinical review\",\n",
        "            \"üìã Review medication interactions and dosages\", \n",
        "            \"ü©∫ Schedule follow-up within 2 weeks\",\n",
        "            \"üìä Monitor vital signs and laboratory values closely\"\n",
        "        ])\n",
        "    elif risk_category == 'MEDIUM':\n",
        "        recommendations.extend([\n",
        "            \"‚ö†Ô∏è Moderate risk - Schedule routine follow-up\",\n",
        "            \"üíä Review medication adherence\",\n",
        "            \"üìÖ Consider preventive care measures\",\n",
        "            \"üìà Monitor for symptom progression\"\n",
        "        ])\n",
        "    else:  # LOW\n",
        "        recommendations.extend([\n",
        "            \"‚úÖ Low risk - Continue routine care\",\n",
        "            \"üèÉ Encourage healthy lifestyle maintenance\",\n",
        "            \"üìÖ Schedule annual wellness check\",\n",
        "            \"üìö Provide patient education resources\"\n",
        "        ])\n",
        "    \n",
        "    # Add specific recommendations based on risk score\n",
        "    if risk_score > 80:\n",
        "        recommendations.append(\"üè• Consider hospitalization or intensive monitoring\")\n",
        "    elif risk_score > 60:\n",
        "        recommendations.append(\"üîÑ Increase monitoring frequency\")\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "def log_inference_request(response: Dict[str, Any]):\n",
        "    \"\"\"Log inference request for monitoring and analysis\"\"\"\n",
        "    \n",
        "    try:\n",
        "        log_data = [(\n",
        "            f\"REQ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{response['patient_id']}\",\n",
        "            'healthcare_risk_model',\n",
        "            datetime.datetime.now().isoformat(),\n",
        "            response['prediction_time_ms'],\n",
        "            json.dumps(response['input_features']),\n",
        "            response.get('risk_score', 0.0),\n",
        "            'INFERENCE_PIPELINE',\n",
        "            response['success']\n",
        "        )]\n",
        "        \n",
        "        log_schema = StructType([\n",
        "            StructField(\"REQUEST_ID\", StringType()),\n",
        "            StructField(\"MODEL_NAME\", StringType()),\n",
        "            StructField(\"REQUEST_TIMESTAMP\", StringType()),\n",
        "            StructField(\"RESPONSE_TIME_MS\", DoubleType()),\n",
        "            StructField(\"INPUT_FEATURES\", StringType()),\n",
        "            StructField(\"PREDICTION_RESULT\", DoubleType()),\n",
        "            StructField(\"REQUEST_SOURCE\", StringType()),\n",
        "            StructField(\"SUCCESS_STATUS\", BooleanType())\n",
        "        ])\n",
        "        \n",
        "        log_df = session.create_dataframe(log_data, schema=log_schema)\n",
        "        log_df.write.mode(\"append\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.INFERENCE_REQUEST_LOG\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Logging error: {e}\")\n",
        "\n",
        "# Test real-time inference with UDF\n",
        "print(\"üß™ Testing UDF-based inference pipeline...\")\n",
        "\n",
        "test_patients = [\n",
        "    {'patient_id': 'TEST_001', 'age': 65, 'num_conditions': 5, 'num_medications': 8, 'num_claims': 25},\n",
        "    {'patient_id': 'TEST_002', 'age': 35, 'num_conditions': 2, 'num_medications': 1, 'num_claims': 5},\n",
        "    {'patient_id': 'TEST_003', 'age': 78, 'num_conditions': 12, 'num_medications': 15, 'num_claims': 45}\n",
        "]\n",
        "\n",
        "for patient in test_patients:\n",
        "    result = predict_patient_risk(patient)\n",
        "    if result['success']:\n",
        "        print(f\"   Patient {result['patient_id']}: {result['risk_score']:.1f} ({result['risk_category']}) - {result['prediction_time_ms']}ms [UDF]\")\n",
        "    else:\n",
        "        print(f\"   Patient {patient['patient_id']}: ‚ùå {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "print(\"‚úÖ UDF-based inference pipeline is operational\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è Creating Streamlit healthcare dashboard...\n",
            "‚úÖ Streamlit app created: healthcare_dashboard.py\n",
            "üìù To run the dashboard:\n",
            "   streamlit run healthcare_dashboard.py\n",
            "\n",
            "üéØ Dashboard Features:\n",
            "   ‚Ä¢ Real-time patient risk assessment\n",
            "   ‚Ä¢ Interactive risk gauge and visualizations\n",
            "   ‚Ä¢ Clinical recommendations based on risk score\n",
            "   ‚Ä¢ Analytics dashboard with inference logs\n",
            "   ‚Ä¢ Model performance monitoring\n",
            "   ‚Ä¢ Responsive design with sidebar controls\n"
          ]
        }
      ],
      "source": [
        "# Streamlit Healthcare Dashboard Application\n",
        "print(\"üñ•Ô∏è Creating Streamlit healthcare dashboard...\")\n",
        "\n",
        "# Note: Streamlit imports are in the external file, not in this notebook\n",
        "\n",
        "# Create the Streamlit application code\n",
        "streamlit_app_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Fix path for snowflake_connection module\n",
        "current_dir = os.getcwd()\n",
        "if \"notebooks\" in current_dir:\n",
        "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
        "else:\n",
        "    src_path = os.path.join(current_dir, \"src\")\n",
        "\n",
        "sys.path.append(src_path)\n",
        "\n",
        "from snowflake_connection import get_session\n",
        "from snowflake.snowpark.functions import col, lit, when, count, avg, sum as sum_, max as max_, min as min_\n",
        "\n",
        "# Initialize Snowflake session\n",
        "@st.cache_resource\n",
        "def get_snowflake_session():\n",
        "    return get_session()\n",
        "\n",
        "session = get_snowflake_session()\n",
        "\n",
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"üè• Healthcare Risk Assessment Dashboard\",\n",
        "    page_icon=\"üè•\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Dashboard title and header\n",
        "st.title(\"üè• Healthcare Risk Assessment Dashboard\")\n",
        "st.markdown(\"### Real-time Patient Risk Scoring & Clinical Decision Support\")\n",
        "\n",
        "# Sidebar for patient input\n",
        "st.sidebar.header(\"ü©∫ Patient Risk Assessment\")\n",
        "\n",
        "# Patient input form\n",
        "with st.sidebar.form(\"patient_form\"):\n",
        "    st.subheader(\"Enter Patient Information\")\n",
        "    \n",
        "    patient_id = st.text_input(\"Patient ID\", value=\"PAT_001\")\n",
        "    age = st.slider(\"Age\", 0, 120, 65)\n",
        "    num_conditions = st.slider(\"Number of Conditions\", 0, 20, 5)\n",
        "    num_medications = st.slider(\"Number of Medications\", 0, 30, 8)\n",
        "    num_claims = st.slider(\"Number of Claims (last year)\", 0, 100, 25)\n",
        "    \n",
        "    submitted = st.form_submit_button(\"üîç Calculate Risk Score\")\n",
        "\n",
        "# Main dashboard content\n",
        "if submitted:\n",
        "    # Calculate risk using UDF\n",
        "    with st.spinner(\"Calculating risk score...\"):\n",
        "        try:\n",
        "            # Call the healthcare risk UDF\n",
        "            prediction_sql = f\"\"\"\n",
        "                SELECT \n",
        "                    healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) as RISK_SCORE,\n",
        "                    CASE \n",
        "                        WHEN healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) < 30 THEN 'LOW'\n",
        "                        WHEN healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) < 70 THEN 'MEDIUM'\n",
        "                        ELSE 'HIGH'\n",
        "                    END as RISK_CATEGORY\n",
        "            \"\"\"\n",
        "            \n",
        "            result = session.sql(prediction_sql).collect()[0]\n",
        "            risk_score = float(result['RISK_SCORE'])\n",
        "            risk_category = result['RISK_CATEGORY']\n",
        "            \n",
        "            # Display results\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            \n",
        "            with col1:\n",
        "                if risk_category == 'HIGH':\n",
        "                    st.error(f\"üö® **HIGH RISK**\")\n",
        "                elif risk_category == 'MEDIUM':\n",
        "                    st.warning(f\"‚ö†Ô∏è **MEDIUM RISK**\")\n",
        "                else:\n",
        "                    st.success(f\"‚úÖ **LOW RISK**\")\n",
        "                \n",
        "                st.metric(\"Risk Score\", f\"{risk_score:.1f}\")\n",
        "            \n",
        "            with col2:\n",
        "                st.metric(\"Patient ID\", patient_id)\n",
        "                st.metric(\"Age\", f\"{age} years\")\n",
        "            \n",
        "            with col3:\n",
        "                st.metric(\"Conditions\", num_conditions)\n",
        "                st.metric(\"Medications\", num_medications)\n",
        "                st.metric(\"Claims\", num_claims)\n",
        "            \n",
        "            # Risk gauge chart\n",
        "            fig_gauge = go.Figure(go.Indicator(\n",
        "                mode = \"gauge+number+delta\",\n",
        "                value = risk_score,\n",
        "                domain = {'x': [0, 1], 'y': [0, 1]},\n",
        "                title = {'text': \"Healthcare Risk Score\"},\n",
        "                delta = {'reference': 50},\n",
        "                gauge = {\n",
        "                    'axis': {'range': [None, 100]},\n",
        "                    'bar': {'color': \"darkblue\"},\n",
        "                    'steps': [\n",
        "                        {'range': [0, 30], 'color': \"lightgreen\"},\n",
        "                        {'range': [30, 70], 'color': \"yellow\"},\n",
        "                        {'range': [70, 100], 'color': \"red\"}\n",
        "                    ],\n",
        "                    'threshold': {\n",
        "                        'line': {'color': \"red\", 'width': 4},\n",
        "                        'thickness': 0.75,\n",
        "                        'value': 90\n",
        "                    }\n",
        "                }\n",
        "            ))\n",
        "            \n",
        "            fig_gauge.update_layout(height=400)\n",
        "            st.plotly_chart(fig_gauge, use_container_width=True)\n",
        "            \n",
        "            # Clinical recommendations\n",
        "            st.subheader(\"üìã Clinical Recommendations\")\n",
        "            \n",
        "            if risk_category == 'HIGH':\n",
        "                recommendations = [\n",
        "                    \"üö® High risk patient - Consider immediate clinical review\",\n",
        "                    \"üìã Review medication interactions and dosages\",\n",
        "                    \"ü©∫ Schedule follow-up within 2 weeks\",\n",
        "                    \"üìä Monitor vital signs and laboratory values closely\"\n",
        "                ]\n",
        "                if risk_score > 80:\n",
        "                    recommendations.append(\"üè• Consider hospitalization or intensive monitoring\")\n",
        "            elif risk_category == 'MEDIUM':\n",
        "                recommendations = [\n",
        "                    \"‚ö†Ô∏è Moderate risk - Schedule routine follow-up\",\n",
        "                    \"üíä Review medication adherence\",\n",
        "                    \"üìÖ Consider preventive care measures\",\n",
        "                    \"üìà Monitor for symptom progression\"\n",
        "                ]\n",
        "                if risk_score > 60:\n",
        "                    recommendations.append(\"üîÑ Increase monitoring frequency\")\n",
        "            else:\n",
        "                recommendations = [\n",
        "                    \"‚úÖ Low risk - Continue routine care\",\n",
        "                    \"üèÉ Encourage healthy lifestyle maintenance\",\n",
        "                    \"üìÖ Schedule annual wellness check\",\n",
        "                    \"üìö Provide patient education resources\"\n",
        "                ]\n",
        "            \n",
        "            for rec in recommendations:\n",
        "                st.write(f\"‚Ä¢ {rec}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Error calculating risk: {e}\")\n",
        "\n",
        "# Analytics section\n",
        "st.header(\"üìä Analytics Dashboard\")\n",
        "\n",
        "# Fetch inference logs\n",
        "try:\n",
        "    logs_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.INFERENCE_REQUEST_LOG\").to_pandas()\n",
        "    \n",
        "    if not logs_df.empty:\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.subheader(\"üìà Recent Inference Requests\")\n",
        "            st.dataframe(logs_df.tail(10))\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"‚ö° Response Time Distribution\")\n",
        "            if 'RESPONSE_TIME_MS' in logs_df.columns:\n",
        "                fig_hist = px.histogram(\n",
        "                    logs_df, \n",
        "                    x='RESPONSE_TIME_MS',\n",
        "                    title=\"Response Time Distribution (ms)\",\n",
        "                    nbins=20\n",
        "                )\n",
        "                st.plotly_chart(fig_hist, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No inference requests logged yet. Submit a patient assessment to see analytics.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    st.warning(f\"Analytics data not available: {e}\")\n",
        "\n",
        "# Model evaluation results\n",
        "st.header(\"üéØ Model Performance\")\n",
        "\n",
        "try:\n",
        "    eval_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_EVALUATION_LOG\").to_pandas()\n",
        "    \n",
        "    if not eval_df.empty:\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.subheader(\"üìä Latest Model Metrics\")\n",
        "            latest_eval = eval_df.iloc[-1]\n",
        "            st.metric(\"MAE\", f\"{latest_eval.get('MAE', 0):.3f}\")\n",
        "            st.metric(\"RMSE\", f\"{latest_eval.get('RMSE', 0):.3f}\")\n",
        "            st.metric(\"R¬≤\", f\"{latest_eval.get('R2_SCORE', 0):.3f}\")\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"üîÑ Cross-Validation Results\")\n",
        "            if 'CV_SCORE_MEAN' in eval_df.columns:\n",
        "                fig_cv = px.line(\n",
        "                    eval_df, \n",
        "                    y='CV_SCORE_MEAN',\n",
        "                    title=\"Cross-Validation Score Over Time\"\n",
        "                )\n",
        "                st.plotly_chart(fig_cv, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No model evaluation data available yet.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    st.warning(f\"Model evaluation data not available: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"**üè• Healthcare ML Platform** | Powered by Snowflake ML & Streamlit\")\n",
        "'''\n",
        "\n",
        "# Write the Streamlit app to a file\n",
        "with open('healthcare_dashboard.py', 'w') as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"‚úÖ Streamlit app created: healthcare_dashboard.py\")\n",
        "print(\"üìù To run the dashboard:\")\n",
        "print(\"   streamlit run healthcare_dashboard.py\")\n",
        "print(\"\")\n",
        "print(\"üéØ Dashboard Features:\")\n",
        "print(\"   ‚Ä¢ Real-time patient risk assessment\")\n",
        "print(\"   ‚Ä¢ Interactive risk gauge and visualizations\")\n",
        "print(\"   ‚Ä¢ Clinical recommendations based on risk score\")\n",
        "print(\"   ‚Ä¢ Analytics dashboard with inference logs\")\n",
        "print(\"   ‚Ä¢ Model performance monitoring\")\n",
        "print(\"   ‚Ä¢ Responsive design with sidebar controls\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Setting up batch inference pipeline...\n",
            "üìù Creating sample patient data for batch inference...\n",
            "‚úÖ Sample patient data created (100 patients)\n",
            "üîÑ Running batch inference on ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS...\n",
            "‚úÖ Batch inference completed:\n",
            "   üìä Processed: 100 patients\n",
            "   ‚ö° Processing time: 2515.64ms\n",
            "   üöÄ Throughput: 39.8 patients/sec\n",
            "   üìà Risk distribution: 81 HIGH | 19 MEDIUM | 0 LOW\n",
            "‚úÖ Batch inference pipeline is operational\n"
          ]
        }
      ],
      "source": [
        "# Batch Inference Pipeline\n",
        "print(\"üìä Setting up batch inference pipeline...\")\n",
        "\n",
        "def run_batch_inference(table_name: str, batch_size: int = 1000) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run batch inference on a table of patients\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Running batch inference on {table_name}...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Get patient data for batch processing\n",
        "        patient_data = session.sql(f\"\"\"\n",
        "            SELECT \n",
        "                PATIENT_ID,\n",
        "                AGE,\n",
        "                NUM_CONDITIONS,\n",
        "                NUM_MEDICATIONS,\n",
        "                NUM_CLAIMS\n",
        "            FROM {table_name}\n",
        "            LIMIT {batch_size}\n",
        "        \"\"\").collect()\n",
        "        \n",
        "        if not patient_data:\n",
        "            return {\"success\": False, \"error\": \"No patient data found\"}\n",
        "        \n",
        "        # Run batch inference using UDF\n",
        "        batch_sql = f\"\"\"\n",
        "            SELECT \n",
        "                PATIENT_ID,\n",
        "                AGE,\n",
        "                NUM_CONDITIONS,\n",
        "                NUM_MEDICATIONS,\n",
        "                NUM_CLAIMS,\n",
        "                healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) as RISK_SCORE,\n",
        "                CASE \n",
        "                    WHEN healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) < 30 THEN 'LOW'\n",
        "                    WHEN healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) < 70 THEN 'MEDIUM'\n",
        "                    ELSE 'HIGH'\n",
        "                END as RISK_CATEGORY,\n",
        "                CURRENT_TIMESTAMP() as INFERENCE_TIMESTAMP\n",
        "            FROM {table_name}\n",
        "            LIMIT {batch_size}\n",
        "        \"\"\"\n",
        "        \n",
        "        results_df = session.sql(batch_sql)\n",
        "        \n",
        "        # Save batch results\n",
        "        results_df.write.mode(\"overwrite\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BATCH_INFERENCE_RESULTS\")\n",
        "        \n",
        "        # Get summary statistics\n",
        "        summary = session.sql(\"\"\"\n",
        "            SELECT \n",
        "                COUNT(*) as TOTAL_PATIENTS,\n",
        "                AVG(RISK_SCORE) as AVG_RISK_SCORE,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'HIGH' THEN 1 END) as HIGH_RISK_COUNT,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'MEDIUM' THEN 1 END) as MEDIUM_RISK_COUNT,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'LOW' THEN 1 END) as LOW_RISK_COUNT\n",
        "            FROM ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BATCH_INFERENCE_RESULTS\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        processing_time = (time.time() - start_time) * 1000\n",
        "        \n",
        "        result = {\n",
        "            \"success\": True,\n",
        "            \"total_patients\": summary['TOTAL_PATIENTS'],\n",
        "            \"avg_risk_score\": float(summary['AVG_RISK_SCORE']),\n",
        "            \"high_risk_count\": summary['HIGH_RISK_COUNT'],\n",
        "            \"medium_risk_count\": summary['MEDIUM_RISK_COUNT'],\n",
        "            \"low_risk_count\": summary['LOW_RISK_COUNT'],\n",
        "            \"processing_time_ms\": processing_time,\n",
        "            \"throughput_patients_per_sec\": summary['TOTAL_PATIENTS'] / (processing_time / 1000)\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Batch inference completed:\")\n",
        "        print(f\"   üìä Processed: {result['total_patients']} patients\")\n",
        "        print(f\"   ‚ö° Processing time: {result['processing_time_ms']:.2f}ms\")\n",
        "        print(f\"   üöÄ Throughput: {result['throughput_patients_per_sec']:.1f} patients/sec\")\n",
        "        print(f\"   üìà Risk distribution: {result['high_risk_count']} HIGH | {result['medium_risk_count']} MEDIUM | {result['low_risk_count']} LOW\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"processing_time_ms\": (time.time() - start_time) * 1000\n",
        "        }\n",
        "\n",
        "# Create sample patient data for batch testing\n",
        "print(\"üìù Creating sample patient data for batch inference...\")\n",
        "\n",
        "try:\n",
        "    # Create sample data\n",
        "    sample_data_sql = \"\"\"\n",
        "        CREATE OR REPLACE TABLE ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS AS\n",
        "        SELECT \n",
        "            'PAT_' || ROW_NUMBER() OVER (ORDER BY UNIFORM(1, 1000, RANDOM())) as PATIENT_ID,\n",
        "            UNIFORM(25, 85, RANDOM()) as AGE,\n",
        "            UNIFORM(1, 15, RANDOM()) as NUM_CONDITIONS,\n",
        "            UNIFORM(1, 20, RANDOM()) as NUM_MEDICATIONS,\n",
        "            UNIFORM(5, 50, RANDOM()) as NUM_CLAIMS\n",
        "        FROM TABLE(GENERATOR(ROWCOUNT => 100))\n",
        "    \"\"\"\n",
        "    \n",
        "    session.sql(sample_data_sql).collect()\n",
        "    print(\"‚úÖ Sample patient data created (100 patients)\")\n",
        "    \n",
        "    # Run batch inference on sample data\n",
        "    batch_results = run_batch_inference(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS\", 100)\n",
        "    \n",
        "    if batch_results[\"success\"]:\n",
        "        print(\"‚úÖ Batch inference pipeline is operational\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Batch inference failed: {batch_results.get('error')}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Sample data creation failed: {e}\")\n",
        "    print(\"üí° Batch inference will be available once patient data exists\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
