{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîÆ Production ML Inference Pipeline\n",
        "\n",
        "**Production-ready inference with UDFs, Streamlit applications, and real-time scoring capabilities**\n",
        "\n",
        "## üéØ **Inference Objectives:**\n",
        "1. **‚ö° Real-time Inference** - Individual patient risk scoring via UDFs\n",
        "2. **üìä Batch Inference** - Large-scale patient cohort processing\n",
        "3. **üñ•Ô∏è Streamlit Application** - Interactive healthcare dashboard\n",
        "4. **üîÑ Streaming Inference** - Real-time data pipeline integration\n",
        "5. **üìã API Integration** - REST endpoints for external system access\n",
        "\n",
        "## üõ†Ô∏è **Inference Components:**\n",
        "- **Production UDFs**: Scalable inference functions\n",
        "- **Streamlit Dashboard**: Interactive clinical decision support\n",
        "- **Batch Processing**: Automated large-scale scoring\n",
        "- **Real-time APIs**: External system integration\n",
        "- **Performance Monitoring**: Latency and throughput tracking\n",
        "\n",
        "**Prerequisites:** Run notebooks 05 (Training) and 06 (Evaluation) first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Added to Python path: /Users/beddy/Desktop/Github/Snowflake_ML_HCLS/notebooks/../src\n",
            "üîÑ Reusing existing Snowflake session\n",
            "‚úÖ Environment ready for production inference\n",
            "üîÆ Capabilities: Real-time UDFs, Batch Processing, Streamlit Integration\n",
            "‚ö° Tools: Production inference, monitoring, external API integration\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup for Production Inference\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "\n",
        "# Fix path for snowflake_connection module\n",
        "current_dir = os.getcwd()\n",
        "if \"notebooks\" in current_dir:\n",
        "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
        "else:\n",
        "    src_path = os.path.join(current_dir, \"src\")\n",
        "\n",
        "sys.path.append(src_path)\n",
        "print(f\"üìÅ Added to Python path: {src_path}\")\n",
        "\n",
        "from snowflake_connection import get_session\n",
        "from snowflake.snowpark.functions import (\n",
        "    col, lit, when, count, avg, sum as sum_, max as max_, min as min_,\n",
        "    current_timestamp, call_udf, sql_expr, udf\n",
        ")\n",
        "from snowflake.snowpark.types import (\n",
        "    StructType, StructField, StringType, DoubleType, IntegerType,\n",
        "    FloatType, BooleanType, TimestampType\n",
        ")\n",
        "\n",
        "# Get Snowflake session\n",
        "session = get_session()\n",
        "print(\"‚úÖ Environment ready for production inference\")\n",
        "print(\"üîÆ Capabilities: Real-time UDFs, Batch Processing, Streamlit Integration\")\n",
        "print(\"‚ö° Tools: Production inference, monitoring, external API integration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up ML infrastructure...\n",
            "‚ö†Ô∏è UDF creation is REQUIRED - will not proceed without it\n",
            "üìÅ Creating ML models stage...\n",
            "‚úÖ ML models stage created with full schema qualification\n",
            "üîß Creating healthcare risk scoring UDF...\n",
            "‚úÖ UDF created WITH stage location: @ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\n",
            "üß™ Testing UDF creation...\n",
            "‚úÖ UDF test SUCCESSFUL - Test risk score: 80.25\n",
            "üéØ UDF is working and ready for production inference!\n",
            "‚úÖ ALL CHECKS PASSED - UDF is fully operational\n"
          ]
        }
      ],
      "source": [
        "# Stage and UDF Setup for Inference - MANDATORY UDF CREATION\n",
        "print(\"üîß Setting up ML infrastructure...\")\n",
        "print(\"‚ö†Ô∏è UDF creation is REQUIRED - will not proceed without it\")\n",
        "\n",
        "# Step 1: Create stage using multiple approaches until one works\n",
        "print(\"üìÅ Creating ML models stage...\")\n",
        "stage_created = False\n",
        "stage_name = None\n",
        "\n",
        "# Approach 1: Try with full schema qualification\n",
        "try:\n",
        "    session.sql(\"\"\"\n",
        "        CREATE STAGE IF NOT EXISTS ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\n",
        "        COMMENT = 'Stage for storing ML model artifacts and UDF dependencies'\n",
        "    \"\"\").collect()\n",
        "    stage_name = \"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.ML_MODELS_STAGE\"\n",
        "    stage_created = True\n",
        "    print(\"‚úÖ ML models stage created with full schema qualification\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Full schema stage creation failed: {e}\")\n",
        "\n",
        "# Approach 2: Try with current schema only\n",
        "if not stage_created:\n",
        "    try:\n",
        "        session.sql(\"CREATE STAGE IF NOT EXISTS ML_MODELS_STAGE\").collect()\n",
        "        stage_name = \"ML_MODELS_STAGE\"\n",
        "        stage_created = True\n",
        "        print(\"‚úÖ ML models stage created in current schema\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Current schema stage creation failed: {e}\")\n",
        "\n",
        "# Approach 3: Try with default temp stage\n",
        "if not stage_created:\n",
        "    try:\n",
        "        session.sql(\"CREATE STAGE IF NOT EXISTS TEMP_ML_STAGE\").collect()\n",
        "        stage_name = \"TEMP_ML_STAGE\"\n",
        "        stage_created = True\n",
        "        print(\"‚úÖ Temporary ML stage created\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Temp stage creation failed: {e}\")\n",
        "\n",
        "if not stage_created:\n",
        "    print(\"‚ùå CRITICAL: Could not create any stage\")\n",
        "    print(\"üí° Trying UDF creation without stage location...\")\n",
        "    stage_name = None\n",
        "\n",
        "# Step 2: Create UDF with stage if available, without stage if necessary\n",
        "print(\"üîß Creating healthcare risk scoring UDF...\")\n",
        "\n",
        "# Try with stage first if available\n",
        "udf_created = False\n",
        "if stage_name:\n",
        "    try:\n",
        "        @udf(name=\"healthcare_risk_score_udf\", \n",
        "             input_types=[FloatType(), IntegerType(), IntegerType(), IntegerType()],\n",
        "             return_type=FloatType(),\n",
        "             replace=True,\n",
        "             stage_location=f\"@{stage_name}\")\n",
        "        def healthcare_risk_score_with_stage(age: float, conditions: int, medications: int, claims: int) -> float:\n",
        "            \"\"\"Healthcare risk scoring UDF with stage location\"\"\"\n",
        "            base_risk = (age / 100.0) * 25\n",
        "            condition_risk = conditions * 6\n",
        "            medication_risk = medications * 3\n",
        "            utilization_risk = (claims / 10.0) * 4\n",
        "            total_risk = base_risk + condition_risk + medication_risk + utilization_risk\n",
        "            return min(100.0, max(0.0, total_risk))\n",
        "        \n",
        "        udf_created = True\n",
        "        print(f\"‚úÖ UDF created WITH stage location: @{stage_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è UDF creation with stage failed: {e}\")\n",
        "        print(\"üîÑ Trying without stage location...\")\n",
        "\n",
        "# Try without stage if stage approach failed\n",
        "if not udf_created:\n",
        "    try:\n",
        "        @udf(name=\"healthcare_risk_score_udf\", \n",
        "             input_types=[FloatType(), IntegerType(), IntegerType(), IntegerType()],\n",
        "             return_type=FloatType(),\n",
        "             replace=True)\n",
        "        def healthcare_risk_score_no_stage(age: float, conditions: int, medications: int, claims: int) -> float:\n",
        "            \"\"\"Healthcare risk scoring UDF without stage location\"\"\"\n",
        "            base_risk = (age / 100.0) * 25\n",
        "            condition_risk = conditions * 6\n",
        "            medication_risk = medications * 3\n",
        "            utilization_risk = (claims / 10.0) * 4\n",
        "            total_risk = base_risk + condition_risk + medication_risk + utilization_risk\n",
        "            return min(100.0, max(0.0, total_risk))\n",
        "        \n",
        "        udf_created = True\n",
        "        print(\"‚úÖ UDF created WITHOUT stage location\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL ERROR: UDF creation failed completely: {e}\")\n",
        "        print(\"üö® Cannot proceed without UDF - check permissions and database setup\")\n",
        "        raise Exception(f\"UDF creation is mandatory but failed: {e}\")\n",
        "\n",
        "# Step 3: Test the UDF - MANDATORY\n",
        "print(\"üß™ Testing UDF creation...\")\n",
        "try:\n",
        "    test_result = session.sql(\"\"\"\n",
        "        SELECT healthcare_risk_score_udf(65.0, 5, 8, 25) as RISK_SCORE\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    risk_score = test_result[0]['RISK_SCORE']\n",
        "    print(f\"‚úÖ UDF test SUCCESSFUL - Test risk score: {risk_score:.2f}\")\n",
        "    print(\"üéØ UDF is working and ready for production inference!\")\n",
        "    udf_available = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL ERROR: UDF test failed: {e}\")\n",
        "    print(\"üö® UDF exists but is not functional\")\n",
        "    raise Exception(f\"UDF test is mandatory but failed: {e}\")\n",
        "\n",
        "print(\"‚úÖ ALL CHECKS PASSED - UDF is fully operational\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö° Setting up real-time inference pipeline...\n",
            "‚úÖ UDF is confirmed operational - proceeding with UDF-only inference\n",
            "üß™ Testing UDF-based inference pipeline...\n",
            "   Patient TEST_001: 80.2 (HIGH) - 1385.1ms [UDF]\n",
            "   Patient TEST_002: 25.8 (LOW) - 1152.04ms [UDF]\n",
            "   Patient TEST_003: 100.0 (HIGH) - 1081.1ms [UDF]\n",
            "‚úÖ UDF-based inference pipeline is operational\n"
          ]
        }
      ],
      "source": [
        "# Real-time Inference Pipeline Setup - UDF REQUIRED\n",
        "print(\"‚ö° Setting up real-time inference pipeline...\")\n",
        "print(\"‚úÖ UDF is confirmed operational - proceeding with UDF-only inference\")\n",
        "\n",
        "# Create real-time inference wrapper function\n",
        "def predict_patient_risk(patient_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Real-time patient risk prediction using UDF (REQUIRED)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Extract patient features\n",
        "        age = float(patient_data.get('age', 0))\n",
        "        conditions = int(patient_data.get('num_conditions', 0))\n",
        "        medications = int(patient_data.get('num_medications', 0))\n",
        "        claims = int(patient_data.get('num_claims', 0))\n",
        "        \n",
        "        # Make prediction using UDF (MANDATORY - no fallback)\n",
        "        prediction_sql = f\"\"\"\n",
        "            SELECT \n",
        "                healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) as RISK_SCORE,\n",
        "                CASE \n",
        "                    WHEN healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) < 30 THEN 'LOW'\n",
        "                    WHEN healthcare_risk_score_udf({age}, {conditions}, {medications}, {claims}) < 70 THEN 'MEDIUM'\n",
        "                    ELSE 'HIGH'\n",
        "                END as RISK_CATEGORY\n",
        "        \"\"\"\n",
        "        \n",
        "        result = session.sql(prediction_sql).collect()[0]\n",
        "        risk_score = float(result['RISK_SCORE'])\n",
        "        risk_category = result['RISK_CATEGORY']\n",
        "        \n",
        "        # Calculate prediction metadata\n",
        "        prediction_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "        \n",
        "        # Prepare comprehensive response\n",
        "        response = {\n",
        "            'patient_id': patient_data.get('patient_id', 'UNKNOWN'),\n",
        "            'risk_score': risk_score,\n",
        "            'risk_category': risk_category,\n",
        "            'prediction_timestamp': datetime.datetime.now().isoformat(),\n",
        "            'prediction_time_ms': round(prediction_time, 2),\n",
        "            'model_version': 'v1.0.0',\n",
        "            'confidence': 0.85,  # Simulated confidence score\n",
        "            'input_features': {\n",
        "                'age': age,\n",
        "                'num_conditions': conditions,\n",
        "                'num_medications': medications,\n",
        "                'num_claims': claims\n",
        "            },\n",
        "            'clinical_recommendations': generate_clinical_recommendations(risk_score, risk_category),\n",
        "            'success': True,\n",
        "            'inference_method': 'UDF'  # Always UDF\n",
        "        }\n",
        "        \n",
        "        # Log inference request\n",
        "        log_inference_request(response)\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        # If UDF fails, this is a critical error since UDF is mandatory\n",
        "        print(f\"‚ùå CRITICAL: UDF inference failed: {e}\")\n",
        "        error_response = {\n",
        "            'patient_id': patient_data.get('patient_id', 'UNKNOWN'),\n",
        "            'error': f\"UDF_FAILURE: {str(e)}\",\n",
        "            'prediction_timestamp': datetime.datetime.now().isoformat(),\n",
        "            'prediction_time_ms': (time.time() - start_time) * 1000,\n",
        "            'success': False\n",
        "        }\n",
        "        \n",
        "        return error_response\n",
        "\n",
        "def generate_clinical_recommendations(risk_score: float, risk_category: str) -> List[str]:\n",
        "    \"\"\"Generate clinical recommendations based on risk score\"\"\"\n",
        "    \n",
        "    recommendations = []\n",
        "    \n",
        "    if risk_category == 'HIGH':\n",
        "        recommendations.extend([\n",
        "            \"üö® High risk patient - Consider immediate clinical review\",\n",
        "            \"üìã Review medication interactions and dosages\", \n",
        "            \"ü©∫ Schedule follow-up within 2 weeks\",\n",
        "            \"üìä Monitor vital signs and laboratory values closely\"\n",
        "        ])\n",
        "    elif risk_category == 'MEDIUM':\n",
        "        recommendations.extend([\n",
        "            \"‚ö†Ô∏è Moderate risk - Schedule routine follow-up\",\n",
        "            \"üíä Review medication adherence\",\n",
        "            \"üìÖ Consider preventive care measures\",\n",
        "            \"üìà Monitor for symptom progression\"\n",
        "        ])\n",
        "    else:  # LOW\n",
        "        recommendations.extend([\n",
        "            \"‚úÖ Low risk - Continue routine care\",\n",
        "            \"üèÉ Encourage healthy lifestyle maintenance\",\n",
        "            \"üìÖ Schedule annual wellness check\",\n",
        "            \"üìö Provide patient education resources\"\n",
        "        ])\n",
        "    \n",
        "    # Add specific recommendations based on risk score\n",
        "    if risk_score > 80:\n",
        "        recommendations.append(\"üè• Consider hospitalization or intensive monitoring\")\n",
        "    elif risk_score > 60:\n",
        "        recommendations.append(\"üîÑ Increase monitoring frequency\")\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "def log_inference_request(response: Dict[str, Any]):\n",
        "    \"\"\"Log inference request for monitoring and analysis\"\"\"\n",
        "    \n",
        "    try:\n",
        "        log_data = [(\n",
        "            f\"REQ_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{response['patient_id']}\",\n",
        "            'healthcare_risk_model',\n",
        "            datetime.datetime.now().isoformat(),\n",
        "            response['prediction_time_ms'],\n",
        "            json.dumps(response['input_features']),\n",
        "            response.get('risk_score', 0.0),\n",
        "            'INFERENCE_PIPELINE',\n",
        "            response['success']\n",
        "        )]\n",
        "        \n",
        "        log_schema = StructType([\n",
        "            StructField(\"REQUEST_ID\", StringType()),\n",
        "            StructField(\"MODEL_NAME\", StringType()),\n",
        "            StructField(\"REQUEST_TIMESTAMP\", StringType()),\n",
        "            StructField(\"RESPONSE_TIME_MS\", DoubleType()),\n",
        "            StructField(\"INPUT_FEATURES\", StringType()),\n",
        "            StructField(\"PREDICTION_RESULT\", DoubleType()),\n",
        "            StructField(\"REQUEST_SOURCE\", StringType()),\n",
        "            StructField(\"SUCCESS_STATUS\", BooleanType())\n",
        "        ])\n",
        "        \n",
        "        log_df = session.create_dataframe(log_data, schema=log_schema)\n",
        "        log_df.write.mode(\"append\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.INFERENCE_REQUEST_LOG\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Logging error: {e}\")\n",
        "\n",
        "# Test real-time inference with UDF\n",
        "print(\"üß™ Testing UDF-based inference pipeline...\")\n",
        "\n",
        "test_patients = [\n",
        "    {'patient_id': 'TEST_001', 'age': 65, 'num_conditions': 5, 'num_medications': 8, 'num_claims': 25},\n",
        "    {'patient_id': 'TEST_002', 'age': 35, 'num_conditions': 2, 'num_medications': 1, 'num_claims': 5},\n",
        "    {'patient_id': 'TEST_003', 'age': 78, 'num_conditions': 12, 'num_medications': 15, 'num_claims': 45}\n",
        "]\n",
        "\n",
        "for patient in test_patients:\n",
        "    result = predict_patient_risk(patient)\n",
        "    if result['success']:\n",
        "        print(f\"   Patient {result['patient_id']}: {result['risk_score']:.1f} ({result['risk_category']}) - {result['prediction_time_ms']}ms [UDF]\")\n",
        "    else:\n",
        "        print(f\"   Patient {patient['patient_id']}: ‚ùå {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "print(\"‚úÖ UDF-based inference pipeline is operational\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è Creating Streamlit healthcare dashboard...\n",
            "‚úÖ Streamlit app created: healthcare_dashboard.py\n",
            "üìù To run the dashboard:\n",
            "   streamlit run healthcare_dashboard.py\n",
            "\n",
            "üéØ Dashboard Features:\n",
            "   ‚Ä¢ Real-time patient risk assessment\n",
            "   ‚Ä¢ Interactive risk gauge and visualizations\n",
            "   ‚Ä¢ Clinical recommendations based on risk score\n",
            "   ‚Ä¢ Analytics dashboard with inference logs\n",
            "   ‚Ä¢ Model performance monitoring\n",
            "   ‚Ä¢ Responsive design with sidebar controls\n"
          ]
        }
      ],
      "source": [
        "# Streamlit Healthcare Dashboard Application\n",
        "print(\"üñ•Ô∏è Creating Streamlit healthcare dashboard...\")\n",
        "\n",
        "# Note: Streamlit imports are in the external file, not in this notebook\n",
        "\n",
        "# Create the Streamlit application code\n",
        "streamlit_app_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Fix path for snowflake_connection module\n",
        "current_dir = os.getcwd()\n",
        "if \"notebooks\" in current_dir:\n",
        "    src_path = os.path.join(current_dir, \"..\", \"src\")\n",
        "else:\n",
        "    src_path = os.path.join(current_dir, \"src\")\n",
        "\n",
        "sys.path.append(src_path)\n",
        "\n",
        "from snowflake_connection import get_session\n",
        "from snowflake.snowpark.functions import col, lit, when, count, avg, sum as sum_, max as max_, min as min_\n",
        "\n",
        "# Initialize Snowflake session\n",
        "@st.cache_resource\n",
        "def get_snowflake_session():\n",
        "    return get_session()\n",
        "\n",
        "session = get_snowflake_session()\n",
        "\n",
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"üè• Healthcare Risk Assessment Dashboard\",\n",
        "    page_icon=\"üè•\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Dashboard title and header\n",
        "st.title(\"üè• Healthcare Risk Assessment Dashboard\")\n",
        "st.markdown(\"### Real-time Patient Risk Scoring & Clinical Decision Support\")\n",
        "\n",
        "# Sidebar for patient input\n",
        "st.sidebar.header(\"ü©∫ Patient Risk Assessment\")\n",
        "\n",
        "# Patient input form\n",
        "with st.sidebar.form(\"patient_form\"):\n",
        "    st.subheader(\"Enter Patient Information\")\n",
        "    \n",
        "    patient_id = st.text_input(\"Patient ID\", value=\"PAT_001\")\n",
        "    age = st.slider(\"Age\", 0, 120, 65)\n",
        "    num_conditions = st.slider(\"Number of Conditions\", 0, 20, 5)\n",
        "    num_medications = st.slider(\"Number of Medications\", 0, 30, 8)\n",
        "    num_claims = st.slider(\"Number of Claims (last year)\", 0, 100, 25)\n",
        "    \n",
        "    submitted = st.form_submit_button(\"üîç Calculate Risk Score\")\n",
        "\n",
        "# Main dashboard content\n",
        "if submitted:\n",
        "    # Calculate risk using UDF\n",
        "    with st.spinner(\"Calculating risk score...\"):\n",
        "        try:\n",
        "            # Call the healthcare risk UDF\n",
        "            prediction_sql = f\"\"\"\n",
        "                SELECT \n",
        "                    healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) as RISK_SCORE,\n",
        "                    CASE \n",
        "                        WHEN healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) < 30 THEN 'LOW'\n",
        "                        WHEN healthcare_risk_score_udf({age}, {num_conditions}, {num_medications}, {num_claims}) < 70 THEN 'MEDIUM'\n",
        "                        ELSE 'HIGH'\n",
        "                    END as RISK_CATEGORY\n",
        "            \"\"\"\n",
        "            \n",
        "            result = session.sql(prediction_sql).collect()[0]\n",
        "            risk_score = float(result['RISK_SCORE'])\n",
        "            risk_category = result['RISK_CATEGORY']\n",
        "            \n",
        "            # Display results\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            \n",
        "            with col1:\n",
        "                if risk_category == 'HIGH':\n",
        "                    st.error(f\"üö® **HIGH RISK**\")\n",
        "                elif risk_category == 'MEDIUM':\n",
        "                    st.warning(f\"‚ö†Ô∏è **MEDIUM RISK**\")\n",
        "                else:\n",
        "                    st.success(f\"‚úÖ **LOW RISK**\")\n",
        "                \n",
        "                st.metric(\"Risk Score\", f\"{risk_score:.1f}\")\n",
        "            \n",
        "            with col2:\n",
        "                st.metric(\"Patient ID\", patient_id)\n",
        "                st.metric(\"Age\", f\"{age} years\")\n",
        "            \n",
        "            with col3:\n",
        "                st.metric(\"Conditions\", num_conditions)\n",
        "                st.metric(\"Medications\", num_medications)\n",
        "                st.metric(\"Claims\", num_claims)\n",
        "            \n",
        "            # Risk gauge chart\n",
        "            fig_gauge = go.Figure(go.Indicator(\n",
        "                mode = \"gauge+number+delta\",\n",
        "                value = risk_score,\n",
        "                domain = {'x': [0, 1], 'y': [0, 1]},\n",
        "                title = {'text': \"Healthcare Risk Score\"},\n",
        "                delta = {'reference': 50},\n",
        "                gauge = {\n",
        "                    'axis': {'range': [None, 100]},\n",
        "                    'bar': {'color': \"darkblue\"},\n",
        "                    'steps': [\n",
        "                        {'range': [0, 30], 'color': \"lightgreen\"},\n",
        "                        {'range': [30, 70], 'color': \"yellow\"},\n",
        "                        {'range': [70, 100], 'color': \"red\"}\n",
        "                    ],\n",
        "                    'threshold': {\n",
        "                        'line': {'color': \"red\", 'width': 4},\n",
        "                        'thickness': 0.75,\n",
        "                        'value': 90\n",
        "                    }\n",
        "                }\n",
        "            ))\n",
        "            \n",
        "            fig_gauge.update_layout(height=400)\n",
        "            st.plotly_chart(fig_gauge, use_container_width=True)\n",
        "            \n",
        "            # Clinical recommendations\n",
        "            st.subheader(\"üìã Clinical Recommendations\")\n",
        "            \n",
        "            if risk_category == 'HIGH':\n",
        "                recommendations = [\n",
        "                    \"üö® High risk patient - Consider immediate clinical review\",\n",
        "                    \"üìã Review medication interactions and dosages\",\n",
        "                    \"ü©∫ Schedule follow-up within 2 weeks\",\n",
        "                    \"üìä Monitor vital signs and laboratory values closely\"\n",
        "                ]\n",
        "                if risk_score > 80:\n",
        "                    recommendations.append(\"üè• Consider hospitalization or intensive monitoring\")\n",
        "            elif risk_category == 'MEDIUM':\n",
        "                recommendations = [\n",
        "                    \"‚ö†Ô∏è Moderate risk - Schedule routine follow-up\",\n",
        "                    \"üíä Review medication adherence\",\n",
        "                    \"üìÖ Consider preventive care measures\",\n",
        "                    \"üìà Monitor for symptom progression\"\n",
        "                ]\n",
        "                if risk_score > 60:\n",
        "                    recommendations.append(\"üîÑ Increase monitoring frequency\")\n",
        "            else:\n",
        "                recommendations = [\n",
        "                    \"‚úÖ Low risk - Continue routine care\",\n",
        "                    \"üèÉ Encourage healthy lifestyle maintenance\",\n",
        "                    \"üìÖ Schedule annual wellness check\",\n",
        "                    \"üìö Provide patient education resources\"\n",
        "                ]\n",
        "            \n",
        "            for rec in recommendations:\n",
        "                st.write(f\"‚Ä¢ {rec}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Error calculating risk: {e}\")\n",
        "\n",
        "# Analytics section\n",
        "st.header(\"üìä Analytics Dashboard\")\n",
        "\n",
        "# Fetch inference logs\n",
        "try:\n",
        "    logs_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.INFERENCE_REQUEST_LOG\").to_pandas()\n",
        "    \n",
        "    if not logs_df.empty:\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.subheader(\"üìà Recent Inference Requests\")\n",
        "            st.dataframe(logs_df.tail(10))\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"‚ö° Response Time Distribution\")\n",
        "            if 'RESPONSE_TIME_MS' in logs_df.columns:\n",
        "                fig_hist = px.histogram(\n",
        "                    logs_df, \n",
        "                    x='RESPONSE_TIME_MS',\n",
        "                    title=\"Response Time Distribution (ms)\",\n",
        "                    nbins=20\n",
        "                )\n",
        "                st.plotly_chart(fig_hist, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No inference requests logged yet. Submit a patient assessment to see analytics.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    st.warning(f\"Analytics data not available: {e}\")\n",
        "\n",
        "# Model evaluation results\n",
        "st.header(\"üéØ Model Performance\")\n",
        "\n",
        "try:\n",
        "    eval_df = session.table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_EVALUATION_LOG\").to_pandas()\n",
        "    \n",
        "    if not eval_df.empty:\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.subheader(\"üìä Latest Model Metrics\")\n",
        "            latest_eval = eval_df.iloc[-1]\n",
        "            st.metric(\"MAE\", f\"{latest_eval.get('MAE', 0):.3f}\")\n",
        "            st.metric(\"RMSE\", f\"{latest_eval.get('RMSE', 0):.3f}\")\n",
        "            st.metric(\"R¬≤\", f\"{latest_eval.get('R2_SCORE', 0):.3f}\")\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"üîÑ Cross-Validation Results\")\n",
        "            if 'CV_SCORE_MEAN' in eval_df.columns:\n",
        "                fig_cv = px.line(\n",
        "                    eval_df, \n",
        "                    y='CV_SCORE_MEAN',\n",
        "                    title=\"Cross-Validation Score Over Time\"\n",
        "                )\n",
        "                st.plotly_chart(fig_cv, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No model evaluation data available yet.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    st.warning(f\"Model evaluation data not available: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"**üè• Healthcare ML Platform** | Powered by Snowflake ML & Streamlit\")\n",
        "'''\n",
        "\n",
        "# Write the Streamlit app to a file\n",
        "with open('healthcare_dashboard.py', 'w') as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"‚úÖ Streamlit app created: healthcare_dashboard.py\")\n",
        "print(\"üìù To run the dashboard:\")\n",
        "print(\"   streamlit run healthcare_dashboard.py\")\n",
        "print(\"\")\n",
        "print(\"üéØ Dashboard Features:\")\n",
        "print(\"   ‚Ä¢ Real-time patient risk assessment\")\n",
        "print(\"   ‚Ä¢ Interactive risk gauge and visualizations\")\n",
        "print(\"   ‚Ä¢ Clinical recommendations based on risk score\")\n",
        "print(\"   ‚Ä¢ Analytics dashboard with inference logs\")\n",
        "print(\"   ‚Ä¢ Model performance monitoring\")\n",
        "print(\"   ‚Ä¢ Responsive design with sidebar controls\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Setting up batch inference pipeline...\n",
            "üìù Creating sample patient data for batch inference...\n",
            "‚úÖ Sample patient data created (100 patients)\n",
            "üîÑ Running batch inference on ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS...\n",
            "‚úÖ Batch inference completed:\n",
            "   üìä Processed: 100 patients\n",
            "   ‚ö° Processing time: 2515.64ms\n",
            "   üöÄ Throughput: 39.8 patients/sec\n",
            "   üìà Risk distribution: 81 HIGH | 19 MEDIUM | 0 LOW\n",
            "‚úÖ Batch inference pipeline is operational\n"
          ]
        }
      ],
      "source": [
        "# Batch Inference Pipeline\n",
        "print(\"üìä Setting up batch inference pipeline...\")\n",
        "\n",
        "def run_batch_inference(table_name: str, batch_size: int = 1000) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run batch inference on a table of patients\n",
        "    \"\"\"\n",
        "    print(f\"üîÑ Running batch inference on {table_name}...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Get patient data for batch processing\n",
        "        patient_data = session.sql(f\"\"\"\n",
        "            SELECT \n",
        "                PATIENT_ID,\n",
        "                AGE,\n",
        "                NUM_CONDITIONS,\n",
        "                NUM_MEDICATIONS,\n",
        "                NUM_CLAIMS\n",
        "            FROM {table_name}\n",
        "            LIMIT {batch_size}\n",
        "        \"\"\").collect()\n",
        "        \n",
        "        if not patient_data:\n",
        "            return {\"success\": False, \"error\": \"No patient data found\"}\n",
        "        \n",
        "        # Run batch inference using UDF\n",
        "        batch_sql = f\"\"\"\n",
        "            SELECT \n",
        "                PATIENT_ID,\n",
        "                AGE,\n",
        "                NUM_CONDITIONS,\n",
        "                NUM_MEDICATIONS,\n",
        "                NUM_CLAIMS,\n",
        "                healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) as RISK_SCORE,\n",
        "                CASE \n",
        "                    WHEN healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) < 30 THEN 'LOW'\n",
        "                    WHEN healthcare_risk_score_udf(AGE, NUM_CONDITIONS, NUM_MEDICATIONS, NUM_CLAIMS) < 70 THEN 'MEDIUM'\n",
        "                    ELSE 'HIGH'\n",
        "                END as RISK_CATEGORY,\n",
        "                CURRENT_TIMESTAMP() as INFERENCE_TIMESTAMP\n",
        "            FROM {table_name}\n",
        "            LIMIT {batch_size}\n",
        "        \"\"\"\n",
        "        \n",
        "        results_df = session.sql(batch_sql)\n",
        "        \n",
        "        # Save batch results\n",
        "        results_df.write.mode(\"overwrite\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BATCH_INFERENCE_RESULTS\")\n",
        "        \n",
        "        # Get summary statistics\n",
        "        summary = session.sql(\"\"\"\n",
        "            SELECT \n",
        "                COUNT(*) as TOTAL_PATIENTS,\n",
        "                AVG(RISK_SCORE) as AVG_RISK_SCORE,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'HIGH' THEN 1 END) as HIGH_RISK_COUNT,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'MEDIUM' THEN 1 END) as MEDIUM_RISK_COUNT,\n",
        "                COUNT(CASE WHEN RISK_CATEGORY = 'LOW' THEN 1 END) as LOW_RISK_COUNT\n",
        "            FROM ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BATCH_INFERENCE_RESULTS\n",
        "        \"\"\").collect()[0]\n",
        "        \n",
        "        processing_time = (time.time() - start_time) * 1000\n",
        "        \n",
        "        result = {\n",
        "            \"success\": True,\n",
        "            \"total_patients\": summary['TOTAL_PATIENTS'],\n",
        "            \"avg_risk_score\": float(summary['AVG_RISK_SCORE']),\n",
        "            \"high_risk_count\": summary['HIGH_RISK_COUNT'],\n",
        "            \"medium_risk_count\": summary['MEDIUM_RISK_COUNT'],\n",
        "            \"low_risk_count\": summary['LOW_RISK_COUNT'],\n",
        "            \"processing_time_ms\": processing_time,\n",
        "            \"throughput_patients_per_sec\": summary['TOTAL_PATIENTS'] / (processing_time / 1000)\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Batch inference completed:\")\n",
        "        print(f\"   üìä Processed: {result['total_patients']} patients\")\n",
        "        print(f\"   ‚ö° Processing time: {result['processing_time_ms']:.2f}ms\")\n",
        "        print(f\"   üöÄ Throughput: {result['throughput_patients_per_sec']:.1f} patients/sec\")\n",
        "        print(f\"   üìà Risk distribution: {result['high_risk_count']} HIGH | {result['medium_risk_count']} MEDIUM | {result['low_risk_count']} LOW\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"processing_time_ms\": (time.time() - start_time) * 1000\n",
        "        }\n",
        "\n",
        "# Create sample patient data for batch testing\n",
        "print(\"üìù Creating sample patient data for batch inference...\")\n",
        "\n",
        "try:\n",
        "    # Create sample data\n",
        "    sample_data_sql = \"\"\"\n",
        "        CREATE OR REPLACE TABLE ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS AS\n",
        "        SELECT \n",
        "            'PAT_' || ROW_NUMBER() OVER (ORDER BY UNIFORM(1, 1000, RANDOM())) as PATIENT_ID,\n",
        "            UNIFORM(25, 85, RANDOM()) as AGE,\n",
        "            UNIFORM(1, 15, RANDOM()) as NUM_CONDITIONS,\n",
        "            UNIFORM(1, 20, RANDOM()) as NUM_MEDICATIONS,\n",
        "            UNIFORM(5, 50, RANDOM()) as NUM_CLAIMS\n",
        "        FROM TABLE(GENERATOR(ROWCOUNT => 100))\n",
        "    \"\"\"\n",
        "    \n",
        "    session.sql(sample_data_sql).collect()\n",
        "    print(\"‚úÖ Sample patient data created (100 patients)\")\n",
        "    \n",
        "    # Run batch inference on sample data\n",
        "    batch_results = run_batch_inference(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.SAMPLE_PATIENTS\", 100)\n",
        "    \n",
        "    if batch_results[\"success\"]:\n",
        "        print(\"‚úÖ Batch inference pipeline is operational\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Batch inference failed: {batch_results.get('error')}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Sample data creation failed: {e}\")\n",
        "    print(\"üí° Batch inference will be available once patient data exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Bonferroni Correction Framework for Multiple Testing\n",
        "print(\"üìä Setting up Bonferroni correction framework for reducing false positives...\")\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class BonferroniResult:\n",
        "    \"\"\"Results from Bonferroni-corrected multiple testing\"\"\"\n",
        "    original_pvalues: List[float]\n",
        "    corrected_pvalues: List[float]\n",
        "    alpha_adjusted: float\n",
        "    significant_tests: List[bool]\n",
        "    num_tests: int\n",
        "    alpha_original: float\n",
        "    correction_method: str\n",
        "\n",
        "class BonferroniCorrection:\n",
        "    \"\"\"\n",
        "    Comprehensive Bonferroni correction framework for healthcare ML applications\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha: float = 0.05):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def apply_correction(self, p_values: List[float], method: str = \"bonferroni\") -> BonferroniResult:\n",
        "        \"\"\"\n",
        "        Apply Bonferroni or related corrections to multiple p-values\n",
        "        \n",
        "        Args:\n",
        "            p_values: List of p-values from multiple tests\n",
        "            method: 'bonferroni', 'holm', or 'hochberg'\n",
        "        \n",
        "        Returns:\n",
        "            BonferroniResult with corrected p-values and significance flags\n",
        "        \"\"\"\n",
        "        if not p_values:\n",
        "            raise ValueError(\"No p-values provided\")\n",
        "            \n",
        "        p_values = np.array(p_values)\n",
        "        n_tests = len(p_values)\n",
        "        \n",
        "        if method == \"bonferroni\":\n",
        "            # Classic Bonferroni: Œ±_adj = Œ± / n\n",
        "            alpha_adjusted = self.alpha / n_tests\n",
        "            corrected_pvalues = p_values * n_tests\n",
        "            corrected_pvalues = np.minimum(corrected_pvalues, 1.0)  # Cap at 1.0\n",
        "            \n",
        "        elif method == \"holm\":\n",
        "            # Holm-Bonferroni (step-down): More powerful than classic Bonferroni\n",
        "            sorted_indices = np.argsort(p_values)\n",
        "            sorted_pvalues = p_values[sorted_indices]\n",
        "            corrected_pvalues = np.zeros_like(p_values)\n",
        "            \n",
        "            for i, idx in enumerate(sorted_indices):\n",
        "                correction_factor = n_tests - i\n",
        "                corrected_pvalues[idx] = min(1.0, sorted_pvalues[i] * correction_factor)\n",
        "                \n",
        "            alpha_adjusted = self.alpha / n_tests  # Most conservative step\n",
        "            \n",
        "        elif method == \"hochberg\":\n",
        "            # Hochberg (step-up): Even more powerful\n",
        "            sorted_indices = np.argsort(p_values)[::-1]  # Descending order\n",
        "            sorted_pvalues = p_values[sorted_indices]\n",
        "            corrected_pvalues = np.zeros_like(p_values)\n",
        "            \n",
        "            for i, idx in enumerate(sorted_indices):\n",
        "                correction_factor = i + 1\n",
        "                corrected_pvalues[idx] = min(1.0, sorted_pvalues[i] * correction_factor)\n",
        "                \n",
        "            alpha_adjusted = self.alpha\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown correction method: {method}\")\n",
        "        \n",
        "        # Determine significance using corrected alpha\n",
        "        if method == \"bonferroni\":\n",
        "            significant_tests = corrected_pvalues <= self.alpha\n",
        "        else:\n",
        "            significant_tests = corrected_pvalues <= self.alpha\n",
        "            \n",
        "        return BonferroniResult(\n",
        "            original_pvalues=p_values.tolist(),\n",
        "            corrected_pvalues=corrected_pvalues.tolist(),\n",
        "            alpha_adjusted=alpha_adjusted,\n",
        "            significant_tests=significant_tests.tolist(),\n",
        "            num_tests=n_tests,\n",
        "            alpha_original=self.alpha,\n",
        "            correction_method=method\n",
        "        )\n",
        "    \n",
        "    def drug_safety_signal_correction(self, drug_event_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Apply Bonferroni correction specifically for drug safety signal detection\n",
        "        \n",
        "        Args:\n",
        "            drug_event_results: List of dicts with 'drug', 'event', 'p_value', 'effect_size'\n",
        "        \n",
        "        Returns:\n",
        "            Corrected results with false discovery control\n",
        "        \"\"\"\n",
        "        print(f\"üîç Applying Bonferroni correction to {len(drug_event_results)} drug-event pairs...\")\n",
        "        \n",
        "        if not drug_event_results:\n",
        "            return {\"corrected_results\": [], \"summary\": \"No drug-event pairs provided\"}\n",
        "        \n",
        "        # Extract p-values\n",
        "        p_values = [result.get('p_value', 1.0) for result in drug_event_results]\n",
        "        \n",
        "        # Apply Holm-Bonferroni (more powerful for safety signals)\n",
        "        correction_result = self.apply_correction(p_values, method=\"holm\")\n",
        "        \n",
        "        # Create corrected results\n",
        "        corrected_results = []\n",
        "        significant_signals = 0\n",
        "        \n",
        "        for i, original_result in enumerate(drug_event_results):\n",
        "            corrected_result = original_result.copy()\n",
        "            corrected_result.update({\n",
        "                'original_p_value': correction_result.original_pvalues[i],\n",
        "                'corrected_p_value': correction_result.corrected_pvalues[i],\n",
        "                'is_significant_corrected': correction_result.significant_tests[i],\n",
        "                'alpha_adjusted': correction_result.alpha_adjusted,\n",
        "                'bonferroni_method': correction_result.correction_method,\n",
        "                'false_positive_controlled': True\n",
        "            })\n",
        "            corrected_results.append(corrected_result)\n",
        "            \n",
        "            if correction_result.significant_tests[i]:\n",
        "                significant_signals += 1\n",
        "        \n",
        "        # Generate summary\n",
        "        summary = {\n",
        "            \"total_tests\": len(drug_event_results),\n",
        "            \"significant_before_correction\": sum(1 for p in p_values if p <= self.alpha),\n",
        "            \"significant_after_correction\": significant_signals,\n",
        "            \"false_positives_reduced\": sum(1 for p in p_values if p <= self.alpha) - significant_signals,\n",
        "            \"correction_method\": correction_result.correction_method,\n",
        "            \"alpha_original\": self.alpha,\n",
        "            \"alpha_adjusted\": correction_result.alpha_adjusted,\n",
        "            \"multiple_testing_controlled\": True\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Bonferroni correction applied:\")\n",
        "        print(f\"   üìä Tests before correction: {summary['significant_before_correction']} significant\")\n",
        "        print(f\"   üìä Tests after correction: {summary['significant_after_correction']} significant\") \n",
        "        print(f\"   üõ°Ô∏è False positives reduced: {summary['false_positives_reduced']}\")\n",
        "        print(f\"   üìà Method: {correction_result.correction_method}\")\n",
        "        \n",
        "        return {\n",
        "            \"corrected_results\": corrected_results,\n",
        "            \"summary\": summary,\n",
        "            \"correction_details\": correction_result\n",
        "        }\n",
        "    \n",
        "    def model_comparison_correction(self, model_comparisons: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Apply Bonferroni correction to model performance comparisons\n",
        "        \n",
        "        Args:\n",
        "            model_comparisons: List of model comparison results with p-values\n",
        "        \n",
        "        Returns:\n",
        "            Corrected model comparison results\n",
        "        \"\"\"\n",
        "        print(f\"üîç Applying Bonferroni correction to {len(model_comparisons)} model comparisons...\")\n",
        "        \n",
        "        if not model_comparisons:\n",
        "            return {\"corrected_comparisons\": [], \"summary\": \"No model comparisons provided\"}\n",
        "        \n",
        "        # Calculate p-values from effect sizes (simplified approach)\n",
        "        p_values = []\n",
        "        for comparison in model_comparisons:\n",
        "            effect_size = abs(comparison.get('effect_size', 0))\n",
        "            \n",
        "            # Convert effect size to approximate p-value (simplified)\n",
        "            # In practice, you'd use proper statistical tests (t-test, etc.)\n",
        "            if effect_size > 0.8:\n",
        "                p_val = 0.01  # Large effect\n",
        "            elif effect_size > 0.5:\n",
        "                p_val = 0.05  # Medium effect\n",
        "            elif effect_size > 0.2:\n",
        "                p_val = 0.15  # Small effect\n",
        "            else:\n",
        "                p_val = 0.50  # No effect\n",
        "                \n",
        "            p_values.append(p_val)\n",
        "        \n",
        "        # Apply Bonferroni correction\n",
        "        correction_result = self.apply_correction(p_values, method=\"bonferroni\")\n",
        "        \n",
        "        # Update comparisons\n",
        "        corrected_comparisons = []\n",
        "        for i, comparison in enumerate(model_comparisons):\n",
        "            corrected_comparison = comparison.copy()\n",
        "            corrected_comparison.update({\n",
        "                'original_p_value': correction_result.original_pvalues[i],\n",
        "                'corrected_p_value': correction_result.corrected_pvalues[i],\n",
        "                'is_significant_corrected': correction_result.significant_tests[i],\n",
        "                'bonferroni_adjusted': True\n",
        "            })\n",
        "            corrected_comparisons.append(corrected_comparison)\n",
        "        \n",
        "        summary = {\n",
        "            \"total_comparisons\": len(model_comparisons),\n",
        "            \"significant_before_correction\": sum(1 for p in p_values if p <= self.alpha),\n",
        "            \"significant_after_correction\": sum(correction_result.significant_tests),\n",
        "            \"alpha_adjusted\": correction_result.alpha_adjusted\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Model comparison correction applied:\")\n",
        "        print(f\"   üìä Significant before: {summary['significant_before_correction']}\")\n",
        "        print(f\"   üìä Significant after: {summary['significant_after_correction']}\")\n",
        "        \n",
        "        return {\n",
        "            \"corrected_comparisons\": corrected_comparisons,\n",
        "            \"summary\": summary\n",
        "        }\n",
        "\n",
        "# Initialize Bonferroni correction framework\n",
        "bonferroni = BonferroniCorrection(alpha=0.05)\n",
        "\n",
        "print(\"‚úÖ Bonferroni correction framework initialized\")\n",
        "print(\"üõ°Ô∏è Ready to control false positives in:\")\n",
        "print(\"   ‚Ä¢ Drug safety signal detection\")\n",
        "print(\"   ‚Ä¢ Model performance comparisons\") \n",
        "print(\"   ‚Ä¢ Multiple risk threshold testing\")\n",
        "print(\"   ‚Ä¢ Any multiple testing scenario\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ Demo: Drug Safety Signal Detection with Bonferroni Correction\n",
        "print(\"üß™ Demonstrating Bonferroni correction for drug safety signals...\")\n",
        "\n",
        "# Simulate drug safety signal testing with multiple drug-event combinations\n",
        "# This represents the type of analysis you'd do with real FAERS data\n",
        "\n",
        "def simulate_drug_safety_testing():\n",
        "    \"\"\"\n",
        "    Simulate multiple drug-event testing scenario where false positives are likely\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define drugs and adverse events from your FAERS data\n",
        "    drugs = ['WARFARIN', 'METFORMIN', 'ATORVASTATIN', 'LISINOPRIL', 'AMLODIPINE', 'LEVOTHYROXINE', 'ASPIRIN']\n",
        "    adverse_events = ['MYOCARDIAL_INFARCTION', 'STROKE', 'BLEEDING', 'LIVER_INJURY', 'KIDNEY_FAILURE', 'ALLERGIC_REACTION']\n",
        "    \n",
        "    # Simulate statistical testing results for each drug-event pair\n",
        "    drug_event_results = []\n",
        "    \n",
        "    for drug in drugs:\n",
        "        for event in adverse_events:\n",
        "            # Simulate a statistical test (e.g., chi-square, Fisher's exact test)\n",
        "            # Most associations should be non-significant (null hypothesis true)\n",
        "            \n",
        "            if drug == 'WARFARIN' and event == 'BLEEDING':\n",
        "                # True positive: Warfarin really causes bleeding\n",
        "                p_value = 0.001\n",
        "                effect_size = 2.5\n",
        "                odds_ratio = 3.2\n",
        "            elif drug == 'ATORVASTATIN' and event == 'LIVER_INJURY':\n",
        "                # True positive: Statins can cause liver injury\n",
        "                p_value = 0.008\n",
        "                effect_size = 1.8\n",
        "                odds_ratio = 2.1\n",
        "            elif drug == 'WARFARIN' and event == 'STROKE':\n",
        "                # Borderline association\n",
        "                p_value = 0.045\n",
        "                effect_size = 1.2\n",
        "                odds_ratio = 1.6\n",
        "            else:\n",
        "                # Random noise - should be non-significant\n",
        "                # But some will appear significant by chance (Type I errors)\n",
        "                p_value = np.random.uniform(0.001, 0.8)\n",
        "                effect_size = np.random.uniform(0.1, 1.5)\n",
        "                odds_ratio = np.random.uniform(0.8, 1.8)\n",
        "            \n",
        "            drug_event_results.append({\n",
        "                'drug': drug,\n",
        "                'adverse_event': event,\n",
        "                'p_value': p_value,\n",
        "                'effect_size': effect_size,\n",
        "                'odds_ratio': odds_ratio,\n",
        "                'drug_event_pair': f\"{drug}_{event}\"\n",
        "            })\n",
        "    \n",
        "    return drug_event_results\n",
        "\n",
        "# Generate simulated drug safety data\n",
        "drug_safety_data = simulate_drug_safety_testing()\n",
        "\n",
        "print(f\"üìä Testing {len(drug_safety_data)} drug-event combinations...\")\n",
        "print(f\"   üî¨ Total comparisons: {len(drug_safety_data)}\")\n",
        "\n",
        "# Show uncorrected results first\n",
        "uncorrected_significant = [result for result in drug_safety_data if result['p_value'] <= 0.05]\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Without Bonferroni correction:\")\n",
        "print(f\"   üìà Significant associations: {len(uncorrected_significant)}\")\n",
        "print(f\"   üìã Uncorrected significant results:\")\n",
        "\n",
        "for result in uncorrected_significant[:5]:  # Show first 5\n",
        "    print(f\"      üî∏ {result['drug']} ‚Üí {result['adverse_event']}: p={result['p_value']:.4f}, OR={result['odds_ratio']:.2f}\")\n",
        "\n",
        "if len(uncorrected_significant) > 5:\n",
        "    print(f\"      ... and {len(uncorrected_significant) - 5} more\")\n",
        "\n",
        "# Apply Bonferroni correction\n",
        "print(f\"\\nüõ°Ô∏è Applying Bonferroni correction...\")\n",
        "corrected_results = bonferroni.drug_safety_signal_correction(drug_safety_data)\n",
        "\n",
        "# Extract the truly significant signals after correction\n",
        "truly_significant = [\n",
        "    result for result in corrected_results['corrected_results'] \n",
        "    if result['is_significant_corrected']\n",
        "]\n",
        "\n",
        "print(f\"\\n‚úÖ After Bonferroni correction:\")\n",
        "print(f\"   üìà Truly significant associations: {len(truly_significant)}\")\n",
        "print(f\"   üõ°Ô∏è False positives eliminated: {len(uncorrected_significant) - len(truly_significant)}\")\n",
        "\n",
        "if truly_significant:\n",
        "    print(f\"   üìã Bonferroni-corrected significant results:\")\n",
        "    for result in truly_significant:\n",
        "        print(f\"      üî∏ {result['drug']} ‚Üí {result['adverse_event']}: \")\n",
        "        print(f\"         Original p={result['original_p_value']:.4f}, Corrected p={result['corrected_p_value']:.4f}\")\n",
        "        print(f\"         OR={result['odds_ratio']:.2f}, Method={result['bonferroni_method']}\")\n",
        "else:\n",
        "    print(f\"   ‚ÑπÔ∏è No associations remain significant after correction\")\n",
        "\n",
        "# Save corrected results to Snowflake for tracking\n",
        "corrected_results_data = [(\n",
        "    result['drug'],\n",
        "    result['adverse_event'], \n",
        "    result['original_p_value'],\n",
        "    result['corrected_p_value'],\n",
        "    result['is_significant_corrected'],\n",
        "    result['odds_ratio'],\n",
        "    result['bonferroni_method'],\n",
        "    datetime.datetime.now().isoformat()\n",
        ") for result in corrected_results['corrected_results']]\n",
        "\n",
        "corrected_schema = StructType([\n",
        "    StructField(\"DRUG_NAME\", StringType()),\n",
        "    StructField(\"ADVERSE_EVENT\", StringType()),\n",
        "    StructField(\"ORIGINAL_P_VALUE\", DoubleType()),\n",
        "    StructField(\"CORRECTED_P_VALUE\", DoubleType()),\n",
        "    StructField(\"IS_SIGNIFICANT_CORRECTED\", BooleanType()),\n",
        "    StructField(\"ODDS_RATIO\", DoubleType()),\n",
        "    StructField(\"CORRECTION_METHOD\", StringType()),\n",
        "    StructField(\"ANALYSIS_TIMESTAMP\", StringType())\n",
        "])\n",
        "\n",
        "try:\n",
        "    corrected_df = session.create_dataframe(corrected_results_data, schema=corrected_schema)\n",
        "    corrected_df.write.mode(\"overwrite\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BONFERRONI_CORRECTED_SIGNALS\")\n",
        "    print(f\"‚úÖ Bonferroni-corrected results saved to database\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not save to database: {e}\")\n",
        "\n",
        "print(f\"\\nüìä Summary Statistics:\")\n",
        "summary = corrected_results['summary']\n",
        "print(f\"   ‚Ä¢ Total drug-event tests: {summary['total_tests']}\")\n",
        "print(f\"   ‚Ä¢ Significant before correction: {summary['significant_before_correction']}\")\n",
        "print(f\"   ‚Ä¢ Significant after correction: {summary['significant_after_correction']}\")\n",
        "print(f\"   ‚Ä¢ False positives prevented: {summary['false_positives_reduced']}\")\n",
        "print(f\"   ‚Ä¢ Family-wise error rate controlled: {summary['multiple_testing_controlled']}\")\n",
        "print(f\"   ‚Ä¢ Adjusted Œ±-level: {summary['alpha_adjusted']:.6f}\")\n",
        "\n",
        "print(f\"\\nüéØ Key Benefits of Bonferroni Correction:\")\n",
        "print(f\"   üõ°Ô∏è Protects against spurious drug safety signals\")\n",
        "print(f\"   üìä Maintains statistical rigor with multiple testing\")\n",
        "print(f\"   ‚öïÔ∏è Reduces false regulatory alerts\")\n",
        "print(f\"   üî¨ Ensures only robust associations are flagged\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ Demo: Model Performance Comparison with Bonferroni Correction\n",
        "print(\"üî¨ Demonstrating Bonferroni correction for model comparisons...\")\n",
        "\n",
        "def simulate_model_comparisons():\n",
        "    \"\"\"\n",
        "    Simulate multiple model comparison scenario from your evaluation pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    # Simulate results from your model evaluation notebook (06_Model_Evaluation.ipynb)\n",
        "    models = [\n",
        "        'XGBoost_Default',\n",
        "        'XGBoost_Optimized', \n",
        "        'XGBoost_Deep',\n",
        "        'Linear_Baseline',\n",
        "        'Random_Forest',\n",
        "        'Gradient_Boosting',\n",
        "        'Neural_Network'\n",
        "    ]\n",
        "    \n",
        "    # Simulate model performance metrics\n",
        "    model_results = []\n",
        "    for i, model in enumerate(models):\n",
        "        # Simulate MAE and RMSE with some realistic variation\n",
        "        if 'XGBoost' in model:\n",
        "            mae = np.random.normal(1.08, 0.05)  # XGBoost models perform well\n",
        "            rmse = np.random.normal(2.45, 0.1)\n",
        "        elif 'Linear' in model:\n",
        "            mae = np.random.normal(4.20, 0.2)   # Linear baseline performs poorly\n",
        "            rmse = np.random.normal(5.30, 0.3)\n",
        "        else:\n",
        "            mae = np.random.normal(1.50, 0.3)   # Other models intermediate\n",
        "            rmse = np.random.normal(3.00, 0.4)\n",
        "            \n",
        "        model_results.append({\n",
        "            'model_name': model,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'mae_std': np.random.uniform(0.02, 0.08),\n",
        "            'rmse_std': np.random.uniform(0.05, 0.15)\n",
        "        })\n",
        "    \n",
        "    return model_results\n",
        "\n",
        "def generate_pairwise_comparisons(model_results):\n",
        "    \"\"\"\n",
        "    Generate all pairwise model comparisons (like in your evaluation notebook)\n",
        "    \"\"\"\n",
        "    comparisons = []\n",
        "    \n",
        "    for i, model_a in enumerate(model_results):\n",
        "        for j, model_b in enumerate(model_results[i+1:], i+1):\n",
        "            # Calculate performance differences\n",
        "            mae_diff = model_b['mae'] - model_a['mae']\n",
        "            rmse_diff = model_b['rmse'] - model_a['rmse']\n",
        "            \n",
        "            # Calculate combined standard error\n",
        "            combined_std = np.sqrt(model_a['mae_std']**2 + model_b['mae_std']**2)\n",
        "            \n",
        "            # Calculate effect size (Cohen's d equivalent)\n",
        "            effect_size = mae_diff / combined_std if combined_std > 0 else 0.0\n",
        "            \n",
        "            # Determine significance level before correction\n",
        "            significance = \"LARGE\" if abs(effect_size) > 0.8 else \"MEDIUM\" if abs(effect_size) > 0.5 else \"SMALL\"\n",
        "            \n",
        "            comparisons.append({\n",
        "                'model_a': model_a['model_name'],\n",
        "                'model_b': model_b['model_name'],\n",
        "                'mae_difference': mae_diff,\n",
        "                'rmse_difference': rmse_diff,\n",
        "                'effect_size': effect_size,\n",
        "                'significance_level': significance,\n",
        "                'combined_std': combined_std,\n",
        "                'comparison_id': f\"{i}_{j}\"\n",
        "            })\n",
        "    \n",
        "    return comparisons\n",
        "\n",
        "# Generate simulated model evaluation data\n",
        "model_results = simulate_model_comparisons()\n",
        "model_comparisons = generate_pairwise_comparisons(model_results)\n",
        "\n",
        "print(f\"üìä Evaluating {len(model_results)} models with {len(model_comparisons)} pairwise comparisons...\")\n",
        "\n",
        "# Show model performance\n",
        "print(f\"\\nüìà Model Performance (simulated):\")\n",
        "for model in model_results:\n",
        "    print(f\"   üî∏ {model['model_name']}: MAE={model['mae']:.4f}¬±{model['mae_std']:.4f}, RMSE={model['rmse']:.4f}¬±{model['rmse_std']:.4f}\")\n",
        "\n",
        "# Show uncorrected comparisons\n",
        "significant_comparisons = [comp for comp in model_comparisons if comp['significance_level'] in ['MEDIUM', 'LARGE']]\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Without Bonferroni correction:\")\n",
        "print(f\"   üìà Significant model differences: {len(significant_comparisons)}\")\n",
        "print(f\"   üìã Uncorrected significant comparisons:\")\n",
        "\n",
        "for comp in significant_comparisons[:5]:  # Show first 5\n",
        "    print(f\"      üî∏ {comp['model_a']} vs {comp['model_b']}: \")\n",
        "    print(f\"         Effect size={comp['effect_size']:.3f} ({comp['significance_level']})\")\n",
        "\n",
        "# Apply Bonferroni correction\n",
        "print(f\"\\nüõ°Ô∏è Applying Bonferroni correction to model comparisons...\")\n",
        "corrected_model_results = bonferroni.model_comparison_correction(model_comparisons)\n",
        "\n",
        "# Extract truly significant comparisons after correction\n",
        "truly_significant_models = [\n",
        "    comp for comp in corrected_model_results['corrected_comparisons'] \n",
        "    if comp['is_significant_corrected']\n",
        "]\n",
        "\n",
        "print(f\"\\n‚úÖ After Bonferroni correction:\")\n",
        "print(f\"   üìà Truly significant comparisons: {len(truly_significant_models)}\")\n",
        "print(f\"   üõ°Ô∏è False significant differences eliminated: {len(significant_comparisons) - len(truly_significant_models)}\")\n",
        "\n",
        "if truly_significant_models:\n",
        "    print(f\"   üìã Bonferroni-corrected significant model differences:\")\n",
        "    for comp in truly_significant_models:\n",
        "        print(f\"      üî∏ {comp['model_a']} vs {comp['model_b']}: \")\n",
        "        print(f\"         Original p={comp['original_p_value']:.4f}, Corrected p={comp['corrected_p_value']:.4f}\")\n",
        "        print(f\"         Effect size={comp['effect_size']:.3f}\")\n",
        "else:\n",
        "    print(f\"   ‚ÑπÔ∏è No model differences remain significant after correction\")\n",
        "\n",
        "# Integration with your existing evaluation logging\n",
        "try:\n",
        "    # Save corrected model comparison results to match your evaluation schema\n",
        "    corrected_comparison_data = [(\n",
        "        f\"COMP_BONFERRONI_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{comp['comparison_id']}\",\n",
        "        f\"EVAL_BONFERRONI_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "        comp['model_a'],\n",
        "        comp['model_b'],\n",
        "        comp['mae_difference'],\n",
        "        comp['rmse_difference'],\n",
        "        comp['effect_size'],\n",
        "        'BONFERRONI_CORRECTED' if comp['is_significant_corrected'] else 'NOT_SIGNIFICANT',\n",
        "        datetime.datetime.now().isoformat(),\n",
        "        f\"Bonferroni-corrected comparison (Œ±={bonferroni.alpha/len(model_comparisons):.6f})\"\n",
        "    ) for comp in corrected_model_results['corrected_comparisons']]\n",
        "    \n",
        "    comparison_schema = StructType([\n",
        "        StructField(\"COMPARISON_ID\", StringType()),\n",
        "        StructField(\"EVALUATION_ID\", StringType()),\n",
        "        StructField(\"MODEL_A\", StringType()),\n",
        "        StructField(\"MODEL_B\", StringType()),\n",
        "        StructField(\"MAE_DIFFERENCE\", DoubleType()),\n",
        "        StructField(\"RMSE_DIFFERENCE\", DoubleType()),\n",
        "        StructField(\"EFFECT_SIZE\", DoubleType()),\n",
        "        StructField(\"SIGNIFICANCE_LEVEL\", StringType()),\n",
        "        StructField(\"COMPARISON_TIMESTAMP\", StringType()),\n",
        "        StructField(\"COMPARISON_NOTES\", StringType())\n",
        "    ])\n",
        "    \n",
        "    comparison_df = session.create_dataframe(corrected_comparison_data, schema=comparison_schema)\n",
        "    comparison_df.write.mode(\"append\").save_as_table(\"ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.MODEL_COMPARISON_LOG\")\n",
        "    print(f\"‚úÖ Bonferroni-corrected model comparisons saved to evaluation log\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not save model comparisons: {e}\")\n",
        "\n",
        "# Summary\n",
        "summary = corrected_model_results['summary']\n",
        "print(f\"\\nüìä Model Comparison Summary:\")\n",
        "print(f\"   ‚Ä¢ Total pairwise comparisons: {summary['total_comparisons']}\")\n",
        "print(f\"   ‚Ä¢ Significant before correction: {summary['significant_before_correction']}\")\n",
        "print(f\"   ‚Ä¢ Significant after correction: {summary['significant_after_correction']}\")\n",
        "print(f\"   ‚Ä¢ Adjusted Œ±-level: {summary['alpha_adjusted']:.6f}\")\n",
        "\n",
        "print(f\"\\nüéØ Benefits for Model Selection:\")\n",
        "print(f\"   üõ°Ô∏è Prevents overstated model differences\")\n",
        "print(f\"   üìä Maintains statistical validity across multiple tests\")\n",
        "print(f\"   üî¨ Ensures robust model selection decisions\")\n",
        "print(f\"   ‚öïÔ∏è Critical for clinical model deployment confidence\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ Enhanced Patient Risk Assessment with Bonferroni-Corrected Drug Safety\n",
        "print(\"üéØ Creating enhanced inference pipeline with Bonferroni-corrected drug safety signals...\")\n",
        "\n",
        "def enhanced_predict_patient_risk(patient_data: Dict[str, Any], include_drug_safety_correction: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enhanced patient risk prediction that incorporates Bonferroni-corrected drug safety signals\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Standard risk prediction using UDF\n",
        "        standard_prediction = predict_patient_risk(patient_data)\n",
        "        \n",
        "        if not include_drug_safety_correction:\n",
        "            return standard_prediction\n",
        "        \n",
        "        # Get patient medications for drug safety analysis\n",
        "        patient_medications = patient_data.get('medications', [])\n",
        "        if isinstance(patient_medications, str):\n",
        "            patient_medications = patient_medications.split(',')\n",
        "        \n",
        "        # If no medications provided, return standard prediction\n",
        "        if not patient_medications:\n",
        "            standard_prediction['drug_safety_correction'] = {\n",
        "                'applied': False,\n",
        "                'reason': 'No medications provided'\n",
        "            }\n",
        "            return standard_prediction\n",
        "        \n",
        "        # Query Bonferroni-corrected drug safety signals\n",
        "        drug_safety_adjustments = []\n",
        "        total_safety_adjustment = 0\n",
        "        \n",
        "        try:\n",
        "            # Check for significant drug safety signals from our corrected database\n",
        "            for medication in patient_medications:\n",
        "                safety_query = f\"\"\"\n",
        "                    SELECT \n",
        "                        DRUG_NAME,\n",
        "                        ADVERSE_EVENT,\n",
        "                        CORRECTED_P_VALUE,\n",
        "                        IS_SIGNIFICANT_CORRECTED,\n",
        "                        ODDS_RATIO\n",
        "                    FROM ADVERSE_EVENT_MONITORING.DEMO_ANALYTICS.BONFERRONI_CORRECTED_SIGNALS\n",
        "                    WHERE UPPER(DRUG_NAME) = UPPER('{medication}')\n",
        "                    AND IS_SIGNIFICANT_CORRECTED = TRUE\n",
        "                \"\"\"\n",
        "                \n",
        "                safety_results = session.sql(safety_query).collect()\n",
        "                \n",
        "                for result in safety_results:\n",
        "                    # Calculate safety adjustment based on corrected p-value and odds ratio\n",
        "                    odds_ratio = result['ODDS_RATIO']\n",
        "                    corrected_p = result['CORRECTED_P_VALUE']\n",
        "                    \n",
        "                    # Safety adjustment formula (can be customized)\n",
        "                    if odds_ratio > 2.0 and corrected_p < 0.01:\n",
        "                        safety_adjustment = 15  # High risk adjustment\n",
        "                    elif odds_ratio > 1.5 and corrected_p < 0.05:\n",
        "                        safety_adjustment = 10  # Moderate risk adjustment\n",
        "                    else:\n",
        "                        safety_adjustment = 5   # Low risk adjustment\n",
        "                    \n",
        "                    drug_safety_adjustments.append({\n",
        "                        'medication': result['DRUG_NAME'],\n",
        "                        'adverse_event': result['ADVERSE_EVENT'],\n",
        "                        'odds_ratio': odds_ratio,\n",
        "                        'corrected_p_value': corrected_p,\n",
        "                        'safety_adjustment': safety_adjustment,\n",
        "                        'bonferroni_corrected': True\n",
        "                    })\n",
        "                    \n",
        "                    total_safety_adjustment += safety_adjustment\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not retrieve drug safety data: {e}\")\n",
        "            drug_safety_adjustments = []\n",
        "        \n",
        "        # Apply safety adjustment to risk score\n",
        "        original_risk_score = standard_prediction['risk_score']\n",
        "        adjusted_risk_score = min(100.0, original_risk_score + total_safety_adjustment)\n",
        "        \n",
        "        # Determine adjusted risk category\n",
        "        if adjusted_risk_score < 30:\n",
        "            adjusted_risk_category = 'LOW'\n",
        "        elif adjusted_risk_score < 70:\n",
        "            adjusted_risk_category = 'MEDIUM'\n",
        "        else:\n",
        "            adjusted_risk_category = 'HIGH'\n",
        "        \n",
        "        # Create enhanced response\n",
        "        enhanced_response = standard_prediction.copy()\n",
        "        enhanced_response.update({\n",
        "            'original_risk_score': original_risk_score,\n",
        "            'adjusted_risk_score': adjusted_risk_score,\n",
        "            'risk_score': adjusted_risk_score,  # Use adjusted score as primary\n",
        "            'original_risk_category': standard_prediction['risk_category'],\n",
        "            'risk_category': adjusted_risk_category,\n",
        "            'drug_safety_correction': {\n",
        "                'applied': True,\n",
        "                'total_adjustment': total_safety_adjustment,\n",
        "                'significant_drug_signals': len(drug_safety_adjustments),\n",
        "                'bonferroni_corrected': True,\n",
        "                'drug_safety_details': drug_safety_adjustments\n",
        "            },\n",
        "            'clinical_recommendations': generate_enhanced_clinical_recommendations(\n",
        "                adjusted_risk_score, \n",
        "                adjusted_risk_category, \n",
        "                drug_safety_adjustments\n",
        "            ),\n",
        "            'inference_method': 'UDF_with_Bonferroni_Drug_Safety'\n",
        "        })\n",
        "        \n",
        "        return enhanced_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_response = standard_prediction.copy() if 'standard_prediction' in locals() else {}\n",
        "        error_response.update({\n",
        "            'error': f\"Enhanced_inference_error: {str(e)}\",\n",
        "            'drug_safety_correction': {'applied': False, 'error': str(e)},\n",
        "            'success': False\n",
        "        })\n",
        "        return error_response\n",
        "\n",
        "def generate_enhanced_clinical_recommendations(risk_score: float, risk_category: str, drug_safety_details: List[Dict]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate clinical recommendations that incorporate Bonferroni-corrected drug safety signals\n",
        "    \"\"\"\n",
        "    recommendations = generate_clinical_recommendations(risk_score, risk_category)\n",
        "    \n",
        "    # Add drug safety-specific recommendations\n",
        "    if drug_safety_details:\n",
        "        recommendations.insert(0, \"üö® DRUG SAFETY ALERTS (Bonferroni-corrected):\")\n",
        "        \n",
        "        for detail in drug_safety_details:\n",
        "            medication = detail['medication']\n",
        "            adverse_event = detail['adverse_event']\n",
        "            odds_ratio = detail['odds_ratio']\n",
        "            corrected_p = detail['corrected_p_value']\n",
        "            \n",
        "            recommendations.append(\n",
        "                f\"   ‚ö†Ô∏è {medication}: Increased risk of {adverse_event} \"\n",
        "                f\"(OR={odds_ratio:.2f}, corrected p={corrected_p:.4f})\"\n",
        "            )\n",
        "        \n",
        "        recommendations.append(\"   üî¨ Monitor for medication-related adverse events\")\n",
        "        recommendations.append(\"   üìã Consider medication review and alternatives\")\n",
        "        \n",
        "        # Add specific monitoring based on adverse event types\n",
        "        adverse_events = [detail['adverse_event'] for detail in drug_safety_details]\n",
        "        if 'BLEEDING' in adverse_events:\n",
        "            recommendations.append(\"   ü©∏ Monitor bleeding parameters and coagulation studies\")\n",
        "        if 'LIVER_INJURY' in adverse_events:\n",
        "            recommendations.append(\"   ü´Ä Monitor liver function tests regularly\")\n",
        "        if any('CARDIAC' in event or 'HEART' in event for event in adverse_events):\n",
        "            recommendations.append(\"   üíì Enhanced cardiac monitoring recommended\")\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "# Test enhanced inference with drug safety correction\n",
        "print(\"üß™ Testing enhanced inference pipeline with Bonferroni drug safety correction...\")\n",
        "\n",
        "test_patients_with_meds = [\n",
        "    {\n",
        "        'patient_id': 'ENHANCED_001', \n",
        "        'age': 75, \n",
        "        'num_conditions': 8, \n",
        "        'num_medications': 12, \n",
        "        'num_claims': 35,\n",
        "        'medications': ['WARFARIN', 'ATORVASTATIN']  # Medications with known safety signals\n",
        "    },\n",
        "    {\n",
        "        'patient_id': 'ENHANCED_002', \n",
        "        'age': 45, \n",
        "        'num_conditions': 3, \n",
        "        'num_medications': 5, \n",
        "        'num_claims': 10,\n",
        "        'medications': ['METFORMIN', 'LISINOPRIL']  # Safer medications\n",
        "    },\n",
        "    {\n",
        "        'patient_id': 'ENHANCED_003', \n",
        "        'age': 82, \n",
        "        'num_conditions': 15, \n",
        "        'num_medications': 18, \n",
        "        'num_claims': 50,\n",
        "        'medications': ['WARFARIN', 'ATORVASTATIN', 'ASPIRIN']  # Multiple high-risk medications\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nüìä Comparing standard vs. enhanced (Bonferroni-corrected) predictions:\")\n",
        "\n",
        "for patient in test_patients_with_meds:\n",
        "    print(f\"\\nüë§ Patient {patient['patient_id']}:\")\n",
        "    print(f\"   Medications: {', '.join(patient['medications'])}\")\n",
        "    \n",
        "    # Standard prediction\n",
        "    standard_result = enhanced_predict_patient_risk(patient, include_drug_safety_correction=False)\n",
        "    standard_score = standard_result['risk_score']\n",
        "    standard_category = standard_result['risk_category']\n",
        "    \n",
        "    # Enhanced prediction with Bonferroni correction\n",
        "    enhanced_result = enhanced_predict_patient_risk(patient, include_drug_safety_correction=True)\n",
        "    enhanced_score = enhanced_result['risk_score']\n",
        "    enhanced_category = enhanced_result['risk_category']\n",
        "    \n",
        "    print(f\"   üìà Standard Risk: {standard_score:.1f} ({standard_category})\")\n",
        "    print(f\"   üõ°Ô∏è Enhanced Risk: {enhanced_score:.1f} ({enhanced_category})\")\n",
        "    \n",
        "    drug_safety = enhanced_result.get('drug_safety_correction', {})\n",
        "    if drug_safety.get('applied', False):\n",
        "        adjustment = drug_safety.get('total_adjustment', 0)\n",
        "        signals = drug_safety.get('significant_drug_signals', 0)\n",
        "        print(f\"   üî¨ Safety Adjustment: +{adjustment} points from {signals} Bonferroni-corrected signals\")\n",
        "        \n",
        "        if drug_safety.get('drug_safety_details'):\n",
        "            print(f\"   ‚ö†Ô∏è Drug Safety Alerts:\")\n",
        "            for detail in drug_safety['drug_safety_details']:\n",
        "                print(f\"      ‚Ä¢ {detail['medication']}: {detail['adverse_event']} risk\")\n",
        "    else:\n",
        "        print(f\"   ‚ÑπÔ∏è No significant drug safety adjustments\")\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced inference pipeline operational with Bonferroni correction\")\n",
        "print(f\"üéØ Key Enhancements:\")\n",
        "print(f\"   üõ°Ô∏è False positive drug safety signals eliminated\")\n",
        "print(f\"   üìä Statistically rigorous risk adjustments\")\n",
        "print(f\"   üî¨ Multiple testing correction applied\")\n",
        "print(f\"   ‚öïÔ∏è More reliable clinical decision support\")\n",
        "\n",
        "# Update todos\n",
        "print(f\"\\nüìã Bonferroni correction integration complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
